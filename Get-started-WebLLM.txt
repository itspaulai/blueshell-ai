PACKAGE.JSON

{
  "name": "get-started-web-worker",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "start": "parcel src/get_started.html  --port 8885",
    "build": "parcel build src/get_started.html --dist-dir lib"
  },
  "devDependencies": {
    "buffer": "^6.0.3",
    "parcel": "^2.8.3",
    "process": "^0.11.10",
    "tslib": "^2.3.1",
    "typescript": "^4.9.5",
    "url": "^0.11.3"
  },
  "dependencies": {
    "@mlc-ai/web-llm": "^0.2.77"
  }
}

WORKER.TS

import { WebWorkerMLCEngineHandler } from "@mlc-ai/web-llm";

// Hookup an engine to a worker handler
const handler = new WebWorkerMLCEngineHandler();
self.onmessage = (msg: MessageEvent) => {
  handler.onmessage(msg);
};

MAIN.TS

import * as webllm from "@mlc-ai/web-llm";

function setLabel(id: string, text: string) {
  const label = document.getElementById(id);
  if (label == null) {
    throw Error("Cannot find label " + id);
  }
  label.innerText = text;
}

// There are two demonstrations, pick one to run

/**
 * Chat completion (OpenAI style) without streaming, where we get the entire response at once.
 */
async function mainNonStreaming() {
  const initProgressCallback = (report: webllm.InitProgressReport) => {
    setLabel("init-label", report.text);
  };
  const selectedModel = "Llama-3.2-3B-Instruct-q4f16_1-MLC";

  const engine: webllm.MLCEngineInterface =
    await webllm.CreateWebWorkerMLCEngine(
      new Worker(new URL("./worker.ts", import.meta.url), { type: "module" }),
      selectedModel,
      { initProgressCallback: initProgressCallback },
    );

  const request: webllm.ChatCompletionRequest = {
    messages: [
      {
        role: "system",
        content:
          "You are a helpful, respectful and honest assistant. " +
          "Be as happy as you can when speaking please. ",
      },
      { role: "user", content: "Provide me three US states." },
      { role: "assistant", content: "California, New York, Pennsylvania." },
      { role: "user", content: "Two more please!" },
    ],
    n: 3,
    temperature: 1.5,
    max_tokens: 256,
  };

  const reply0 = await engine.chat.completions.create(request);
  console.log(reply0);

  console.log(reply0.usage);
}

/**
 * Chat completion (OpenAI style) with streaming, where delta is sent while generating response.
 */
async function mainStreaming() {
  const initProgressCallback = (report: webllm.InitProgressReport) => {
    setLabel("init-label", report.text);
  };
  const selectedModel = "Llama-3.1-8B-Instruct-q4f32_1-MLC";

  const engine: webllm.MLCEngineInterface =
    await webllm.CreateWebWorkerMLCEngine(
      new Worker(new URL("./worker.ts", import.meta.url), { type: "module" }),
      selectedModel,
      { initProgressCallback: initProgressCallback },
    );

  const request: webllm.ChatCompletionRequest = {
    stream: true,
    stream_options: { include_usage: true },
    messages: [
      {
        role: "system",
        content:
          "You are a helpful, respectful and honest assistant. " +
          "Be as happy as you can when speaking please. ",
      },
      { role: "user", content: "Provide me three US states." },
      { role: "assistant", content: "California, New York, Pennsylvania." },
      { role: "user", content: "Two more please!" },
    ],
    temperature: 1.5,
    max_tokens: 256,
  };

  const asyncChunkGenerator = await engine.chat.completions.create(request);
  let message = "";
  for await (const chunk of asyncChunkGenerator) {
    console.log(chunk);
    message += chunk.choices[0]?.delta?.content || "";
    setLabel("generate-label", message);
    if (chunk.usage) {
      console.log(chunk.usage); // only last chunk has usage
    }
    // engine.interruptGenerate();  // works with interrupt as well
  }
  console.log("Final message:\n", await engine.getMessage()); // the concatenated message
}

// Run one of the function below
// mainNonStreaming();
mainStreaming();


MULTI-ROUND.TS

import * as webllm from "@mlc-ai/web-llm";

function setLabel(id: string, text: string) {
  const label = document.getElementById(id);
  if (label == null) {
    throw Error("Cannot find label " + id);
  }
  label.innerText = text;
}

/**
 * We demonstrate multiround chatting. Though users are required to maintain chat history, internally
 * we compare provided `messages` with the internal chat history. If it matches, we will reuse KVs
 * and hence save computation -- essentially an implicit internal optimization.
 */
async function main() {
  const initProgressCallback = (report: webllm.InitProgressReport) => {
    setLabel("init-label", report.text);
  };
  const selectedModel = "Llama-3.1-8B-Instruct-q4f32_1-MLC";
  const engine: webllm.MLCEngineInterface = await webllm.CreateMLCEngine(
    selectedModel,
    { initProgressCallback: initProgressCallback },
  );

  // Round 0
  const messages: webllm.ChatCompletionMessageParam[] = [
    {
      role: "system",
      content:
        "You are a helpful, respectful and honest assistant. " +
        "Be as happy as you can when speaking please. ",
    },
    { role: "user", content: "Provide me three US states." },
  ];

  const request0: webllm.ChatCompletionRequest = {
    stream: false, // can be streaming, same behavior
    messages: messages,
  };

  const reply0 = await engine.chat.completions.create(request0);
  const replyMessage0 = await engine.getMessage();
  console.log(reply0);
  console.log(replyMessage0);
  console.log(reply0.usage);

  // Round 1
  // Append generated response to messages
  messages.push({ role: "assistant", content: replyMessage0 });
  // Append new user input
  messages.push({ role: "user", content: "Two more please!" });
  // Below line would cause an internal reset (clear KV cache, etc.) since the history no longer
  // matches the new request
  // messages[0].content = "Another system prompt";

  const request1: webllm.ChatCompletionRequest = {
    stream: false, // can be streaming, same behavior
    messages: messages,
  };

  const reply1 = await engine.chat.completions.create(request1);
  const replyMessage1 = await engine.getMessage();
  console.log(reply1);
  console.log(replyMessage1);
  console.log(reply1.usage);

  // If we used multiround chat, request1 should only prefill a small number of tokens
  const prefillTokens0 = reply0.usage?.prompt_tokens;
  const prefillTokens1 = reply1.usage?.prompt_tokens;
  console.log("Requset 0 prompt tokens: ", prefillTokens0);
  console.log("Requset 1 prompt tokens: ", prefillTokens1);
  if (
    prefillTokens0 === undefined ||
    prefillTokens1 === undefined ||
    prefillTokens1 > prefillTokens0
  ) {
    throw Error("Multi-round chat is not triggered as expected.");
  }
}

main();