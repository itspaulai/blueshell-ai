[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

backends/onnx

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

[ONNX ](/docs/transformers.js/v3.0.0/en/api/backends/onnx)

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  backends/onnx

Handler file for choosing the correct version of ONNX Runtime, based on the
environment. Ideally, we could import the `onnxruntime-web` and `onnxruntime-
node` packages only when needed, but dynamic imports don‚Äôt seem to work with
the current webpack version and/or configuration. This is possibly due to the
experimental nature of top-level await statements. So, we just import both
packages, and use the appropriate one based on the environment:

  * When running in node, we use `onnxruntime-node`.
  * When running in the browser, we use `onnxruntime-web` (`onnxruntime-node` is not bundled).

This module is not directly exported, but can be accessed through the
environment variables:

Copied

    
    
    import { env } from '@huggingface/transformers';
    console.log(env.backends.onnx);

  * backends/onnx
    *  _static_
      * `.deviceToExecutionProviders([device])` ‚áí `Array.<ONNXExecutionProviders>`
      * `.createInferenceSession(buffer, session_options, session_config)` ‚áí `*`
      * `.isONNXTensor(x)` ‚áí `boolean`
      * `.isONNXProxy()` ‚áí `boolean`
    * _inner_
      * `~defaultDevices` : `Array.<ONNXExecutionProviders>`
      * `~wasmInitPromise` : `Promise<any>` | `null`
      * `~DEVICE_TO_EXECUTION_PROVIDER_MAPPING` : `*`
      * `~supportedDevices` : `*`
      * `~ONNX_ENV` : `*`
      * `~ONNXExecutionProviders` : `*`

* * *

##  backends/onnx.deviceToExecutionProviders([device]) ‚áí <code> Array. <
ONNXExecutionProviders > </code>

Map a device to the execution providers to use for the given device.

**Kind** : static method of `backends/onnx`  
**Returns** : `Array.<ONNXExecutionProviders>` \- The execution providers to
use for the given device.

Param| Type| Default| Description  
---|---|---|---  
[device]| `*`| ``| (Optional) The device to run the inference on.  
  
* * *

##  backends/onnx.createInferenceSession(buffer, session_options,
session_config) ‚áí <code> * </code>

Create an ONNX inference session.

**Kind** : static method of `backends/onnx`  
**Returns** : `*` \- The ONNX inference session.

Param| Type| Description  
---|---|---  
buffer| `Uint8Array`| The ONNX model buffer.  
session_options| `*`| ONNX inference session options.  
session_config| `Object`| ONNX inference session configuration.  
  
* * *

##  backends/onnx.isONNXTensor(x) ‚áí <code> boolean </code>

Check if an object is an ONNX tensor.

**Kind** : static method of `backends/onnx`  
**Returns** : `boolean` \- Whether the object is an ONNX tensor.

Param| Type| Description  
---|---|---  
x| `any`| The object to check  
  
* * *

##  backends/onnx.isONNXProxy() ‚áí <code> boolean </code>

Check if ONNX‚Äôs WASM backend is being proxied.

**Kind** : static method of `backends/onnx`  
**Returns** : `boolean` \- Whether ONNX‚Äôs WASM backend is being proxied.

* * *

##  backends/onnx~defaultDevices : <code> Array. < ONNXExecutionProviders >
</code>

**Kind** : inner property of `backends/onnx`

* * *

##  backends/onnx~wasmInitPromise : <code> Promise < any > </code> | <code> null </code>

To prevent multiple calls to `initWasm()`, we store the first call in a
Promise that is resolved when the first InferenceSession is created.
Subsequent calls will wait for this Promise to resolve before creating their
own InferenceSession.

**Kind** : inner property of `backends/onnx`

* * *

##  backends/onnx~DEVICE_TO_EXECUTION_PROVIDER_MAPPING : <code> * </code>

**Kind** : inner constant of `backends/onnx`

* * *

##  backends/onnx~supportedDevices : <code> * </code>

The list of supported devices, sorted by priority/performance.

**Kind** : inner constant of `backends/onnx`

* * *

##  backends/onnx~ONNX_ENV : <code> * </code>

**Kind** : inner constant of `backends/onnx`

* * *

##  backends/onnx~ONNXExecutionProviders : <code> * </code>

**Kind** : inner typedef of `backends/onnx`

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/backends/onnx.md)

[‚ÜêEnvironment variables](/docs/transformers.js/v3.0.0/en/api/env)
[Parameters‚Üí](/docs/transformers.js/v3.0.0/en/api/generation/parameters)

backends/onnx backends/onnx.deviceToExecutionProviders([device]) ‚áí ` Array. < ONNXExecutionProviders > ` backends/onnx.createInferenceSession(buffer, session_options, session_config) ‚áí ` * ` backends/onnx.isONNXTensor(x) ‚áí ` boolean ` backends/onnx.isONNXProxy() ‚áí ` boolean ` backends/onnx~defaultDevices : ` Array. < ONNXExecutionProviders > ` backends/onnx~wasmInitPromise : ` Promise < any > ` | ` null ` backends/onnx~DEVICE_TO_EXECUTION_PROVIDER_MAPPING : ` * ` backends/onnx~supportedDevices : ` * ` backends/onnx~ONNX_ENV : ` * ` backends/onnx~ONNXExecutionProviders : ` * `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

configs

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  configs

Helper module for using model configs. For more information, see the
corresponding [Python
documentation](https://huggingface.co/docs/transformers/v3.0.0/en/model_doc/auto#transformers.AutoConfig).

**Example:** Load an `AutoConfig`.

Copied

    
    
    import { AutoConfig } from '@huggingface/transformers';
    const config = await AutoConfig.from_pretrained('bert-base-uncased');
    console.log(config);
    // PretrainedConfig {
    //   "model_type": "bert",
    //   "is_encoder_decoder": false,
    //   "architectures": [
    //       "BertForMaskedLM"
    //   ],
    //   "vocab_size": 30522
    //   "num_attention_heads": 12,
    //   "num_hidden_layers": 12,
    //   "hidden_size": 768,
    //   "max_position_embeddings": 512,
    //   ...
    // }

  * configs
    *  _static_
      * .PretrainedConfig
        * `new PretrainedConfig(configJSON)`
        * _instance_
          * `.model_type` : `string` | `null`
          * `.is_encoder_decoder` : `boolean`
          * `.max_position_embeddings` : `number`
        * _static_
          * `.from_pretrained(pretrained_model_name_or_path, options)` ‚áí `Promise.<PretrainedConfig>`
      * .AutoConfig
        * `.from_pretrained()` : `*`
      * `.getKeyValueShapes(config)` ‚áí `Record.<string, Array<number>>`
        * `~decoderFeeds` : `Record.<string, Array<number>>`
    * _inner_
      * `~loadConfig(pretrained_model_name_or_path, options)` ‚áí `Promise.<Object>`
      * `~getNormalizedConfig(config)` ‚áí `Object`
      * `~PretrainedOptions` : `*`

* * *

##  configs.PretrainedConfig

Base class for all configuration classes. For more information, see the
corresponding [Python
documentation](https://huggingface.co/docs/transformers/v3.0.0/en/main_classes/configuration#transformers.PretrainedConfig).

**Kind** : static class of `configs`

  * .PretrainedConfig
    * `new PretrainedConfig(configJSON)`
    * _instance_
      * `.model_type` : `string` | `null`
      * `.is_encoder_decoder` : `boolean`
      * `.max_position_embeddings` : `number`
    * _static_
      * `.from_pretrained(pretrained_model_name_or_path, options)` ‚áí `Promise.<PretrainedConfig>`

* * *

###  new PretrainedConfig(configJSON)

Create a new PreTrainedTokenizer instance.

Param| Type| Description  
---|---|---  
configJSON| `Object`| The JSON of the config.  
  
* * *

###  pretrainedConfig.model_type : <code> string </code> | <code> null </code>

**Kind** : instance property of `PretrainedConfig`

* * *

###  pretrainedConfig.is_encoder_decoder : <code> boolean </code>

**Kind** : instance property of `PretrainedConfig`

* * *

###  pretrainedConfig.max_position_embeddings : <code> number </code>

**Kind** : instance property of `PretrainedConfig`

* * *

###  PretrainedConfig.from_pretrained(pretrained_model_name_or_path, options)
‚áí <code> Promise. < PretrainedConfig > </code>

Loads a pre-trained config from the given `pretrained_model_name_or_path`.

**Kind** : static method of `PretrainedConfig`  
**Returns** : `Promise.<PretrainedConfig>` \- A new instance of the
`PretrainedConfig` class.  
**Throws** :

  * `Error` Throws an error if the config.json is not found in the `pretrained_model_name_or_path`.

Param| Type| Description  
---|---|---  
pretrained_model_name_or_path| `string`| The path to the pre-trained config.  
options| `PretrainedOptions`| Additional options for loading the config.  
  
* * *

##  configs.AutoConfig

Helper class which is used to instantiate pretrained configs with the
`from_pretrained` function.

**Kind** : static class of `configs`

* * *

###  AutoConfig.from_pretrained() : <code> * </code>

**Kind** : static method of `AutoConfig`

* * *

##  configs.getKeyValueShapes(config) ‚áí <code> Record. < string, Array <
number > > </code>

**Kind** : static method of `configs`

Param| Type  
---|---  
config| `PretrainedConfig`  
  
* * *

###  getKeyValueShapes~decoderFeeds : <code> Record. < string, Array < number
> > </code>

**Kind** : inner constant of `getKeyValueShapes`

* * *

##  configs~loadConfig(pretrained_model_name_or_path, options) ‚áí <code>
Promise. < Object > </code>

Loads a config from the specified path.

**Kind** : inner method of `configs`  
**Returns** : `Promise.<Object>` \- A promise that resolves with information
about the loaded config.

Param| Type| Description  
---|---|---  
pretrained_model_name_or_path| `string`| The path to the config directory.  
options| `PretrainedOptions`| Additional options for loading the config.  
  
* * *

##  configs~getNormalizedConfig(config) ‚áí <code> Object </code>

**Kind** : inner method of `configs`  
**Returns** : `Object` \- The normalized configuration.

Param| Type  
---|---  
config| `PretrainedConfig`  
  
* * *

##  configs~PretrainedOptions : <code> * </code>

**Kind** : inner typedef of `configs`

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/configs.md)

[‚ÜêProcessors](/docs/transformers.js/v3.0.0/en/api/processors) [Environment
variables‚Üí](/docs/transformers.js/v3.0.0/en/api/env)

configs configs.PretrainedConfig new PretrainedConfig(configJSON) pretrainedConfig.model_type : ` string ` | ` null ` pretrainedConfig.is_encoder_decoder : ` boolean ` pretrainedConfig.max_position_embeddings : ` number ` PretrainedConfig.from_pretrained(pretrained_model_name_or_path, options) ‚áí ` Promise. < PretrainedConfig > ` configs.AutoConfig AutoConfig.from_pretrained() : ` * ` configs.getKeyValueShapes(config) ‚áí ` Record. < string, Array < number > > ` getKeyValueShapes~decoderFeeds : ` Record. < string, Array < number > > ` configs~loadConfig(pretrained_model_name_or_path, options) ‚áí ` Promise. < Object > ` configs~getNormalizedConfig(config) ‚áí ` Object ` configs~PretrainedOptions : ` * `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

env

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  env

Module used to configure Transformers.js.

**Example:** Disable remote models.

Copied

    
    
    import { env } from '@huggingface/transformers';
    env.allowRemoteModels = false;

**Example:** Set local model path.

Copied

    
    
    import { env } from '@huggingface/transformers';
    env.localModelPath = '/path/to/local/models/';

**Example:** Set cache directory.

Copied

    
    
    import { env } from '@huggingface/transformers';
    env.cacheDir = '/path/to/cache/directory/';

  * env
    *  _static_
      * `.apis`
      * `.env` : `TransformersEnvironment`
    * _inner_
      * `~IS_BROWSER_ENV`
      * `~IS_WEBWORKER_ENV`
      * `~IS_WEB_CACHE_AVAILABLE`
      * `~IS_WEBGPU_AVAILABLE`
      * `~IS_WEBNN_AVAILABLE`
      * `~IS_PROCESS_AVAILABLE`
      * `~IS_NODE_ENV`
      * `~IS_FS_AVAILABLE`
      * `~IS_PATH_AVAILABLE`
      * `~TransformersEnvironment` : `Object`

* * *

##  env.apis

A read-only object containing information about the APIs available in the
current environment.

**Kind** : static constant of `env`

* * *

##  env.env : <code> TransformersEnvironment </code>

**Kind** : static constant of `env`

* * *

##  env~IS_BROWSER_ENV

Whether we are running in a browser environment

**Kind** : inner property of `env`

* * *

##  env~IS_WEBWORKER_ENV

Whether we are running in a web worker environment

**Kind** : inner property of `env`

* * *

##  env~IS_WEB_CACHE_AVAILABLE

Whether the Cache API is available

**Kind** : inner property of `env`

* * *

##  env~IS_WEBGPU_AVAILABLE

Whether the WebGPU API is available

**Kind** : inner property of `env`

* * *

##  env~IS_WEBNN_AVAILABLE

Whether the WebNN API is available

**Kind** : inner property of `env`

* * *

##  env~IS_PROCESS_AVAILABLE

Whether the Node.js process API is available

**Kind** : inner property of `env`

* * *

##  env~IS_NODE_ENV

Whether we are running in a Node.js environment

**Kind** : inner property of `env`

* * *

##  env~IS_FS_AVAILABLE

Whether the filesystem API is available

**Kind** : inner property of `env`

* * *

##  env~IS_PATH_AVAILABLE

Whether the path API is available

**Kind** : inner property of `env`

* * *

##  env~TransformersEnvironment : <code> Object </code>

Global variable given visible to users to control execution. This provides
users a simple way to configure Transformers.js.

**Kind** : inner typedef of `env`  
**Properties**

Name| Type| Description  
---|---|---  
version| `string`| This version of Transformers.js.  
backends| `*`| Expose environment variables of different backends, allowing
users to set these variables if they want to.  
allowRemoteModels| `boolean`| Whether to allow loading of remote files,
defaults to `true`. If set to `false`, it will have the same effect as setting
`local_files_only=true` when loading pipelines, models, tokenizers,
processors, etc.  
remoteHost| `string`| Host URL to load models from. Defaults to the Hugging
Face Hub.  
remotePathTemplate| `string`| Path template to fill in and append to
`remoteHost` when loading models.  
allowLocalModels| `boolean`| Whether to allow loading of local files, defaults
to `false` if running in-browser, and `true` otherwise. If set to `false`, it
will skip the local file check and try to load the model from the remote host.  
localModelPath| `string`| Path to load local models from. Defaults to
`/models/`.  
useFS| `boolean`| Whether to use the file system to load files. By default, it
is `true` if available.  
useBrowserCache| `boolean`| Whether to use Cache API to cache models. By
default, it is `true` if available.  
useFSCache| `boolean`| Whether to use the file system to cache files. By
default, it is `true` if available.  
cacheDir| `string`| The directory to use for caching files with the file
system. By default, it is `./.cache`.  
useCustomCache| `boolean`| Whether to use a custom cache system (defined by
`customCache`), defaults to `false`.  
customCache| `Object`| The custom cache to use. Defaults to `null`. Note: this
must be an object which implements the `match` and `put` functions of the Web
Cache API. For more information, see <https://developer.mozilla.org/en-
US/docs/Web/API/Cache>  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/env.md)

[‚ÜêConfigs](/docs/transformers.js/v3.0.0/en/api/configs)
[ONNX‚Üí](/docs/transformers.js/v3.0.0/en/api/backends/onnx)

env env.apis env.env : ` TransformersEnvironment ` env~IS_BROWSER_ENV
env~IS_WEBWORKER_ENV env~IS_WEB_CACHE_AVAILABLE env~IS_WEBGPU_AVAILABLE
env~IS_WEBNN_AVAILABLE env~IS_PROCESS_AVAILABLE env~IS_NODE_ENV
env~IS_FS_AVAILABLE env~IS_PATH_AVAILABLE env~TransformersEnvironment : `
Object `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

generation/configuration_utils

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

[Parameters
](/docs/transformers.js/v3.0.0/en/api/generation/parameters)[Configuration
](/docs/transformers.js/v3.0.0/en/api/generation/configuration_utils)[Logits
Processors
](/docs/transformers.js/v3.0.0/en/api/generation/logits_process)[Logits
Samplers
](/docs/transformers.js/v3.0.0/en/api/generation/logits_sampler)[Stopping
Criteria
](/docs/transformers.js/v3.0.0/en/api/generation/stopping_criteria)[Streamers
](/docs/transformers.js/v3.0.0/en/api/generation/streamers)

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  generation/configuration_utils

  * generation/configuration_utils
    * .GenerationConfig
      * `new GenerationConfig(config)`
      * `.max_length` : `number`
      * `.max_new_tokens` : `number`
      * `.min_length` : `number`
      * `.min_new_tokens` : `number`
      * `.early_stopping` : `boolean` | `‚Äùnever‚Äù`
      * `.max_time` : `number`
      * `.do_sample` : `boolean`
      * `.num_beams` : `number`
      * `.num_beam_groups` : `number`
      * `.penalty_alpha` : `number`
      * `.use_cache` : `boolean`
      * `.temperature` : `number`
      * `.top_k` : `number`
      * `.top_p` : `number`
      * `.typical_p` : `number`
      * `.epsilon_cutoff` : `number`
      * `.eta_cutoff` : `number`
      * `.diversity_penalty` : `number`
      * `.repetition_penalty` : `number`
      * `.encoder_repetition_penalty` : `number`
      * `.length_penalty` : `number`
      * `.no_repeat_ngram_size` : `number`
      * `.bad_words_ids` : `Array.<Array<number>>`
      * `.force_words_ids` : `Array<Array<number>>` | `Array<Array<Array<number>>>`
      * `.renormalize_logits` : `boolean`
      * `.constraints` : `Array.<Object>`
      * `.forced_bos_token_id` : `number`
      * `.forced_eos_token_id` : `number` | `Array<number>`
      * `.remove_invalid_values` : `boolean`
      * `.exponential_decay_length_penalty` : `*`
      * `.suppress_tokens` : `Array.<number>`
      * `.begin_suppress_tokens` : `Array.<number>`
      * `.forced_decoder_ids` : `*`
      * `.guidance_scale` : `number`
      * `.num_return_sequences` : `number`
      * `.output_attentions` : `boolean`
      * `.output_hidden_states` : `boolean`
      * `.output_scores` : `boolean`
      * `.return_dict_in_generate` : `boolean`
      * `.pad_token_id` : `number`
      * `.bos_token_id` : `number`
      * `.eos_token_id` : `number` | `Array<number>`
      * `.encoder_no_repeat_ngram_size` : `number`
      * `.decoder_start_token_id` : `number`
      * `.generation_kwargs` : `Object`

* * *

##  generation/configuration_utils.GenerationConfig

Class that holds a configuration for a generation task.

**Kind** : static class of `generation/configuration_utils`

  * .GenerationConfig
    * `new GenerationConfig(config)`
    * `.max_length` : `number`
    * `.max_new_tokens` : `number`
    * `.min_length` : `number`
    * `.min_new_tokens` : `number`
    * `.early_stopping` : `boolean` | `‚Äùnever‚Äù`
    * `.max_time` : `number`
    * `.do_sample` : `boolean`
    * `.num_beams` : `number`
    * `.num_beam_groups` : `number`
    * `.penalty_alpha` : `number`
    * `.use_cache` : `boolean`
    * `.temperature` : `number`
    * `.top_k` : `number`
    * `.top_p` : `number`
    * `.typical_p` : `number`
    * `.epsilon_cutoff` : `number`
    * `.eta_cutoff` : `number`
    * `.diversity_penalty` : `number`
    * `.repetition_penalty` : `number`
    * `.encoder_repetition_penalty` : `number`
    * `.length_penalty` : `number`
    * `.no_repeat_ngram_size` : `number`
    * `.bad_words_ids` : `Array.<Array<number>>`
    * `.force_words_ids` : `Array<Array<number>>` | `Array<Array<Array<number>>>`
    * `.renormalize_logits` : `boolean`
    * `.constraints` : `Array.<Object>`
    * `.forced_bos_token_id` : `number`
    * `.forced_eos_token_id` : `number` | `Array<number>`
    * `.remove_invalid_values` : `boolean`
    * `.exponential_decay_length_penalty` : `*`
    * `.suppress_tokens` : `Array.<number>`
    * `.begin_suppress_tokens` : `Array.<number>`
    * `.forced_decoder_ids` : `*`
    * `.guidance_scale` : `number`
    * `.num_return_sequences` : `number`
    * `.output_attentions` : `boolean`
    * `.output_hidden_states` : `boolean`
    * `.output_scores` : `boolean`
    * `.return_dict_in_generate` : `boolean`
    * `.pad_token_id` : `number`
    * `.bos_token_id` : `number`
    * `.eos_token_id` : `number` | `Array<number>`
    * `.encoder_no_repeat_ngram_size` : `number`
    * `.decoder_start_token_id` : `number`
    * `.generation_kwargs` : `Object`

* * *

###  new GenerationConfig(config)

Param| Type  
---|---  
config| `*`  
  
* * *

###  generationConfig.max_length : <code> number </code>

The maximum length the generated tokens can have. Corresponds to the length of
the input prompt + `max_new_tokens`. Its effect is overridden by
`max_new_tokens`, if also set.

**Kind** : instance property of `GenerationConfig`  
**Default** : `20`

* * *

###  generationConfig.max_new_tokens : <code> number </code>

The maximum numbers of tokens to generate, ignoring the number of tokens in
the prompt.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.min_length : <code> number </code>

The minimum length of the sequence to be generated. Corresponds to the length
of the input prompt + `min_new_tokens`. Its effect is overridden by
`min_new_tokens`, if also set.

**Kind** : instance property of `GenerationConfig`  
**Default** : `0`

* * *

###  generationConfig.min_new_tokens : <code> number </code>

The minimum numbers of tokens to generate, ignoring the number of tokens in
the prompt.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.early_stopping : <code> boolean </code> | <code> ‚Äù never ‚Äù </code>

Controls the stopping condition for beam-based methods, like beam-search. It
accepts the following values:

  * `true`, where the generation stops as soon as there are `num_beams` complete candidates;
  * `false`, where an heuristic is applied and the generation stops when is it very unlikely to find better candidates;
  * `"never"`, where the beam search procedure only stops when there cannot be better candidates (canonical beam search algorithm).

**Kind** : instance property of `GenerationConfig`  
**Default** : `false`

* * *

###  generationConfig.max_time : <code> number </code>

The maximum amount of time you allow the computation to run for in seconds.
Generation will still finish the current pass after allocated time has been
passed.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.do_sample : <code> boolean </code>

Whether or not to use sampling; use greedy decoding otherwise.

**Kind** : instance property of `GenerationConfig`  
**Default** : `false`

* * *

###  generationConfig.num_beams : <code> number </code>

Number of beams for beam search. 1 means no beam search.

**Kind** : instance property of `GenerationConfig`  
**Default** : `1`

* * *

###  generationConfig.num_beam_groups : <code> number </code>

Number of groups to divide `num_beams` into in order to ensure diversity among
different groups of beams. See [this
paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.

**Kind** : instance property of `GenerationConfig`  
**Default** : `1`

* * *

###  generationConfig.penalty_alpha : <code> number </code>

The values balance the model confidence and the degeneration penalty in
contrastive search decoding.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.use_cache : <code> boolean </code>

Whether or not the model should use the past last key/values attentions (if
applicable to the model) to speed up decoding.

**Kind** : instance property of `GenerationConfig`  
**Default** : `true`

* * *

###  generationConfig.temperature : <code> number </code>

The value used to modulate the next token probabilities.

**Kind** : instance property of `GenerationConfig`  
**Default** : `1.0`

* * *

###  generationConfig.top_k : <code> number </code>

The number of highest probability vocabulary tokens to keep for top-k-
filtering.

**Kind** : instance property of `GenerationConfig`  
**Default** : `50`

* * *

###  generationConfig.top_p : <code> number </code>

If set to float < 1, only the smallest set of most probable tokens with
probabilities that add up to `top_p` or higher are kept for generation.

**Kind** : instance property of `GenerationConfig`  
**Default** : `1.0`

* * *

###  generationConfig.typical_p : <code> number </code>

Local typicality measures how similar the conditional probability of
predicting a target token next is to the expected conditional probability of
predicting a random token next, given the partial text already generated. If
set to float < 1, the smallest set of the most locally typical tokens with
probabilities that add up to `typical_p` or higher are kept for generation.
See [this paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.

**Kind** : instance property of `GenerationConfig`  
**Default** : `1.0`

* * *

###  generationConfig.epsilon_cutoff : <code> number </code>

If set to float strictly between 0 and 1, only tokens with a conditional
probability greater than `epsilon_cutoff` will be sampled. In the paper,
suggested values range from 3e-4 to 9e-4, depending on the size of the model.
See [Truncation Sampling as Language Model
Desmoothing](https://arxiv.org/abs/2210.15191) for more details.

**Kind** : instance property of `GenerationConfig`  
**Default** : `0.0`

* * *

###  generationConfig.eta_cutoff : <code> number </code>

Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If
set to float strictly between 0 and 1, a token is only considered if it is
greater than either `eta_cutoff` or `sqrt(eta_cutoff) *
exp(-entropy(softmax(next_token_logits)))`. The latter term is intuitively the
expected next token probability, scaled by `sqrt(eta_cutoff)`. In the paper,
suggested values range from 3e-4 to 2e-3, depending on the size of the model.
See [Truncation Sampling as Language Model
Desmoothing](https://arxiv.org/abs/2210.15191) for more details.

**Kind** : instance property of `GenerationConfig`  
**Default** : `0.0`

* * *

###  generationConfig.diversity_penalty : <code> number </code>

This value is subtracted from a beam‚Äôs score if it generates a token same as
any beam from other group at a particular time. Note that `diversity_penalty`
is only effective if `group beam search` is enabled.

**Kind** : instance property of `GenerationConfig`  
**Default** : `0.0`

* * *

###  generationConfig.repetition_penalty : <code> number </code>

The parameter for repetition penalty. 1.0 means no penalty. See [this
paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.

**Kind** : instance property of `GenerationConfig`  
**Default** : `1.0`

* * *

###  generationConfig.encoder_repetition_penalty : <code> number </code>

The paramater for encoder_repetition_penalty. An exponential penalty on
sequences that are not in the original input. 1.0 means no penalty.

**Kind** : instance property of `GenerationConfig`  
**Default** : `1.0`

* * *

###  generationConfig.length_penalty : <code> number </code>

Exponential penalty to the length that is used with beam-based generation. It
is applied as an exponent to the sequence length, which in turn is used to
divide the score of the sequence. Since the score is the log likelihood of the
sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences,
while `length_penalty` < 0.0 encourages shorter sequences.

**Kind** : instance property of `GenerationConfig`  
**Default** : `1.0`

* * *

###  generationConfig.no_repeat_ngram_size : <code> number </code>

If set to int > 0, all ngrams of that size can only occur once.

**Kind** : instance property of `GenerationConfig`  
**Default** : `0`

* * *

###  generationConfig.bad_words_ids : <code> Array. < Array < number > >
</code>

List of token ids that are not allowed to be generated. In order to get the
token ids of the words that should not appear in the generated text, use
`tokenizer(bad_words, { add_prefix_space: true, add_special_tokens: false
}).input_ids`.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.force_words_ids : <code> Array < Array < number > > </code> | <code> Array < Array < Array < number > > > </code>

List of token ids that must be generated. If given a `number[][]`, this is
treated as a simple list of words that must be included, the opposite to
`bad_words_ids`. If given `number[][][]`, this triggers a [disjunctive
constraint](https://github.com/huggingface/transformers/issues/14081), where
one can allow different forms of each word.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.renormalize_logits : <code> boolean </code>

Whether to renormalize the logits after applying all the logits processors or
warpers (including the custom ones). It‚Äôs highly recommended to set this flag
to `true` as the search algorithms suppose the score logits are normalized but
some logit processors or warpers break the normalization.

**Kind** : instance property of `GenerationConfig`  
**Default** : `false`

* * *

###  generationConfig.constraints : <code> Array. < Object > </code>

Custom constraints that can be added to the generation to ensure that the
output will contain the use of certain tokens as defined by `Constraint`
objects, in the most sensible way possible.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.forced_bos_token_id : <code> number </code>

The id of the token to force as the first generated token after the
`decoder_start_token_id`. Useful for multilingual models like mBART where the
first generated token needs to be the target language token.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.forced_eos_token_id : <code> number </code> | <code> Array < number > </code>

The id of the token to force as the last generated token when `max_length` is
reached. Optionally, use a list to set multiple _end-of-sequence_ tokens.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.remove_invalid_values : <code> boolean </code>

Whether to remove possible _nan_ and _inf_ outputs of the model to prevent the
generation method to crash. Note that using `remove_invalid_values` can slow
down generation.

**Kind** : instance property of `GenerationConfig`

* * *

###  generationConfig.exponential_decay_length_penalty : <code> * </code>

This Tuple adds an exponentially increasing length penalty, after a certain
amount of tokens have been generated. The tuple shall consist of:
`(start_index, decay_factor)` where `start_index` indicates where penalty
starts and `decay_factor` represents the factor of exponential decay.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.suppress_tokens : <code> Array. < number > </code>

A list of tokens that will be suppressed at generation. The `SuppressTokens`
logit processor will set their log probs to `-inf` so that they are not
sampled.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.begin_suppress_tokens : <code> Array. < number > </code>

A list of tokens that will be suppressed at the beginning of the generation.
The `SuppressBeginTokens` logit processor will set their log probs to `-inf`
so that they are not sampled.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.forced_decoder_ids : <code> * </code>

A list of pairs of integers which indicates a mapping from generation indices
to token indices that will be forced before sampling. For example, `[[1,
123]]` means the second generated token will always be a token of index 123.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.guidance_scale : <code> number </code>

The guidance scale for classifier free guidance (CFG). CFG is enabled by
setting `guidance_scale > 1`. Higher guidance scale encourages the model to
generate samples that are more closely linked to the input prompt, usually at
the expense of poorer quality.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.num_return_sequences : <code> number </code>

The number of independently computed returned sequences for each element in
the batch.

**Kind** : instance property of `GenerationConfig`  
**Default** : `1`

* * *

###  generationConfig.output_attentions : <code> boolean </code>

Whether or not to return the attentions tensors of all attention layers. See
`attentions` under returned tensors for more details.

**Kind** : instance property of `GenerationConfig`  
**Default** : `false`

* * *

###  generationConfig.output_hidden_states : <code> boolean </code>

Whether or not to return the hidden states of all layers. See `hidden_states`
under returned tensors for more details.

**Kind** : instance property of `GenerationConfig`  
**Default** : `false`

* * *

###  generationConfig.output_scores : <code> boolean </code>

Whether or not to return the prediction scores. See `scores` under returned
tensors for more details.

**Kind** : instance property of `GenerationConfig`  
**Default** : `false`

* * *

###  generationConfig.return_dict_in_generate : <code> boolean </code>

Whether or not to return a `ModelOutput` instead of a plain tuple.

**Kind** : instance property of `GenerationConfig`  
**Default** : `false`

* * *

###  generationConfig.pad_token_id : <code> number </code>

The id of the _padding_ token.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.bos_token_id : <code> number </code>

The id of the _beginning-of-sequence_ token.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.eos_token_id : <code> number </code> | <code> Array < number > </code>

The id of the _end-of-sequence_ token. Optionally, use a list to set multiple
_end-of-sequence_ tokens.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.encoder_no_repeat_ngram_size : <code> number </code>

If set to int > 0, all ngrams of that size that occur in the
`encoder_input_ids` cannot occur in the `decoder_input_ids`.

**Kind** : instance property of `GenerationConfig`  
**Default** : `0`

* * *

###  generationConfig.decoder_start_token_id : <code> number </code>

If an encoder-decoder model starts decoding with a different token than _bos_
, the id of that token.

**Kind** : instance property of `GenerationConfig`  
**Default** : `null`

* * *

###  generationConfig.generation_kwargs : <code> Object </code>

Additional generation kwargs will be forwarded to the `generate` function of
the model. Kwargs that are not present in `generate`‚Äôs signature will be used
in the model forward pass.

**Kind** : instance property of `GenerationConfig`  
**Default** : `{}`

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/generation/configuration_utils.md)

[‚ÜêParameters](/docs/transformers.js/v3.0.0/en/api/generation/parameters)
[Logits
Processors‚Üí](/docs/transformers.js/v3.0.0/en/api/generation/logits_process)

generation/configuration_utils generation/configuration_utils.GenerationConfig new GenerationConfig(config) generationConfig.max_length : ` number ` generationConfig.max_new_tokens : ` number ` generationConfig.min_length : ` number ` generationConfig.min_new_tokens : ` number ` generationConfig.early_stopping : ` boolean ` | ` ‚Äù never ‚Äù ` generationConfig.max_time : ` number ` generationConfig.do_sample : ` boolean ` generationConfig.num_beams : ` number ` generationConfig.num_beam_groups : ` number ` generationConfig.penalty_alpha : ` number ` generationConfig.use_cache : ` boolean ` generationConfig.temperature : ` number ` generationConfig.top_k : ` number ` generationConfig.top_p : ` number ` generationConfig.typical_p : ` number ` generationConfig.epsilon_cutoff : ` number ` generationConfig.eta_cutoff : ` number ` generationConfig.diversity_penalty : ` number ` generationConfig.repetition_penalty : ` number ` generationConfig.encoder_repetition_penalty : ` number ` generationConfig.length_penalty : ` number ` generationConfig.no_repeat_ngram_size : ` number ` generationConfig.bad_words_ids : ` Array. < Array < number > > ` generationConfig.force_words_ids : ` Array < Array < number > > ` | ` Array < Array < Array < number > > > ` generationConfig.renormalize_logits : ` boolean ` generationConfig.constraints : ` Array. < Object > ` generationConfig.forced_bos_token_id : ` number ` generationConfig.forced_eos_token_id : ` number ` | ` Array < number > ` generationConfig.remove_invalid_values : ` boolean ` generationConfig.exponential_decay_length_penalty : ` * ` generationConfig.suppress_tokens : ` Array. < number > ` generationConfig.begin_suppress_tokens : ` Array. < number > ` generationConfig.forced_decoder_ids : ` * ` generationConfig.guidance_scale : ` number ` generationConfig.num_return_sequences : ` number ` generationConfig.output_attentions : ` boolean ` generationConfig.output_hidden_states : ` boolean ` generationConfig.output_scores : ` boolean ` generationConfig.return_dict_in_generate : ` boolean ` generationConfig.pad_token_id : ` number ` generationConfig.bos_token_id : ` number ` generationConfig.eos_token_id : ` number ` | ` Array < number > ` generationConfig.encoder_no_repeat_ngram_size : ` number ` generationConfig.decoder_start_token_id : ` number ` generationConfig.generation_kwargs : ` Object `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

generation/logits_process

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

[Parameters
](/docs/transformers.js/v3.0.0/en/api/generation/parameters)[Configuration
](/docs/transformers.js/v3.0.0/en/api/generation/configuration_utils)[Logits
Processors
](/docs/transformers.js/v3.0.0/en/api/generation/logits_process)[Logits
Samplers
](/docs/transformers.js/v3.0.0/en/api/generation/logits_sampler)[Stopping
Criteria
](/docs/transformers.js/v3.0.0/en/api/generation/stopping_criteria)[Streamers
](/docs/transformers.js/v3.0.0/en/api/generation/streamers)

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  generation/logits_process

  * generation/logits_process
    * .LogitsProcessor
      *  _`._call(input_ids, logits)`_
    * .LogitsWarper
      *  _`._call(input_ids, logits)`_
    * .LogitsProcessorList
      * `new LogitsProcessorList()`
      * `.push(item)`
      * `.extend(items)`
      * `._call(input_ids, logits)`
    * .ForcedBOSTokenLogitsProcessor
      * `new ForcedBOSTokenLogitsProcessor(bos_token_id)`
      * `._call(input_ids, logits)` ‚áí `Object`
    * .ForcedEOSTokenLogitsProcessor
      * `new ForcedEOSTokenLogitsProcessor(max_length, eos_token_id)`
      * `._call(input_ids, logits)`
    * .SuppressTokensAtBeginLogitsProcessor
      * `new SuppressTokensAtBeginLogitsProcessor(begin_suppress_tokens, begin_index)`
      * `._call(input_ids, logits)` ‚áí `Object`
    * .WhisperTimeStampLogitsProcessor
      * `new WhisperTimeStampLogitsProcessor(generate_config, init_tokens)`
      * `._call(input_ids, logits)` ‚áí `Tensor`
    * .NoRepeatNGramLogitsProcessor
      * `new NoRepeatNGramLogitsProcessor(no_repeat_ngram_size)`
      * `.getNgrams(prevInputIds)` ‚áí `Map.<string, Array<number>>`
      * `.getGeneratedNgrams(bannedNgrams, prevInputIds)` ‚áí `Array.<number>`
      * `.calcBannedNgramTokens(prevInputIds)` ‚áí `Array.<number>`
      * `._call(input_ids, logits)` ‚áí `Object`
    * .RepetitionPenaltyLogitsProcessor
      * `new RepetitionPenaltyLogitsProcessor(penalty)`
      * `._call(input_ids, logits)` ‚áí `Object`
    * .MinLengthLogitsProcessor
      * `new MinLengthLogitsProcessor(min_length, eos_token_id)`
      * `._call(input_ids, logits)` ‚áí `Object`
    * .MinNewTokensLengthLogitsProcessor
      * `new MinNewTokensLengthLogitsProcessor(prompt_length_to_skip, min_new_tokens, eos_token_id)`
      * `._call(input_ids, logits)` ‚áí `Object`
    * .NoBadWordsLogitsProcessor
      * `new NoBadWordsLogitsProcessor(bad_words_ids, eos_token_id)`
      * `._call(input_ids, logits)` ‚áí `Object`
    * .ClassifierFreeGuidanceLogitsProcessor
      * `new ClassifierFreeGuidanceLogitsProcessor(guidance_scale)`
      * `._call(input_ids, logits)` ‚áí `Object`
    * .TemperatureLogitsWarper
      * `new TemperatureLogitsWarper(temperature)`
      * `._call(input_ids, logits)` ‚áí `Object`
    * .TopPLogitsWarper
      * `new TopPLogitsWarper(top_p, options)`
    * .TopKLogitsWarper
      * `new TopKLogitsWarper(top_k, options)`

* * *

##  generation/logits_process.LogitsProcessor

Abstract base class for all logit processors that can be applied during
generation.

**Kind** : static class of `generation/logits_process`

* * *

###  logitsProcessor._call(input_ids, logits)

Apply the processor to the input logits.

**Kind** : instance abstract method of `LogitsProcessor`  
**Throws** :

  * `Error` Throws an error if `_call` is not implemented in the subclass.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input ids.  
logits| `Tensor`| The logits to process.  
  
* * *

##  generation/logits_process.LogitsWarper

Abstract base class for all logit warpers that can be applied during
generation with multinomial sampling.

**Kind** : static class of `generation/logits_process`

* * *

###  logitsWarper._call(input_ids, logits)

Apply the processor to the input logits.

**Kind** : instance abstract method of `LogitsWarper`  
**Throws** :

  * `Error` Throws an error if `_call` is not implemented in the subclass.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input ids.  
logits| `Tensor`| The logits to process.  
  
* * *

##  generation/logits_process.LogitsProcessorList

A class representing a list of logits processors. A logits processor is a
function that modifies the logits output of a language model. This class
provides methods for adding new processors and applying all processors to a
batch of logits.

**Kind** : static class of `generation/logits_process`

  * .LogitsProcessorList
    * `new LogitsProcessorList()`
    * `.push(item)`
    * `.extend(items)`
    * `._call(input_ids, logits)`

* * *

###  new LogitsProcessorList()

Constructs a new instance of `LogitsProcessorList`.

* * *

###  logitsProcessorList.push(item)

Adds a new logits processor to the list.

**Kind** : instance method of `LogitsProcessorList`

Param| Type| Description  
---|---|---  
item| `LogitsProcessor`| The logits processor function to add.  
  
* * *

###  logitsProcessorList.extend(items)

Adds multiple logits processors to the list.

**Kind** : instance method of `LogitsProcessorList`

Param| Type| Description  
---|---|---  
items| `Array.<LogitsProcessor>`| The logits processor functions to add.  
  
* * *

###  logitsProcessorList._call(input_ids, logits)

Applies all logits processors in the list to a batch of logits, modifying them
in-place.

**Kind** : instance method of `LogitsProcessorList`

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs for the language model.  
logits| `Tensor`|  
  
* * *

##  generation/logits_process.ForcedBOSTokenLogitsProcessor

A LogitsProcessor that forces a BOS token at the beginning of the generated
sequence.

**Kind** : static class of `generation/logits_process`

  * .ForcedBOSTokenLogitsProcessor
    * `new ForcedBOSTokenLogitsProcessor(bos_token_id)`
    * `._call(input_ids, logits)` ‚áí `Object`

* * *

###  new ForcedBOSTokenLogitsProcessor(bos_token_id)

Create a ForcedBOSTokenLogitsProcessor.

Param| Type| Description  
---|---|---  
bos_token_id| `number`| The ID of the beginning-of-sequence token to be
forced.  
  
* * *

###  forcedBOSTokenLogitsProcessor._call(input_ids, logits) ‚áí <code> Object
</code>

Apply the BOS token forcing to the logits.

**Kind** : instance method of `ForcedBOSTokenLogitsProcessor`  
**Returns** : `Object` \- The logits with BOS token forcing.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs.  
logits| `Tensor`| The logits.  
  
* * *

##  generation/logits_process.ForcedEOSTokenLogitsProcessor

A logits processor that enforces the specified token as the last generated
token when `max_length` is reached.

**Kind** : static class of `generation/logits_process`

  * .ForcedEOSTokenLogitsProcessor
    * `new ForcedEOSTokenLogitsProcessor(max_length, eos_token_id)`
    * `._call(input_ids, logits)`

* * *

###  new ForcedEOSTokenLogitsProcessor(max_length, eos_token_id)

Create a ForcedEOSTokenLogitsProcessor.

Param| Type| Description  
---|---|---  
max_length| `number`| The maximum length of the sequence to be generated.  
eos_token_id| `number` | `Array<number>`| The id(s) of the _end-of-sequence_ token.  
  
* * *

###  forcedEOSTokenLogitsProcessor._call(input_ids, logits)

Apply the processor to input_ids and logits.

**Kind** : instance method of `ForcedEOSTokenLogitsProcessor`

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input ids.  
logits| `Tensor`| The logits tensor.  
  
* * *

##  generation/logits_process.SuppressTokensAtBeginLogitsProcessor

A LogitsProcessor that suppresses a list of tokens as soon as the `generate`
function starts generating using `begin_index` tokens. This should ensure that
the tokens defined by `begin_suppress_tokens` at not sampled at the begining
of the generation.

**Kind** : static class of `generation/logits_process`

  * .SuppressTokensAtBeginLogitsProcessor
    * `new SuppressTokensAtBeginLogitsProcessor(begin_suppress_tokens, begin_index)`
    * `._call(input_ids, logits)` ‚áí `Object`

* * *

###  new SuppressTokensAtBeginLogitsProcessor(begin_suppress_tokens,
begin_index)

Create a SuppressTokensAtBeginLogitsProcessor.

Param| Type| Description  
---|---|---  
begin_suppress_tokens| `Array.<number>`| The IDs of the tokens to suppress.  
begin_index| `number`| The number of tokens to generate before suppressing
tokens.  
  
* * *

###  suppressTokensAtBeginLogitsProcessor._call(input_ids, logits) ‚áí <code>
Object </code>

Apply the BOS token forcing to the logits.

**Kind** : instance method of `SuppressTokensAtBeginLogitsProcessor`  
**Returns** : `Object` \- The logits with BOS token forcing.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs.  
logits| `Tensor`| The logits.  
  
* * *

##  generation/logits_process.WhisperTimeStampLogitsProcessor

A LogitsProcessor that handles adding timestamps to generated text.

**Kind** : static class of `generation/logits_process`

  * .WhisperTimeStampLogitsProcessor
    * `new WhisperTimeStampLogitsProcessor(generate_config, init_tokens)`
    * `._call(input_ids, logits)` ‚áí `Tensor`

* * *

###  new WhisperTimeStampLogitsProcessor(generate_config, init_tokens)

Constructs a new WhisperTimeStampLogitsProcessor.

Param| Type| Description  
---|---|---  
generate_config| `*`| The config object passed to the `generate()` method of a
transformer model.  
init_tokens| `Array.<number>`| The initial tokens of the input sequence.  
  
* * *

###  whisperTimeStampLogitsProcessor._call(input_ids, logits) ‚áí <code> Tensor
</code>

Modify the logits to handle timestamp tokens.

**Kind** : instance method of `WhisperTimeStampLogitsProcessor`  
**Returns** : `Tensor` \- The modified logits.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input sequence of tokens.  
logits| `Tensor`| The logits output by the model.  
  
* * *

##  generation/logits_process.NoRepeatNGramLogitsProcessor

A logits processor that disallows ngrams of a certain size to be repeated.

**Kind** : static class of `generation/logits_process`

  * .NoRepeatNGramLogitsProcessor
    * `new NoRepeatNGramLogitsProcessor(no_repeat_ngram_size)`
    * `.getNgrams(prevInputIds)` ‚áí `Map.<string, Array<number>>`
    * `.getGeneratedNgrams(bannedNgrams, prevInputIds)` ‚áí `Array.<number>`
    * `.calcBannedNgramTokens(prevInputIds)` ‚áí `Array.<number>`
    * `._call(input_ids, logits)` ‚áí `Object`

* * *

###  new NoRepeatNGramLogitsProcessor(no_repeat_ngram_size)

Create a NoRepeatNGramLogitsProcessor.

Param| Type| Description  
---|---|---  
no_repeat_ngram_size| `number`| The no-repeat-ngram size. All ngrams of this
size can only occur once.  
  
* * *

###  noRepeatNGramLogitsProcessor.getNgrams(prevInputIds) ‚áí <code> Map. <
string, Array < number > > </code>

Generate n-grams from a sequence of token ids.

**Kind** : instance method of `NoRepeatNGramLogitsProcessor`  
**Returns** : `Map.<string, Array<number>>` \- Map of generated n-grams

Param| Type| Description  
---|---|---  
prevInputIds| `Array.<bigint>`| List of previous input ids  
  
* * *

###  noRepeatNGramLogitsProcessor.getGeneratedNgrams(bannedNgrams,
prevInputIds) ‚áí <code> Array. < number > </code>

Generate n-grams from a sequence of token ids.

**Kind** : instance method of `NoRepeatNGramLogitsProcessor`  
**Returns** : `Array.<number>` \- Map of generated n-grams

Param| Type| Description  
---|---|---  
bannedNgrams| `Map.<string, Array<number>>`| Map of banned n-grams  
prevInputIds| `Array.<bigint>`| List of previous input ids  
  
* * *

###  noRepeatNGramLogitsProcessor.calcBannedNgramTokens(prevInputIds) ‚áí <code>
Array. < number > </code>

Calculate banned n-gram tokens

**Kind** : instance method of `NoRepeatNGramLogitsProcessor`  
**Returns** : `Array.<number>` \- Map of generated n-grams

Param| Type| Description  
---|---|---  
prevInputIds| `Array.<bigint>`| List of previous input ids  
  
* * *

###  noRepeatNGramLogitsProcessor._call(input_ids, logits) ‚áí <code> Object
</code>

Apply the no-repeat-ngram processor to the logits.

**Kind** : instance method of `NoRepeatNGramLogitsProcessor`  
**Returns** : `Object` \- The logits with no-repeat-ngram processing.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs.  
logits| `Tensor`| The logits.  
  
* * *

##  generation/logits_process.RepetitionPenaltyLogitsProcessor

A logits processor that penalises repeated output tokens.

**Kind** : static class of `generation/logits_process`

  * .RepetitionPenaltyLogitsProcessor
    * `new RepetitionPenaltyLogitsProcessor(penalty)`
    * `._call(input_ids, logits)` ‚áí `Object`

* * *

###  new RepetitionPenaltyLogitsProcessor(penalty)

Create a RepetitionPenaltyLogitsProcessor.

Param| Type| Description  
---|---|---  
penalty| `number`| The penalty to apply for repeated tokens.  
  
* * *

###  repetitionPenaltyLogitsProcessor._call(input_ids, logits) ‚áí <code> Object
</code>

Apply the repetition penalty to the logits.

**Kind** : instance method of `RepetitionPenaltyLogitsProcessor`  
**Returns** : `Object` \- The logits with repetition penalty processing.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs.  
logits| `Tensor`| The logits.  
  
* * *

##  generation/logits_process.MinLengthLogitsProcessor

A logits processor that enforces a minimum number of tokens.

**Kind** : static class of `generation/logits_process`

  * .MinLengthLogitsProcessor
    * `new MinLengthLogitsProcessor(min_length, eos_token_id)`
    * `._call(input_ids, logits)` ‚áí `Object`

* * *

###  new MinLengthLogitsProcessor(min_length, eos_token_id)

Create a MinLengthLogitsProcessor.

Param| Type| Description  
---|---|---  
min_length| `number`| The minimum length below which the score of
`eos_token_id` is set to negative infinity.  
eos_token_id| `number` | `Array<number>`| The ID/IDs of the end-of-sequence token.  
  
* * *

###  minLengthLogitsProcessor._call(input_ids, logits) ‚áí <code> Object </code>

Apply logit processor.

**Kind** : instance method of `MinLengthLogitsProcessor`  
**Returns** : `Object` \- The processed logits.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs.  
logits| `Tensor`| The logits.  
  
* * *

##  generation/logits_process.MinNewTokensLengthLogitsProcessor

A logits processor that enforces a minimum number of new tokens.

**Kind** : static class of `generation/logits_process`

  * .MinNewTokensLengthLogitsProcessor
    * `new MinNewTokensLengthLogitsProcessor(prompt_length_to_skip, min_new_tokens, eos_token_id)`
    * `._call(input_ids, logits)` ‚áí `Object`

* * *

###  new MinNewTokensLengthLogitsProcessor(prompt_length_to_skip,
min_new_tokens, eos_token_id)

Create a MinNewTokensLengthLogitsProcessor.

Param| Type| Description  
---|---|---  
prompt_length_to_skip| `number`| The input tokens length.  
min_new_tokens| `number`| The minimum _new_ tokens length below which the
score of `eos_token_id` is set to negative infinity.  
eos_token_id| `number` | `Array<number>`| The ID/IDs of the end-of-sequence token.  
  
* * *

###  minNewTokensLengthLogitsProcessor._call(input_ids, logits) ‚áí <code>
Object </code>

Apply logit processor.

**Kind** : instance method of `MinNewTokensLengthLogitsProcessor`  
**Returns** : `Object` \- The processed logits.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs.  
logits| `Tensor`| The logits.  
  
* * *

##  generation/logits_process.NoBadWordsLogitsProcessor

**Kind** : static class of `generation/logits_process`

  * .NoBadWordsLogitsProcessor
    * `new NoBadWordsLogitsProcessor(bad_words_ids, eos_token_id)`
    * `._call(input_ids, logits)` ‚áí `Object`

* * *

###  new NoBadWordsLogitsProcessor(bad_words_ids, eos_token_id)

Create a `NoBadWordsLogitsProcessor`.

Param| Type| Description  
---|---|---  
bad_words_ids| `Array.<Array<number>>`| List of list of token ids that are not
allowed to be generated.  
eos_token_id| `number` | `Array<number>`| The id of the _end-of-sequence_ token. Optionally, use a list to set multiple _end-of-sequence_ tokens.  
  
* * *

###  noBadWordsLogitsProcessor._call(input_ids, logits) ‚áí <code> Object
</code>

Apply logit processor.

**Kind** : instance method of `NoBadWordsLogitsProcessor`  
**Returns** : `Object` \- The processed logits.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs.  
logits| `Tensor`| The logits.  
  
* * *

##  generation/logits_process.ClassifierFreeGuidanceLogitsProcessor

[`LogitsProcessor`] for classifier free guidance (CFG). The scores are split
over the batch dimension, where the first half correspond to the conditional
logits (predicted from the input prompt) and the second half correspond to the
unconditional logits (predicted from an empty or ‚Äònull‚Äô prompt). The processor
computes a weighted average across the conditional and unconditional logits,
parameterised by the `guidance_scale`.

See [the paper](https://arxiv.org/abs/2306.05284) for more information.

**Kind** : static class of `generation/logits_process`

  * .ClassifierFreeGuidanceLogitsProcessor
    * `new ClassifierFreeGuidanceLogitsProcessor(guidance_scale)`
    * `._call(input_ids, logits)` ‚áí `Object`

* * *

###  new ClassifierFreeGuidanceLogitsProcessor(guidance_scale)

Create a `ClassifierFreeGuidanceLogitsProcessor`.

Param| Type| Description  
---|---|---  
guidance_scale| `number`| The guidance scale for classifier free guidance
(CFG). CFG is enabled by setting `guidance_scale > 1`. Higher guidance scale
encourages the model to generate samples that are more closely linked to the
input prompt, usually at the expense of poorer quality.  
  
* * *

###  classifierFreeGuidanceLogitsProcessor._call(input_ids, logits) ‚áí <code>
Object </code>

Apply logit processor.

**Kind** : instance method of `ClassifierFreeGuidanceLogitsProcessor`  
**Returns** : `Object` \- The processed logits.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs.  
logits| `Tensor`| The logits.  
  
* * *

##  generation/logits_process.TemperatureLogitsWarper

[`LogitsWarper`] for temperature (exponential scaling output probability
distribution), which effectively means that it can control the randomness of
the predicted tokens. Often used together with [`TopPLogitsWarper`] and
[`TopKLogitsWarper`].

**Kind** : static class of `generation/logits_process`

  * .TemperatureLogitsWarper
    * `new TemperatureLogitsWarper(temperature)`
    * `._call(input_ids, logits)` ‚áí `Object`

* * *

###  new TemperatureLogitsWarper(temperature)

Create a `TemperatureLogitsWarper`.

Param| Type| Description  
---|---|---  
temperature| `number`| Strictly positive float value used to modulate the
logits distribution. A value smaller than `1` decreases randomness (and vice
versa), with `0` being equivalent to shifting all probability mass to the most
likely token.  
  
* * *

###  temperatureLogitsWarper._call(input_ids, logits) ‚áí <code> Object </code>

Apply logit warper.

**Kind** : instance method of `TemperatureLogitsWarper`  
**Returns** : `Object` \- The processed logits.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<bigint>>`| The input IDs.  
logits| `Tensor`| The logits.  
  
* * *

##  generation/logits_process.TopPLogitsWarper

[`LogitsWarper`] that performs top-p, i.e. restricting to top tokens summing
to prob_cut_off <= prob_cut_off. Often used together with
[`TemperatureLogitsWarper`] and [`TopKLogitsWarper`].

**Kind** : static class of `generation/logits_process`

* * *

###  new TopPLogitsWarper(top_p, options)

Create a `TopPLogitsWarper`.

Param| Type| Default| Description  
---|---|---|---  
top_p| `number`| | If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or higher are kept for generation.  
options| `Object`| | Additional options for the top-p sampling.  
[options.filter_value]| `number`| `-Infinity`| All filtered values will be set
to this float value.  
[options.min_tokens_to_keep]| `number`| `1`| Minimum number of tokens that
cannot be filtered.  
  
* * *

##  generation/logits_process.TopKLogitsWarper

[`LogitsWarper`] that performs top-k, i.e. restricting to the k highest
probability elements. Often used together with [`TemperatureLogitsWarper`] and
[`TopPLogitsWarper`].

**Kind** : static class of `generation/logits_process`

* * *

###  new TopKLogitsWarper(top_k, options)

Create a `TopKLogitsWarper`.

Param| Type| Default| Description  
---|---|---|---  
top_k| `number`| | If set to > 0, only the top `top_k` tokens are kept for generation.  
options| `Object`| | Additional options for the top-k sampling.  
[options.filter_value]| `number`| `-Infinity`| All filtered values will be set
to this float value.  
[options.min_tokens_to_keep]| `number`| `1`| Minimum number of tokens that
cannot be filtered.  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/generation/logits_process.md)

[‚ÜêConfiguration](/docs/transformers.js/v3.0.0/en/api/generation/configuration_utils)
[Logits
Samplers‚Üí](/docs/transformers.js/v3.0.0/en/api/generation/logits_sampler)

generation/logits_process generation/logits_process.LogitsProcessor
logitsProcessor._call(input_ids, logits)
generation/logits_process.LogitsWarper logitsWarper._call(input_ids, logits)
generation/logits_process.LogitsProcessorList new LogitsProcessorList()
logitsProcessorList.push(item) logitsProcessorList.extend(items)
logitsProcessorList._call(input_ids, logits)
generation/logits_process.ForcedBOSTokenLogitsProcessor new
ForcedBOSTokenLogitsProcessor(bos_token_id)
forcedBOSTokenLogitsProcessor._call(input_ids, logits) ‚áí ` Object `
generation/logits_process.ForcedEOSTokenLogitsProcessor new
ForcedEOSTokenLogitsProcessor(max_length, eos_token_id)
forcedEOSTokenLogitsProcessor._call(input_ids, logits)
generation/logits_process.SuppressTokensAtBeginLogitsProcessor new
SuppressTokensAtBeginLogitsProcessor(begin_suppress_tokens, begin_index)
suppressTokensAtBeginLogitsProcessor._call(input_ids, logits) ‚áí ` Object `
generation/logits_process.WhisperTimeStampLogitsProcessor new
WhisperTimeStampLogitsProcessor(generate_config, init_tokens)
whisperTimeStampLogitsProcessor._call(input_ids, logits) ‚áí ` Tensor `
generation/logits_process.NoRepeatNGramLogitsProcessor new
NoRepeatNGramLogitsProcessor(no_repeat_ngram_size)
noRepeatNGramLogitsProcessor.getNgrams(prevInputIds) ‚áí ` Map. < string, Array
< number > > ` noRepeatNGramLogitsProcessor.getGeneratedNgrams(bannedNgrams,
prevInputIds) ‚áí ` Array. < number > `
noRepeatNGramLogitsProcessor.calcBannedNgramTokens(prevInputIds) ‚áí ` Array. <
number > ` noRepeatNGramLogitsProcessor._call(input_ids, logits) ‚áí ` Object `
generation/logits_process.RepetitionPenaltyLogitsProcessor new
RepetitionPenaltyLogitsProcessor(penalty)
repetitionPenaltyLogitsProcessor._call(input_ids, logits) ‚áí ` Object `
generation/logits_process.MinLengthLogitsProcessor new
MinLengthLogitsProcessor(min_length, eos_token_id)
minLengthLogitsProcessor._call(input_ids, logits) ‚áí ` Object `
generation/logits_process.MinNewTokensLengthLogitsProcessor new
MinNewTokensLengthLogitsProcessor(prompt_length_to_skip, min_new_tokens,
eos_token_id) minNewTokensLengthLogitsProcessor._call(input_ids, logits) ‚áí `
Object ` generation/logits_process.NoBadWordsLogitsProcessor new
NoBadWordsLogitsProcessor(bad_words_ids, eos_token_id)
noBadWordsLogitsProcessor._call(input_ids, logits) ‚áí ` Object `
generation/logits_process.ClassifierFreeGuidanceLogitsProcessor new
ClassifierFreeGuidanceLogitsProcessor(guidance_scale)
classifierFreeGuidanceLogitsProcessor._call(input_ids, logits) ‚áí ` Object `
generation/logits_process.TemperatureLogitsWarper new
TemperatureLogitsWarper(temperature) temperatureLogitsWarper._call(input_ids,
logits) ‚áí ` Object ` generation/logits_process.TopPLogitsWarper new
TopPLogitsWarper(top_p, options) generation/logits_process.TopKLogitsWarper
new TopKLogitsWarper(top_k, options)

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

generation/logits_sampler

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

[Parameters
](/docs/transformers.js/v3.0.0/en/api/generation/parameters)[Configuration
](/docs/transformers.js/v3.0.0/en/api/generation/configuration_utils)[Logits
Processors
](/docs/transformers.js/v3.0.0/en/api/generation/logits_process)[Logits
Samplers
](/docs/transformers.js/v3.0.0/en/api/generation/logits_sampler)[Stopping
Criteria
](/docs/transformers.js/v3.0.0/en/api/generation/stopping_criteria)[Streamers
](/docs/transformers.js/v3.0.0/en/api/generation/streamers)

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  generation/logits_sampler

  * generation/logits_sampler
    *  _static_
      * .LogitsSampler
        * `new LogitsSampler(generation_config)`
        * _instance_
          * `._call(logits)` ‚áí `*`
          * `.sample(logits)` ‚áí `*`
          * `.getLogits(logits, index)` ‚áí `Float32Array`
          * `.randomSelect(probabilities)` ‚áí `number`
        * _static_
          * `.getSampler(generation_config)` ‚áí `LogitsSampler`
    * _inner_
      * ~GreedySampler
        * `.sample(logits)` ‚áí `*`
      * ~MultinomialSampler
        * `.sample(logits)` ‚áí `*`
      * ~BeamSearchSampler
        * `.sample(logits)` ‚áí `*`

* * *

##  generation/logits_sampler.LogitsSampler

Sampler is a base class for all sampling methods used for text generation.

**Kind** : static class of `generation/logits_sampler`

  * .LogitsSampler
    * `new LogitsSampler(generation_config)`
    * _instance_
      * `._call(logits)` ‚áí `*`
      * `.sample(logits)` ‚áí `*`
      * `.getLogits(logits, index)` ‚áí `Float32Array`
      * `.randomSelect(probabilities)` ‚áí `number`
    * _static_
      * `.getSampler(generation_config)` ‚áí `LogitsSampler`

* * *

###  new LogitsSampler(generation_config)

Creates a new Sampler object with the specified generation config.

Param| Type| Description  
---|---|---  
generation_config| `GenerationConfig`| The generation config.  
  
* * *

###  logitsSampler._call(logits) ‚áí <code> * </code>

Executes the sampler, using the specified logits.

**Kind** : instance method of `LogitsSampler`

Param| Type  
---|---  
logits| `Tensor`  
  
* * *

###  logitsSampler.sample(logits) ‚áí <code> * </code>

Abstract method for sampling the logits.

**Kind** : instance method of `LogitsSampler`  
**Throws** :

  * `Error` If not implemented in subclass.

Param| Type  
---|---  
logits| `Tensor`  
  
* * *

###  logitsSampler.getLogits(logits, index) ‚áí <code> Float32Array </code>

Returns the specified logits as an array, with temperature applied.

**Kind** : instance method of `LogitsSampler`

Param| Type  
---|---  
logits| `Tensor`  
index| `number`  
  
* * *

###  logitsSampler.randomSelect(probabilities) ‚áí <code> number </code>

Selects an item randomly based on the specified probabilities.

**Kind** : instance method of `LogitsSampler`  
**Returns** : `number` \- The index of the selected item.

Param| Type| Description  
---|---|---  
probabilities| `*`| An array of probabilities to use for selection.  
  
* * *

###  LogitsSampler.getSampler(generation_config) ‚áí <code> LogitsSampler
</code>

Returns a Sampler object based on the specified options.

**Kind** : static method of `LogitsSampler`  
**Returns** : `LogitsSampler` \- A Sampler object.

Param| Type| Description  
---|---|---  
generation_config| `GenerationConfig`| An object containing options for the
sampler.  
  
* * *

##  generation/logits_sampler~GreedySampler

Class representing a Greedy Sampler.

**Kind** : inner class of `generation/logits_sampler`

* * *

###  greedySampler.sample(logits) ‚áí <code> * </code>

Sample the maximum probability of a given logits tensor.

**Kind** : instance method of `GreedySampler`  
**Returns** : `*` \- An array with a single tuple, containing the index of the
maximum value and a meaningless score (since this is a greedy search).

Param| Type  
---|---  
logits| `Tensor`  
  
* * *

##  generation/logits_sampler~MultinomialSampler

Class representing a MultinomialSampler.

**Kind** : inner class of `generation/logits_sampler`

* * *

###  multinomialSampler.sample(logits) ‚áí <code> * </code>

Sample from the logits.

**Kind** : instance method of `MultinomialSampler`

Param| Type  
---|---  
logits| `Tensor`  
  
* * *

##  generation/logits_sampler~BeamSearchSampler

Class representing a BeamSearchSampler.

**Kind** : inner class of `generation/logits_sampler`

* * *

###  beamSearchSampler.sample(logits) ‚áí <code> * </code>

Sample from the logits.

**Kind** : instance method of `BeamSearchSampler`

Param| Type  
---|---  
logits| `Tensor`  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/generation/logits_sampler.md)

[‚ÜêLogits
Processors](/docs/transformers.js/v3.0.0/en/api/generation/logits_process)
[Stopping
Criteria‚Üí](/docs/transformers.js/v3.0.0/en/api/generation/stopping_criteria)

generation/logits_sampler generation/logits_sampler.LogitsSampler new
LogitsSampler(generation_config) logitsSampler._call(logits) ‚áí ` * `
logitsSampler.sample(logits) ‚áí ` * ` logitsSampler.getLogits(logits, index) ‚áí
` Float32Array ` logitsSampler.randomSelect(probabilities) ‚áí ` number `
LogitsSampler.getSampler(generation_config) ‚áí ` LogitsSampler `
generation/logits_sampler~GreedySampler greedySampler.sample(logits) ‚áí ` * `
generation/logits_sampler~MultinomialSampler multinomialSampler.sample(logits)
‚áí ` * ` generation/logits_sampler~BeamSearchSampler
beamSearchSampler.sample(logits) ‚áí ` * `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

generation/parameters

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

[Parameters
](/docs/transformers.js/v3.0.0/en/api/generation/parameters)[Configuration
](/docs/transformers.js/v3.0.0/en/api/generation/configuration_utils)[Logits
Processors
](/docs/transformers.js/v3.0.0/en/api/generation/logits_process)[Logits
Samplers
](/docs/transformers.js/v3.0.0/en/api/generation/logits_sampler)[Stopping
Criteria
](/docs/transformers.js/v3.0.0/en/api/generation/stopping_criteria)[Streamers
](/docs/transformers.js/v3.0.0/en/api/generation/streamers)

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  generation/parameters

* * *

##  generation/parameters~GenerationFunctionParameters : <code> Object </code>

**Kind** : inner typedef of `generation/parameters`

Param| Type| Description  
---|---|---  
[kwargs]| `any`| (`Dict[str, any]`, _optional_):  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
[inputs]| `*`| ``| (`Tensor` of varying shape depending on the modality,
_optional_): The sequence used as a prompt for the generation or as model
inputs to the encoder. If `null` the method initializes it with `bos_token_id`
and a batch size of 1. For decoder-only models `inputs` should be in the
format of `input_ids`. For encoder-decoder models _inputs_ can represent any
of `input_ids`, `input_values`, `input_features`, or `pixel_values`.  
[generation_config]| `*`| ``| (`GenerationConfig`, _optional_): The generation
configuration to be used as base parametrization for the generation call.
`**kwargs` passed to generate matching the attributes of `generation_config`
will override them. If `generation_config` is not provided, the default will
be used, which has the following loading priority:

  * (1) from the `generation_config.json` model file, if it exists;
  * (2) from the model configuration. Please note that unspecified parameters will inherit [`GenerationConfig`]'s default values, whose documentation should be checked to parameterize generation.

  
[logits_processor]| `*`| ``| (`LogitsProcessorList`, _optional_): Custom
logits processors that complement the default logits processors built from
arguments and generation config. If a logit processor is passed that is
already created with the arguments or a generation config an error is thrown.
This feature is intended for advanced users.  
[stopping_criteria]| `*`| ``| (`StoppingCriteriaList`, _optional_): Custom
stopping criteria that complements the default stopping criteria built from
arguments and a generation config. If a stopping criteria is passed that is
already created with the arguments or a generation config an error is thrown.
This feature is intended for advanced users.  
[streamer]| `*`| ``| (`BaseStreamer`, _optional_): Streamer object that will
be used to stream the generated sequences. Generated tokens are passed through
`streamer.put(token_ids)` and the streamer is responsible for any further
processing.  
[decoder_input_ids]| `Array.<number>`| ``| (`number[]`, _optional_): If the
model is an encoder-decoder model, this argument is used to pass the
`decoder_input_ids`.  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/generation/parameters.md)

[‚ÜêONNX](/docs/transformers.js/v3.0.0/en/api/backends/onnx)
[Configuration‚Üí](/docs/transformers.js/v3.0.0/en/api/generation/configuration_utils)

generation/parameters generation/parameters~GenerationFunctionParameters : `
Object `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

generation/stopping_criteria

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

[Parameters
](/docs/transformers.js/v3.0.0/en/api/generation/parameters)[Configuration
](/docs/transformers.js/v3.0.0/en/api/generation/configuration_utils)[Logits
Processors
](/docs/transformers.js/v3.0.0/en/api/generation/logits_process)[Logits
Samplers
](/docs/transformers.js/v3.0.0/en/api/generation/logits_sampler)[Stopping
Criteria
](/docs/transformers.js/v3.0.0/en/api/generation/stopping_criteria)[Streamers
](/docs/transformers.js/v3.0.0/en/api/generation/streamers)

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  generation/stopping_criteria

  * generation/stopping_criteria
    * .StoppingCriteria
      * `._call(input_ids, scores)` ‚áí `Array.<boolean>`
    * .StoppingCriteriaList
      * `new StoppingCriteriaList()`
      * `.push(item)`
      * `.extend(items)`
    * .MaxLengthCriteria
      * `new MaxLengthCriteria(max_length, [max_position_embeddings])`
    * .EosTokenCriteria
      * `new EosTokenCriteria(eos_token_id)`
      * `._call(input_ids, scores)` ‚áí `Array.<boolean>`
    * .InterruptableStoppingCriteria

* * *

##  generation/stopping_criteria.StoppingCriteria

Abstract base class for all stopping criteria that can be applied during
generation.

**Kind** : static class of `generation/stopping_criteria`

* * *

###  stoppingCriteria._call(input_ids, scores) ‚áí <code> Array. < boolean >
</code>

**Kind** : instance method of `StoppingCriteria`  
**Returns** : `Array.<boolean>` \- A list of booleans indicating whether each
sequence should be stopped.

Param| Type| Description  
---|---|---  
input_ids| `Array.<Array<number>>`| (`number[][]` of shape `(batch_size,
sequence_length)`): Indices of input sequence tokens in the vocabulary.  
scores| `Array.<Array<number>>`| scores (`number[][]` of shape `(batch_size,
config.vocab_size)`): Prediction scores of a language modeling head. These can
be scores for each vocabulary token before SoftMax or scores for each
vocabulary token after SoftMax.  
  
* * *

##  generation/stopping_criteria.StoppingCriteriaList

**Kind** : static class of `generation/stopping_criteria`

  * .StoppingCriteriaList
    * `new StoppingCriteriaList()`
    * `.push(item)`
    * `.extend(items)`

* * *

###  new StoppingCriteriaList()

Constructs a new instance of `StoppingCriteriaList`.

* * *

###  stoppingCriteriaList.push(item)

Adds a new stopping criterion to the list.

**Kind** : instance method of `StoppingCriteriaList`

Param| Type| Description  
---|---|---  
item| `StoppingCriteria`| The stopping criterion to add.  
  
* * *

###  stoppingCriteriaList.extend(items)

Adds multiple stopping criteria to the list.

**Kind** : instance method of `StoppingCriteriaList`

Param| Type| Description  
---|---|---  
items| `StoppingCriteria` | `StoppingCriteriaList` | `Array<StoppingCriteria>`| The stopping criteria to add.  
  
* * *

##  generation/stopping_criteria.MaxLengthCriteria

This class can be used to stop generation whenever the full generated number
of tokens exceeds `max_length`. Keep in mind for decoder-only type of
transformers, this will include the initial prompted tokens.

**Kind** : static class of `generation/stopping_criteria`

* * *

###  new MaxLengthCriteria(max_length, [max_position_embeddings])

Param| Type| Default| Description  
---|---|---|---  
max_length| `number`| | The maximum length that the output sequence can have in number of tokens.  
[max_position_embeddings]| `number`| ``| The maximum model length, as defined
by the model's `config.max_position_embeddings` attribute.  
  
* * *

##  generation/stopping_criteria.EosTokenCriteria

This class can be used to stop generation whenever the ‚Äúend-of-sequence‚Äù token
is generated. By default, it uses the `model.generation_config.eos_token_id`.

**Kind** : static class of `generation/stopping_criteria`

  * .EosTokenCriteria
    * `new EosTokenCriteria(eos_token_id)`
    * `._call(input_ids, scores)` ‚áí `Array.<boolean>`

* * *

###  new EosTokenCriteria(eos_token_id)

Param| Type| Description  
---|---|---  
eos_token_id| `number` | `Array<number>`| The id of the _end-of-sequence_ token. Optionally, use a list to set multiple _end-of-sequence_ tokens.  
  
* * *

###  eosTokenCriteria._call(input_ids, scores) ‚áí <code> Array. < boolean >
</code>

**Kind** : instance method of `EosTokenCriteria`

Param| Type  
---|---  
input_ids| `Array.<Array<number>>`  
scores| `Array.<Array<number>>`  
  
* * *

##  generation/stopping_criteria.InterruptableStoppingCriteria

This class can be used to stop generation whenever the user interrupts the
process.

**Kind** : static class of `generation/stopping_criteria`

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/generation/stopping_criteria.md)

[‚ÜêLogits
Samplers](/docs/transformers.js/v3.0.0/en/api/generation/logits_sampler)
[Streamers‚Üí](/docs/transformers.js/v3.0.0/en/api/generation/streamers)

generation/stopping_criteria generation/stopping_criteria.StoppingCriteria
stoppingCriteria._call(input_ids, scores) ‚áí ` Array. < boolean > `
generation/stopping_criteria.StoppingCriteriaList new StoppingCriteriaList()
stoppingCriteriaList.push(item) stoppingCriteriaList.extend(items)
generation/stopping_criteria.MaxLengthCriteria new
MaxLengthCriteria(max_length, [max_position_embeddings])
generation/stopping_criteria.EosTokenCriteria new
EosTokenCriteria(eos_token_id) eosTokenCriteria._call(input_ids, scores) ‚áí `
Array. < boolean > `
generation/stopping_criteria.InterruptableStoppingCriteria

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

generation/streamers

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

[Parameters
](/docs/transformers.js/v3.0.0/en/api/generation/parameters)[Configuration
](/docs/transformers.js/v3.0.0/en/api/generation/configuration_utils)[Logits
Processors
](/docs/transformers.js/v3.0.0/en/api/generation/logits_process)[Logits
Samplers
](/docs/transformers.js/v3.0.0/en/api/generation/logits_sampler)[Stopping
Criteria
](/docs/transformers.js/v3.0.0/en/api/generation/stopping_criteria)[Streamers
](/docs/transformers.js/v3.0.0/en/api/generation/streamers)

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  generation/streamers

  * generation/streamers
    * .TextStreamer
      * `new TextStreamer(tokenizer)`
      * `.put(value)`
      * `.end()`
      * `.on_finalized_text(text, stream_end)`
    * .WhisperTextStreamer
      * `new WhisperTextStreamer(tokenizer, options)`
      * `.put(value)`

* * *

##  generation/streamers.TextStreamer

Simple text streamer that prints the token(s) to stdout as soon as entire
words are formed.

**Kind** : static class of `generation/streamers`

  * .TextStreamer
    * `new TextStreamer(tokenizer)`
    * `.put(value)`
    * `.end()`
    * `.on_finalized_text(text, stream_end)`

* * *

###  new TextStreamer(tokenizer)

Param| Type  
---|---  
tokenizer| `*`  
  
* * *

###  textStreamer.put(value)

Receives tokens, decodes them, and prints them to stdout as soon as they form
entire words.

**Kind** : instance method of `TextStreamer`

Param| Type  
---|---  
value| `Array.<Array<bigint>>`  
  
* * *

###  textStreamer.end()

Flushes any remaining cache and prints a newline to stdout.

**Kind** : instance method of `TextStreamer`

* * *

###  textStreamer.on_finalized_text(text, stream_end)

Prints the new text to stdout. If the stream is ending, also prints a newline.

**Kind** : instance method of `TextStreamer`

Param| Type  
---|---  
text| `string`  
stream_end| `boolean`  
  
* * *

##  generation/streamers.WhisperTextStreamer

Utility class to handle streaming of tokens generated by whisper speech-to-
text models. Callback functions are invoked when each of the following events
occur:

  * A new chunk starts (on_chunk_start)
  * A new token is generated (callback_function)
  * A chunk ends (on_chunk_end)
  * The stream is finalized (on_finalize)

**Kind** : static class of `generation/streamers`

  * .WhisperTextStreamer
    * `new WhisperTextStreamer(tokenizer, options)`
    * `.put(value)`

* * *

###  new WhisperTextStreamer(tokenizer, options)

Param| Type| Default| Description  
---|---|---|---  
tokenizer| `*`| |   
options| `Object`| |   
[options.skip_prompt]| `boolean`| `false`| Whether to skip the prompt tokens  
[options.callback_function]| `function`| ``| Function to call when a piece of
text is ready to display  
[options.token_callback_function]| `function`| ``| Function to call when a new
token is generated  
[options.on_chunk_start]| `function`| ``| Function to call when a new chunk
starts  
[options.on_chunk_end]| `function`| ``| Function to call when a chunk ends  
[options.on_finalize]| `function`| ``| Function to call when the stream is
finalized  
[options.time_precision]| `number`| `0.02`| Precision of the timestamps  
[options.skip_special_tokens]| `boolean`| `true`| Whether to skip special
tokens when decoding  
[options.decode_kwargs]| `Object`| `{}`| Additional keyword arguments to pass
to the tokenizer's decode method  
  
* * *

###  whisperTextStreamer.put(value)

**Kind** : instance method of `WhisperTextStreamer`

Param| Type  
---|---  
value| `Array.<Array<bigint>>`  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/generation/streamers.md)

[‚ÜêStopping
Criteria](/docs/transformers.js/v3.0.0/en/api/generation/stopping_criteria)
[Core‚Üí](/docs/transformers.js/v3.0.0/en/api/utils/core)

generation/streamers generation/streamers.TextStreamer new
TextStreamer(tokenizer) textStreamer.put(value) textStreamer.end()
textStreamer.on_finalized_text(text, stream_end)
generation/streamers.WhisperTextStreamer new WhisperTextStreamer(tokenizer,
options) whisperTextStreamer.put(value)

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

models

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  models

Definitions of all models available in Transformers.js.

**Example:** Load and run an `AutoModel`.

Copied

    
    
    import { AutoModel, AutoTokenizer } from '@huggingface/transformers';
    
    let tokenizer = await AutoTokenizer.from_pretrained('Xenova/bert-base-uncased');
    let model = await AutoModel.from_pretrained('Xenova/bert-base-uncased');
    
    let inputs = await tokenizer('I love transformers!');
    let { logits } = await model(inputs);
    // Tensor {
    //     data: Float32Array(183132) [-7.117443084716797, -7.107812881469727, -7.092104911804199, ...]
    //     dims: (3) [1, 6, 30522],
    //     type: "float32",
    //     size: 183132,
    // }

We also provide other `AutoModel`s (listed below), which you can use in the
same way as the Python library. For example:

**Example:** Load and run an `AutoModelForSeq2SeqLM`.

Copied

    
    
    import { AutoModelForSeq2SeqLM, AutoTokenizer } from '@huggingface/transformers';
    
    let tokenizer = await AutoTokenizer.from_pretrained('Xenova/t5-small');
    let model = await AutoModelForSeq2SeqLM.from_pretrained('Xenova/t5-small');
    
    let { input_ids } = await tokenizer('translate English to German: I love transformers!');
    let outputs = await model.generate(input_ids);
    let decoded = tokenizer.decode(outputs[0], { skip_special_tokens: true });
    // 'Ich liebe Transformatoren!'

  * models
    *  _static_
      * .PreTrainedModel
        * `new PreTrainedModel(config, sessions, configs)`
        * _instance_
          * `.custom_config` : `*`
          * `.generation_config` ‚áí `GenerationConfig` | `null`
          * `.dispose()` ‚áí `Promise.<Array<unknown>>`
          * `._call(model_inputs)` ‚áí `Promise.<Object>`
          * `.forward(model_inputs)` ‚áí `Promise.<Object>`
          * `._get_logits_warper(generation_config)` ‚áí `LogitsProcessorList`
          * `._prepare_generation_config(generation_config, kwargs)` ‚áí `GenerationConfig`
          * `._get_stopping_criteria(generation_config, [stopping_criteria])`
          * `._validate_model_class()`
          * `._update_model_kwargs_for_generation(inputs)` ‚áí `Object`
          * `._prepare_model_inputs(params)` ‚áí `Object`
          * `._prepare_decoder_input_ids_for_generation(param0)`
          * `.generate(options)` ‚áí `Promise.<(ModelOutput|Tensor)>`
          * `.getPastKeyValues(decoderResults, pastKeyValues)` ‚áí `Object`
          * `.getAttentions(model_output)` ‚áí `*`
          * `.addPastKeyValues(decoderFeeds, pastKeyValues)`
        * _static_
          * `.from_pretrained(pretrained_model_name_or_path, options)` ‚áí `Promise.<PreTrainedModel>`
      * .BaseModelOutput
        * `new BaseModelOutput(output)`
      * .BertForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .BertForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .BertForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .BertForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .RoFormerModel
      * .RoFormerForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .RoFormerForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .RoFormerForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .RoFormerForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .ConvBertModel
      * .ConvBertForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .ConvBertForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .ConvBertForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .ConvBertForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .ElectraModel
      * .ElectraForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .ElectraForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .ElectraForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .ElectraForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .CamembertModel
      * .CamembertForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .CamembertForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .CamembertForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .CamembertForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .DebertaModel
      * .DebertaForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .DebertaForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .DebertaForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .DebertaForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .DebertaV2Model
      * .DebertaV2ForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .DebertaV2ForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .DebertaV2ForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .DebertaV2ForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .DistilBertForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .DistilBertForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .DistilBertForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .DistilBertForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .EsmModel
      * .EsmForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .EsmForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .EsmForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .MobileBertForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .MobileBertForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .MobileBertForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .MPNetModel
      * .MPNetForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .MPNetForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .MPNetForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .MPNetForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .T5ForConditionalGeneration
      * .LongT5PreTrainedModel
      * .LongT5Model
      * .LongT5ForConditionalGeneration
      * .MT5ForConditionalGeneration
      * .BartModel
      * .BartForConditionalGeneration
      * .BartForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .MBartModel
      * .MBartForConditionalGeneration
      * .MBartForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .BlenderbotModel
      * .BlenderbotForConditionalGeneration
      * .BlenderbotSmallModel
      * .BlenderbotSmallForConditionalGeneration
      * .RobertaForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .RobertaForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .RobertaForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .RobertaForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .XLMPreTrainedModel
      * .XLMModel
      * .XLMWithLMHeadModel
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .XLMForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .XLMForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .XLMForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .XLMRobertaForMaskedLM
        * `._call(model_inputs)` ‚áí `Promise.<MaskedLMOutput>`
      * .XLMRobertaForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .XLMRobertaForTokenClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .XLMRobertaForQuestionAnswering
        * `._call(model_inputs)` ‚áí `Promise.<QuestionAnsweringModelOutput>`
      * .ASTModel
      * .ASTForAudioClassification
      * .WhisperModel
      * .WhisperForConditionalGeneration
        * `._retrieve_init_tokens(generation_config)`
        * `.generate(options)` ‚áí `Promise.<(ModelOutput|Tensor)>`
        * `._extract_token_timestamps(generate_outputs, alignment_heads, [num_frames], [time_precision])` ‚áí `Tensor`
      * .VisionEncoderDecoderModel
      * .LlavaForConditionalGeneration
      * .CLIPModel
      * .CLIPTextModel
        * `.from_pretrained()` : `PreTrainedModel.from_pretrained`
      * .CLIPTextModelWithProjection
        * `.from_pretrained()` : `PreTrainedModel.from_pretrained`
      * .CLIPVisionModel
        * `.from_pretrained()` : `PreTrainedModel.from_pretrained`
      * .CLIPVisionModelWithProjection
        * `.from_pretrained()` : `PreTrainedModel.from_pretrained`
      * .SiglipModel
      * .SiglipTextModel
        * `.from_pretrained()` : `PreTrainedModel.from_pretrained`
      * .SiglipVisionModel
        * `.from_pretrained()` : `PreTrainedModel.from_pretrained`
      * .CLIPSegForImageSegmentation
      * .GPT2LMHeadModel
      * .JAISModel
      * .JAISLMHeadModel
      * .CodeGenModel
      * .CodeGenForCausalLM
      * .LlamaPreTrainedModel
      * .LlamaModel
      * .CoherePreTrainedModel
      * .GemmaPreTrainedModel
      * .GemmaModel
      * .Gemma2PreTrainedModel
      * .Gemma2Model
      * .Qwen2PreTrainedModel
      * .Qwen2Model
      * .PhiModel
      * .Phi3Model
      * .BloomPreTrainedModel
      * .BloomModel
      * .BloomForCausalLM
      * .MptModel
      * .MptForCausalLM
      * .OPTModel
      * .OPTForCausalLM
      * .VitMatteForImageMatting
        * `._call(model_inputs)`
      * .DetrObjectDetectionOutput
        * `new DetrObjectDetectionOutput(output)`
      * .DetrSegmentationOutput
        * `new DetrSegmentationOutput(output)`
      * .RTDetrObjectDetectionOutput
        * `new RTDetrObjectDetectionOutput(output)`
      * .TableTransformerModel
      * .TableTransformerForObjectDetection
        * `._call(model_inputs)`
      * .ResNetPreTrainedModel
      * .ResNetModel
      * .ResNetForImageClassification
        * `._call(model_inputs)`
      * .Swin2SRModel
      * .Swin2SRForImageSuperResolution
      * .DPTModel
      * .DPTForDepthEstimation
      * .DepthAnythingForDepthEstimation
      * .GLPNModel
      * .GLPNForDepthEstimation
      * .DonutSwinModel
      * .ConvNextModel
      * .ConvNextForImageClassification
        * `._call(model_inputs)`
      * .ConvNextV2Model
      * .ConvNextV2ForImageClassification
        * `._call(model_inputs)`
      * .Dinov2Model
      * .Dinov2ForImageClassification
        * `._call(model_inputs)`
      * .YolosObjectDetectionOutput
        * `new YolosObjectDetectionOutput(output)`
      * .SamModel
        * `.get_image_embeddings(model_inputs)` ‚áí `Promise.<{image_embeddings: Tensor, image_positional_embeddings: Tensor}>`
        * `.forward(model_inputs)` ‚áí `Promise.<Object>`
        * `._call(model_inputs)` ‚áí `Promise.<SamImageSegmentationOutput>`
      * .SamImageSegmentationOutput
        * `new SamImageSegmentationOutput(output)`
      * .Wav2Vec2Model
      * .Wav2Vec2ForAudioFrameClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .PyAnnoteModel
      * .PyAnnoteForAudioFrameClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .UniSpeechModel
      * .UniSpeechForCTC
        * `._call(model_inputs)`
      * .UniSpeechForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .UniSpeechSatModel
      * .UniSpeechSatForCTC
        * `._call(model_inputs)`
      * .UniSpeechSatForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .UniSpeechSatForAudioFrameClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .Wav2Vec2BertModel
      * .Wav2Vec2BertForCTC
        * `._call(model_inputs)`
      * .Wav2Vec2BertForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .HubertModel
      * .HubertForCTC
        * `._call(model_inputs)`
      * .HubertForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .WavLMPreTrainedModel
      * .WavLMModel
      * .WavLMForCTC
        * `._call(model_inputs)`
      * .WavLMForSequenceClassification
        * `._call(model_inputs)` ‚áí `Promise.<SequenceClassifierOutput>`
      * .WavLMForXVector
        * `._call(model_inputs)` ‚áí `Promise.<XVectorOutput>`
      * .WavLMForAudioFrameClassification
        * `._call(model_inputs)` ‚áí `Promise.<TokenClassifierOutput>`
      * .SpeechT5PreTrainedModel
      * .SpeechT5Model
      * .SpeechT5ForSpeechToText
      * .SpeechT5ForTextToSpeech
        * `.generate_speech(input_values, speaker_embeddings, options)` ‚áí `Promise.<SpeechOutput>`
      * .SpeechT5HifiGan
      * .TrOCRForCausalLM
      * .MistralPreTrainedModel
      * .Starcoder2PreTrainedModel
      * .FalconPreTrainedModel
      * .ClapTextModelWithProjection
        * `.from_pretrained()` : `PreTrainedModel.from_pretrained`
      * .ClapAudioModelWithProjection
        * `.from_pretrained()` : `PreTrainedModel.from_pretrained`
      * .VitsModel
        * `._call(model_inputs)` ‚áí `Promise.<VitsModelOutput>`
      * .SegformerModel
      * .SegformerForImageClassification
      * .SegformerForSemanticSegmentation
      * .StableLmModel
      * .StableLmForCausalLM
      * .EfficientNetModel
      * .EfficientNetForImageClassification
        * `._call(model_inputs)`
      * .MusicgenModel
      * .MusicgenForCausalLM
      * .MusicgenForConditionalGeneration
        * `._apply_and_filter_by_delay_pattern_mask(outputs)` ‚áí `Tensor`
        * `.generate(options)` ‚áí `Promise.<(ModelOutput|Tensor)>`
      * .MobileNetV1Model
      * .MobileNetV1ForImageClassification
        * `._call(model_inputs)`
      * .MobileNetV2Model
      * .MobileNetV2ForImageClassification
        * `._call(model_inputs)`
      * .MobileNetV3Model
      * .MobileNetV3ForImageClassification
        * `._call(model_inputs)`
      * .MobileNetV4Model
      * .MobileNetV4ForImageClassification
        * `._call(model_inputs)`
      * .DecisionTransformerModel
      * .PretrainedMixin
        *  _instance_
          * `.MODEL_CLASS_MAPPINGS` : `*`
          * `.BASE_IF_FAIL`
        * _static_
          * `.from_pretrained()` : `*`
      * .AutoModel
        * `.MODEL_CLASS_MAPPINGS` : `*`
      * .AutoModelForSequenceClassification
      * .AutoModelForTokenClassification
      * .AutoModelForSeq2SeqLM
      * .AutoModelForSpeechSeq2Seq
      * .AutoModelForTextToSpectrogram
      * .AutoModelForTextToWaveform
      * .AutoModelForCausalLM
      * .AutoModelForMaskedLM
      * .AutoModelForQuestionAnswering
      * .AutoModelForVision2Seq
      * .AutoModelForImageClassification
      * .AutoModelForImageSegmentation
      * .AutoModelForSemanticSegmentation
      * .AutoModelForUniversalSegmentation
      * .AutoModelForObjectDetection
      * .AutoModelForMaskGeneration
      * .Seq2SeqLMOutput
        * `new Seq2SeqLMOutput(output)`
      * .SequenceClassifierOutput
        * `new SequenceClassifierOutput(output)`
      * .XVectorOutput
        * `new XVectorOutput(output)`
      * .TokenClassifierOutput
        * `new TokenClassifierOutput(output)`
      * .MaskedLMOutput
        * `new MaskedLMOutput(output)`
      * .QuestionAnsweringModelOutput
        * `new QuestionAnsweringModelOutput(output)`
      * .CausalLMOutput
        * `new CausalLMOutput(output)`
      * .CausalLMOutputWithPast
        * `new CausalLMOutputWithPast(output)`
      * .ImageMattingOutput
        * `new ImageMattingOutput(output)`
      * .VitsModelOutput
        * `new VitsModelOutput(output)`
    * _inner_
      * `~SamModelInputs` : `Object`
      * `~SpeechOutput` : `Object`

* * *

##  models.PreTrainedModel

A base class for pre-trained models that provides the model configuration and
an ONNX session.

**Kind** : static class of `models`

  * .PreTrainedModel
    * `new PreTrainedModel(config, sessions, configs)`
    * _instance_
      * `.custom_config` : `*`
      * `.generation_config` ‚áí `GenerationConfig` | `null`
      * `.dispose()` ‚áí `Promise.<Array<unknown>>`
      * `._call(model_inputs)` ‚áí `Promise.<Object>`
      * `.forward(model_inputs)` ‚áí `Promise.<Object>`
      * `._get_logits_warper(generation_config)` ‚áí `LogitsProcessorList`
      * `._prepare_generation_config(generation_config, kwargs)` ‚áí `GenerationConfig`
      * `._get_stopping_criteria(generation_config, [stopping_criteria])`
      * `._validate_model_class()`
      * `._update_model_kwargs_for_generation(inputs)` ‚áí `Object`
      * `._prepare_model_inputs(params)` ‚áí `Object`
      * `._prepare_decoder_input_ids_for_generation(param0)`
      * `.generate(options)` ‚áí `Promise.<(ModelOutput|Tensor)>`
      * `.getPastKeyValues(decoderResults, pastKeyValues)` ‚áí `Object`
      * `.getAttentions(model_output)` ‚áí `*`
      * `.addPastKeyValues(decoderFeeds, pastKeyValues)`
    * _static_
      * `.from_pretrained(pretrained_model_name_or_path, options)` ‚áí `Promise.<PreTrainedModel>`

* * *

###  new PreTrainedModel(config, sessions, configs)

Creates a new instance of the `PreTrainedModel` class.

Param| Type| Description  
---|---|---  
config| `*`| The model configuration.  
sessions| `Record.<string, any>`| The inference sessions for the model.  
configs| `Record.<string, Object>`| Additional configuration files (e.g.,
generation_config.json).  
  
* * *

###  preTrainedModel.custom_config : <code> * </code>

**Kind** : instance property of `PreTrainedModel`

* * *

###  preTrainedModel.generation_config ‚áí <code> GenerationConfig </code> | <code> null </code>

Get the model‚Äôs generation config, if it exists.

**Kind** : instance property of `PreTrainedModel`  
**Returns** : `GenerationConfig` | `null` \- The model‚Äôs generation config if it exists, otherwise `null`.

* * *

###  preTrainedModel.dispose() ‚áí <code> Promise. < Array < unknown > > </code>

Disposes of all the ONNX sessions that were created during inference.

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `Promise.<Array<unknown>>` \- An array of promises, one for each
ONNX session that is being disposed.  
**Todo**

  * Use <https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/FinalizationRegistry>

* * *

###  preTrainedModel._call(model_inputs) ‚áí <code> Promise. < Object > </code>

Runs the model with the provided inputs

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `Promise.<Object>` \- Object containing output tensors

Param| Type| Description  
---|---|---  
model_inputs| `Object`| Object containing input tensors  
  
* * *

###  preTrainedModel.forward(model_inputs) ‚áí <code> Promise. < Object >
</code>

Forward method for a pretrained model. If not overridden by a subclass, the
correct forward method will be chosen based on the model type.

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `Promise.<Object>` \- The output data from the model in the
format specified in the ONNX model.  
**Throws** :

  * `Error` This method must be implemented in subclasses.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The input data to the model in the format specified in
the ONNX model.  
  
* * *

###  preTrainedModel._get_logits_warper(generation_config) ‚áí <code>
LogitsProcessorList </code>

This function returns a [`LogitsProcessorList`] list object that contains all
relevant [`LogitsWarper`] instances used for multinomial sampling.

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `LogitsProcessorList` \- generation_config

Param| Type| Description  
---|---|---  
generation_config| `GenerationConfig`| The generation config.  
  
* * *

###  preTrainedModel._prepare_generation_config(generation_config, kwargs) ‚áí
<code> GenerationConfig </code>

This function merges multiple generation configs together to form a final
generation config to be used by the model for text generation. It first
creates an empty `GenerationConfig` object, then it applies the model‚Äôs own
`generation_config` property to it. Finally, if a `generation_config` object
was passed in the arguments, it overwrites the corresponding properties in the
final config with those of the passed config object.

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `GenerationConfig` \- The final generation config object to be
used by the model for text generation.

Param| Type| Description  
---|---|---  
generation_config| `GenerationConfig` | `null`| A `GenerationConfig` object containing generation parameters.  
kwargs| `Object`| Additional generation parameters to be used in place of
those in the `generation_config` object.  
  
* * *

###  preTrainedModel._get_stopping_criteria(generation_config,
[stopping_criteria])

**Kind** : instance method of `PreTrainedModel`

Param| Type| Default  
---|---|---  
generation_config| `GenerationConfig`|  
[stopping_criteria]| `StoppingCriteriaList`| ``  
  
* * *

###  preTrainedModel._validate_model_class()

Confirms that the model class is compatible with generation. If not, raises an
exception that points to the right class to use.

**Kind** : instance method of `PreTrainedModel`

* * *

###  preTrainedModel._update_model_kwargs_for_generation(inputs) ‚áí <code>
Object </code>

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `Object` \- The updated model inputs for the next generation
iteration.

Param| Type  
---|---  
inputs| `Object`  
inputs.generated_input_ids| `Array.<Array<bigint>>`  
inputs.outputs| `Object`  
inputs.model_inputs| `Object`  
inputs.is_encoder_decoder| `boolean`  
  
* * *

###  preTrainedModel._prepare_model_inputs(params) ‚áí <code> Object </code>

This function extracts the model-specific `inputs` for generation.

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `Object` \- The model-specific inputs for generation.

Param| Type| Default  
---|---|---  
params| `Object`|  
[params.inputs]| `Tensor`| ``  
[params.bos_token_id]| `number`| ``  
[params.model_kwargs]| `Record.<string, (Tensor|Array<number>)>`|  
  
* * *

###  preTrainedModel._prepare_decoder_input_ids_for_generation(param0)

Prepares `decoder_input_ids` for generation with encoder-decoder models

**Kind** : instance method of `PreTrainedModel`

Param| Type  
---|---  
param0| `*`  
  
* * *

###  preTrainedModel.generate(options) ‚áí <code> Promise. <
(ModelOutput|Tensor) > </code>

Generates sequences of token ids for models with a language modeling head.

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `Promise.<(ModelOutput|Tensor)>` \- The output of the model,
which can contain the generated token ids, attentions, and scores.

Param| Type  
---|---  
options| `*`  
  
* * *

###  preTrainedModel.getPastKeyValues(decoderResults, pastKeyValues) ‚áí <code>
Object </code>

Returns an object containing past key values from the given decoder results
object.

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `Object` \- An object containing past key values.

Param| Type| Description  
---|---|---  
decoderResults| `Object`| The decoder results object.  
pastKeyValues| `Object`| The previous past key values.  
  
* * *

###  preTrainedModel.getAttentions(model_output) ‚áí <code> * </code>

Returns an object containing attentions from the given model output object.

**Kind** : instance method of `PreTrainedModel`  
**Returns** : `*` \- An object containing attentions.

Param| Type| Description  
---|---|---  
model_output| `Object`| The output of the model.  
  
* * *

###  preTrainedModel.addPastKeyValues(decoderFeeds, pastKeyValues)

Adds past key values to the decoder feeds object. If pastKeyValues is null,
creates new tensors for past key values.

**Kind** : instance method of `PreTrainedModel`

Param| Type| Description  
---|---|---  
decoderFeeds| `Object`| The decoder feeds object to add past key values to.  
pastKeyValues| `Object`| An object containing past key values.  
  
* * *

###  PreTrainedModel.from_pretrained(pretrained_model_name_or_path, options) ‚áí
<code> Promise. < PreTrainedModel > </code>

Instantiate one of the model classes of the library from a pretrained model.

The model class to instantiate is selected based on the `model_type` property
of the config object (either passed as an argument or loaded from
`pretrained_model_name_or_path` if possible)

**Kind** : static method of `PreTrainedModel`  
**Returns** : `Promise.<PreTrainedModel>` \- A new instance of the
`PreTrainedModel` class.

Param| Type| Description  
---|---|---  
pretrained_model_name_or_path| `string`| The name or path of the pretrained
model. Can be either:

  * A string, the _model id_ of a pretrained model hosted inside a model repo on huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  * A path to a _directory_ containing model weights, e.g., `./my_model_directory/`.

  
options| `*`| Additional options for loading the model.  
  
* * *

##  models.BaseModelOutput

Base class for model‚Äôs outputs, with potential hidden states and attentions.

**Kind** : static class of `models`

* * *

###  new BaseModelOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.last_hidden_state| `Tensor`| Sequence of hidden-states at the output of
the last layer of the model.  
[output.hidden_states]| `Tensor`| Hidden-states of the model at the output of
each layer plus the optional initial embedding outputs.  
[output.attentions]| `Tensor`| Attentions weights after the attention softmax,
used to compute the weighted average in the self-attention heads.  
  
* * *

##  models.BertForMaskedLM

BertForMaskedLM is a class representing a BERT model for masked language
modeling.

**Kind** : static class of `models`

* * *

###  bertForMaskedLM._call(model_inputs) ‚áí <code> Promise. < MaskedLMOutput >
</code>

Calls the model on new inputs.

**Kind** : instance method of `BertForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- An object containing the model‚Äôs
output logits for masked language modeling.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.BertForSequenceClassification

BertForSequenceClassification is a class representing a BERT model for
sequence classification.

**Kind** : static class of `models`

* * *

###  bertForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `BertForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.BertForTokenClassification

BertForTokenClassification is a class representing a BERT model for token
classification.

**Kind** : static class of `models`

* * *

###  bertForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `BertForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.BertForQuestionAnswering

BertForQuestionAnswering is a class representing a BERT model for question
answering.

**Kind** : static class of `models`

* * *

###  bertForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `BertForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- An object containing
the model‚Äôs output logits for question answering.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.RoFormerModel

The bare RoFormer Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.RoFormerForMaskedLM

RoFormer Model with a `language modeling` head on top.

**Kind** : static class of `models`

* * *

###  roFormerForMaskedLM._call(model_inputs) ‚áí <code> Promise. <
MaskedLMOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `RoFormerForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- An object containing the model‚Äôs
output logits for masked language modeling.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.RoFormerForSequenceClassification

RoFormer Model transformer with a sequence classification/regression head on
top (a linear layer on top of the pooled output)

**Kind** : static class of `models`

* * *

###  roFormerForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `RoFormerForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.RoFormerForTokenClassification

RoFormer Model with a token classification head on top (a linear layer on top
of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.

**Kind** : static class of `models`

* * *

###  roFormerForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `RoFormerForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.RoFormerForQuestionAnswering

RoFormer Model with a span classification head on top for extractive question-
answering tasks like SQuAD (a linear layers on top of the hidden-states output
to compute `span start logits` and `span end logits`).

**Kind** : static class of `models`

* * *

###  roFormerForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `RoFormerForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- An object containing
the model‚Äôs output logits for question answering.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.ConvBertModel

The bare ConvBERT Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.ConvBertForMaskedLM

ConvBERT Model with a language modeling head on top.

**Kind** : static class of `models`

* * *

###  convBertForMaskedLM._call(model_inputs) ‚áí <code> Promise. <
MaskedLMOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `ConvBertForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- An object containing the model‚Äôs
output logits for masked language modeling.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.ConvBertForSequenceClassification

ConvBERT Model transformer with a sequence classification/regression head on
top (a linear layer on top of the pooled output)

**Kind** : static class of `models`

* * *

###  convBertForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `ConvBertForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.ConvBertForTokenClassification

ConvBERT Model with a token classification head on top (a linear layer on top
of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.

**Kind** : static class of `models`

* * *

###  convBertForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `ConvBertForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.ConvBertForQuestionAnswering

ConvBERT Model with a span classification head on top for extractive question-
answering tasks like SQuAD (a linear layers on top of the hidden-states output
to compute `span start logits` and `span end logits`)

**Kind** : static class of `models`

* * *

###  convBertForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `ConvBertForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- An object containing
the model‚Äôs output logits for question answering.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.ElectraModel

The bare Electra Model transformer outputting raw hidden-states without any
specific head on top. Identical to the BERT model except that it uses an
additional linear layer between the embedding layer and the encoder if the
hidden size and embedding size are different.

**Kind** : static class of `models`

* * *

##  models.ElectraForMaskedLM

Electra model with a language modeling head on top.

**Kind** : static class of `models`

* * *

###  electraForMaskedLM._call(model_inputs) ‚áí <code> Promise. < MaskedLMOutput
> </code>

Calls the model on new inputs.

**Kind** : instance method of `ElectraForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- An object containing the model‚Äôs
output logits for masked language modeling.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.ElectraForSequenceClassification

ELECTRA Model transformer with a sequence classification/regression head on
top (a linear layer on top of the pooled output)

**Kind** : static class of `models`

* * *

###  electraForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `ElectraForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.ElectraForTokenClassification

Electra model with a token classification head on top.

**Kind** : static class of `models`

* * *

###  electraForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `ElectraForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.ElectraForQuestionAnswering

LECTRA Model with a span classification head on top for extractive question-
answering tasks like SQuAD (a linear layers on top of the hidden-states output
to compute `span start logits` and `span end logits`).

**Kind** : static class of `models`

* * *

###  electraForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `ElectraForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- An object containing
the model‚Äôs output logits for question answering.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.CamembertModel

The bare CamemBERT Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.CamembertForMaskedLM

CamemBERT Model with a `language modeling` head on top.

**Kind** : static class of `models`

* * *

###  camembertForMaskedLM._call(model_inputs) ‚áí <code> Promise. <
MaskedLMOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `CamembertForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- An object containing the model‚Äôs
output logits for masked language modeling.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.CamembertForSequenceClassification

CamemBERT Model transformer with a sequence classification/regression head on
top (a linear layer on top of the pooled output) e.g. for GLUE tasks.

**Kind** : static class of `models`

* * *

###  camembertForSequenceClassification._call(model_inputs) ‚áí <code> Promise.
< SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `CamembertForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.CamembertForTokenClassification

CamemBERT Model with a token classification head on top (a linear layer on top
of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.

**Kind** : static class of `models`

* * *

###  camembertForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `CamembertForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.CamembertForQuestionAnswering

CamemBERT Model with a span classification head on top for extractive
question-answering tasks

**Kind** : static class of `models`

* * *

###  camembertForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `CamembertForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- An object containing
the model‚Äôs output logits for question answering.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DebertaModel

The bare DeBERTa Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.DebertaForMaskedLM

DeBERTa Model with a `language modeling` head on top.

**Kind** : static class of `models`

* * *

###  debertaForMaskedLM._call(model_inputs) ‚áí <code> Promise. < MaskedLMOutput
> </code>

Calls the model on new inputs.

**Kind** : instance method of `DebertaForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- An object containing the model‚Äôs
output logits for masked language modeling.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DebertaForSequenceClassification

DeBERTa Model transformer with a sequence classification/regression head on
top (a linear layer on top of the pooled output)

**Kind** : static class of `models`

* * *

###  debertaForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DebertaForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DebertaForTokenClassification

DeBERTa Model with a token classification head on top (a linear layer on top
of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.

**Kind** : static class of `models`

* * *

###  debertaForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DebertaForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DebertaForQuestionAnswering

DeBERTa Model with a span classification head on top for extractive question-
answering tasks like SQuAD (a linear layers on top of the hidden-states output
to compute `span start logits` and `span end logits`).

**Kind** : static class of `models`

* * *

###  debertaForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DebertaForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- An object containing
the model‚Äôs output logits for question answering.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DebertaV2Model

The bare DeBERTa-V2 Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.DebertaV2ForMaskedLM

DeBERTa-V2 Model with a `language modeling` head on top.

**Kind** : static class of `models`

* * *

###  debertaV2ForMaskedLM._call(model_inputs) ‚áí <code> Promise. <
MaskedLMOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DebertaV2ForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- An object containing the model‚Äôs
output logits for masked language modeling.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DebertaV2ForSequenceClassification

DeBERTa-V2 Model transformer with a sequence classification/regression head on
top (a linear layer on top of the pooled output)

**Kind** : static class of `models`

* * *

###  debertaV2ForSequenceClassification._call(model_inputs) ‚áí <code> Promise.
< SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DebertaV2ForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DebertaV2ForTokenClassification

DeBERTa-V2 Model with a token classification head on top (a linear layer on
top of the hidden-states output) e.g. for Named-Entity-Recognition (NER)
tasks.

**Kind** : static class of `models`

* * *

###  debertaV2ForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DebertaV2ForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DebertaV2ForQuestionAnswering

DeBERTa-V2 Model with a span classification head on top for extractive
question-answering tasks like SQuAD (a linear layers on top of the hidden-
states output to compute `span start logits` and `span end logits`).

**Kind** : static class of `models`

* * *

###  debertaV2ForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DebertaV2ForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- An object containing
the model‚Äôs output logits for question answering.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DistilBertForSequenceClassification

DistilBertForSequenceClassification is a class representing a DistilBERT model
for sequence classification.

**Kind** : static class of `models`

* * *

###  distilBertForSequenceClassification._call(model_inputs) ‚áí <code> Promise.
< SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DistilBertForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DistilBertForTokenClassification

DistilBertForTokenClassification is a class representing a DistilBERT model
for token classification.

**Kind** : static class of `models`

* * *

###  distilBertForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DistilBertForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DistilBertForQuestionAnswering

DistilBertForQuestionAnswering is a class representing a DistilBERT model for
question answering.

**Kind** : static class of `models`

* * *

###  distilBertForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DistilBertForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- An object containing
the model‚Äôs output logits for question answering.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.DistilBertForMaskedLM

DistilBertForMaskedLM is a class representing a DistilBERT model for masking
task.

**Kind** : static class of `models`

* * *

###  distilBertForMaskedLM._call(model_inputs) ‚áí <code> Promise. <
MaskedLMOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `DistilBertForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.EsmModel

The bare ESM Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.EsmForMaskedLM

ESM Model with a `language modeling` head on top.

**Kind** : static class of `models`

* * *

###  esmForMaskedLM._call(model_inputs) ‚áí <code> Promise. < MaskedLMOutput >
</code>

Calls the model on new inputs.

**Kind** : instance method of `EsmForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- An object containing the model‚Äôs
output logits for masked language modeling.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.EsmForSequenceClassification

ESM Model transformer with a sequence classification/regression head on top (a
linear layer on top of the pooled output)

**Kind** : static class of `models`

* * *

###  esmForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `EsmForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.EsmForTokenClassification

ESM Model with a token classification head on top (a linear layer on top of
the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.

**Kind** : static class of `models`

* * *

###  esmForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `EsmForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.MobileBertForMaskedLM

MobileBertForMaskedLM is a class representing a MobileBERT model for masking
task.

**Kind** : static class of `models`

* * *

###  mobileBertForMaskedLM._call(model_inputs) ‚áí <code> Promise. <
MaskedLMOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `MobileBertForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.MobileBertForSequenceClassification

MobileBert Model transformer with a sequence classification/regression head on
top (a linear layer on top of the pooled output)

**Kind** : static class of `models`

* * *

###  mobileBertForSequenceClassification._call(model_inputs) ‚áí <code> Promise.
< SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `MobileBertForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.MobileBertForQuestionAnswering

MobileBert Model with a span classification head on top for extractive
question-answering tasks

**Kind** : static class of `models`

* * *

###  mobileBertForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `MobileBertForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.MPNetModel

The bare MPNet Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.MPNetForMaskedLM

MPNetForMaskedLM is a class representing a MPNet model for masked language
modeling.

**Kind** : static class of `models`

* * *

###  mpNetForMaskedLM._call(model_inputs) ‚áí <code> Promise. < MaskedLMOutput >
</code>

Calls the model on new inputs.

**Kind** : instance method of `MPNetForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- An object containing the model‚Äôs
output logits for masked language modeling.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.MPNetForSequenceClassification

MPNetForSequenceClassification is a class representing a MPNet model for
sequence classification.

**Kind** : static class of `models`

* * *

###  mpNetForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `MPNetForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.MPNetForTokenClassification

MPNetForTokenClassification is a class representing a MPNet model for token
classification.

**Kind** : static class of `models`

* * *

###  mpNetForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `MPNetForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.MPNetForQuestionAnswering

MPNetForQuestionAnswering is a class representing a MPNet model for question
answering.

**Kind** : static class of `models`

* * *

###  mpNetForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `MPNetForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- An object containing
the model‚Äôs output logits for question answering.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.T5ForConditionalGeneration

T5Model is a class representing a T5 model for conditional generation.

**Kind** : static class of `models`

* * *

##  models.LongT5PreTrainedModel

An abstract class to handle weights initialization and a simple interface for
downloading and loading pretrained models.

**Kind** : static class of `models`

* * *

##  models.LongT5Model

The bare LONGT5 Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.LongT5ForConditionalGeneration

LONGT5 Model with a `language modeling` head on top.

**Kind** : static class of `models`

* * *

##  models.MT5ForConditionalGeneration

A class representing a conditional sequence-to-sequence model based on the MT5
architecture.

**Kind** : static class of `models`

* * *

##  models.BartModel

The bare BART Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.BartForConditionalGeneration

The BART Model with a language modeling head. Can be used for summarization.

**Kind** : static class of `models`

* * *

##  models.BartForSequenceClassification

Bart model with a sequence classification/head on top (a linear layer on top
of the pooled output)

**Kind** : static class of `models`

* * *

###  bartForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `BartForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.MBartModel

The bare MBART Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.MBartForConditionalGeneration

The MBART Model with a language modeling head. Can be used for summarization,
after fine-tuning the pretrained models.

**Kind** : static class of `models`

* * *

##  models.MBartForSequenceClassification

MBart model with a sequence classification/head on top (a linear layer on top
of the pooled output).

**Kind** : static class of `models`

* * *

###  mBartForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `MBartForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.BlenderbotModel

The bare Blenderbot Model outputting raw hidden-states without any specific
head on top.

**Kind** : static class of `models`

* * *

##  models.BlenderbotForConditionalGeneration

The Blenderbot Model with a language modeling head. Can be used for
summarization.

**Kind** : static class of `models`

* * *

##  models.BlenderbotSmallModel

The bare BlenderbotSmall Model outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.BlenderbotSmallForConditionalGeneration

The BlenderbotSmall Model with a language modeling head. Can be used for
summarization.

**Kind** : static class of `models`

* * *

##  models.RobertaForMaskedLM

RobertaForMaskedLM class for performing masked language modeling on Roberta
models.

**Kind** : static class of `models`

* * *

###  robertaForMaskedLM._call(model_inputs) ‚áí <code> Promise. < MaskedLMOutput
> </code>

Calls the model on new inputs.

**Kind** : instance method of `RobertaForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.RobertaForSequenceClassification

RobertaForSequenceClassification class for performing sequence classification
on Roberta models.

**Kind** : static class of `models`

* * *

###  robertaForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `RobertaForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.RobertaForTokenClassification

RobertaForTokenClassification class for performing token classification on
Roberta models.

**Kind** : static class of `models`

* * *

###  robertaForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `RobertaForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.RobertaForQuestionAnswering

RobertaForQuestionAnswering class for performing question answering on Roberta
models.

**Kind** : static class of `models`

* * *

###  robertaForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `RobertaForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.XLMPreTrainedModel

An abstract class to handle weights initialization and a simple interface for
downloading and loading pretrained models.

**Kind** : static class of `models`

* * *

##  models.XLMModel

The bare XLM Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.XLMWithLMHeadModel

The XLM Model transformer with a language modeling head on top (linear layer
with weights tied to the input embeddings).

**Kind** : static class of `models`

* * *

###  xlmWithLMHeadModel._call(model_inputs) ‚áí <code> Promise. < MaskedLMOutput
> </code>

Calls the model on new inputs.

**Kind** : instance method of `XLMWithLMHeadModel`  
**Returns** : `Promise.<MaskedLMOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.XLMForSequenceClassification

XLM Model with a sequence classification/regression head on top (a linear
layer on top of the pooled output)

**Kind** : static class of `models`

* * *

###  xlmForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `XLMForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.XLMForTokenClassification

XLM Model with a token classification head on top (a linear layer on top of
the hidden-states output)

**Kind** : static class of `models`

* * *

###  xlmForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `XLMForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.XLMForQuestionAnswering

XLM Model with a span classification head on top for extractive question-
answering tasks

**Kind** : static class of `models`

* * *

###  xlmForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `XLMForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.XLMRobertaForMaskedLM

XLMRobertaForMaskedLM class for performing masked language modeling on
XLMRoberta models.

**Kind** : static class of `models`

* * *

###  xlmRobertaForMaskedLM._call(model_inputs) ‚áí <code> Promise. <
MaskedLMOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `XLMRobertaForMaskedLM`  
**Returns** : `Promise.<MaskedLMOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.XLMRobertaForSequenceClassification

XLMRobertaForSequenceClassification class for performing sequence
classification on XLMRoberta models.

**Kind** : static class of `models`

* * *

###  xlmRobertaForSequenceClassification._call(model_inputs) ‚áí <code> Promise.
< SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `XLMRobertaForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.XLMRobertaForTokenClassification

XLMRobertaForTokenClassification class for performing token classification on
XLMRoberta models.

**Kind** : static class of `models`

* * *

###  xlmRobertaForTokenClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `XLMRobertaForTokenClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for token classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.XLMRobertaForQuestionAnswering

XLMRobertaForQuestionAnswering class for performing question answering on
XLMRoberta models.

**Kind** : static class of `models`

* * *

###  xlmRobertaForQuestionAnswering._call(model_inputs) ‚áí <code> Promise. <
QuestionAnsweringModelOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `XLMRobertaForQuestionAnswering`  
**Returns** : `Promise.<QuestionAnsweringModelOutput>` \- returned object

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.ASTModel

The bare AST Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.ASTForAudioClassification

Audio Spectrogram Transformer model with an audio classification head on top
(a linear layer on top of the pooled output) e.g. for datasets like AudioSet,
Speech Commands v2.

**Kind** : static class of `models`

* * *

##  models.WhisperModel

WhisperModel class for training Whisper models without a language model head.

**Kind** : static class of `models`

* * *

##  models.WhisperForConditionalGeneration

WhisperForConditionalGeneration class for generating conditional outputs from
Whisper models.

**Kind** : static class of `models`

  * .WhisperForConditionalGeneration
    * `._retrieve_init_tokens(generation_config)`
    * `.generate(options)` ‚áí `Promise.<(ModelOutput|Tensor)>`
    * `._extract_token_timestamps(generate_outputs, alignment_heads, [num_frames], [time_precision])` ‚áí `Tensor`

* * *

###  whisperForConditionalGeneration._retrieve_init_tokens(generation_config)

**Kind** : instance method of `WhisperForConditionalGeneration`

Param| Type  
---|---  
generation_config| `WhisperGenerationConfig`  
  
* * *

###  whisperForConditionalGeneration.generate(options) ‚áí <code> Promise. <
(ModelOutput|Tensor) > </code>

Transcribes or translates log-mel input features to a sequence of auto-
regressively generated token ids.

**Kind** : instance method of `WhisperForConditionalGeneration`  
**Returns** : `Promise.<(ModelOutput|Tensor)>` \- The output of the model,
which can contain the generated token ids, attentions, and scores.

Param| Type  
---|---  
options| `*`  
  
* * *

###
whisperForConditionalGeneration._extract_token_timestamps(generate_outputs,
alignment_heads, [num_frames], [time_precision]) ‚áí <code> Tensor </code>

Calculates token-level timestamps using the encoder-decoder cross-attentions
and dynamic time-warping (DTW) to map each output token to a position in the
input audio. If `num_frames` is specified, the encoder-decoder cross-
attentions will be cropped before applying DTW.

**Kind** : instance method of `WhisperForConditionalGeneration`  
**Returns** : `Tensor` \- tensor containing the timestamps in seconds for each
predicted token

Param| Type| Default| Description  
---|---|---|---  
generate_outputs| `Object`| | Outputs generated by the model  
generate_outputs.cross_attentions| `Array.<Array<Tensor>>`| | The cross attentions output by the model  
generate_outputs.sequences| `Tensor`| | The sequences output by the model  
alignment_heads| `Array.<Array<number>>`| | Alignment heads of the model  
[num_frames]| `number`| ``| Number of frames in the input audio.  
[time_precision]| `number`| `0.02`| Precision of the timestamps in seconds  
  
* * *

##  models.VisionEncoderDecoderModel

Vision Encoder-Decoder model based on OpenAI‚Äôs GPT architecture for image
captioning and other vision tasks

**Kind** : static class of `models`

* * *

##  models.LlavaForConditionalGeneration

The LLAVA model which consists of a vision backbone and a language model.

**Kind** : static class of `models`

* * *

##  models.CLIPModel

CLIP Text and Vision Model with a projection layers on top

**Example:** Perform zero-shot image classification with a `CLIPModel`.

Copied

    
    
    import { AutoTokenizer, AutoProcessor, CLIPModel, RawImage } from '@huggingface/transformers';
    
    // Load tokenizer, processor, and model
    let tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');
    let processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');
    let model = await CLIPModel.from_pretrained('Xenova/clip-vit-base-patch16');
    
    // Run tokenization
    let texts = ['a photo of a car', 'a photo of a football match']
    let text_inputs = tokenizer(texts, { padding: true, truncation: true });
    
    // Read image and run processor
    let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/football-match.jpg');
    let image_inputs = await processor(image);
    
    // Run model with both text and pixel inputs
    let output = await model({ ...text_inputs, ...image_inputs });
    // {
    //   logits_per_image: Tensor {
    //     dims: [ 1, 2 ],
    //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],
    //   },
    //   logits_per_text: Tensor {
    //     dims: [ 2, 1 ],
    //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],
    //   },
    //   text_embeds: Tensor {
    //     dims: [ 2, 512 ],
    //     data: Float32Array(1024) [ ... ],
    //   },
    //   image_embeds: Tensor {
    //     dims: [ 1, 512 ],
    //     data: Float32Array(512) [ ... ],
    //   }
    // }

**Kind** : static class of `models`

* * *

##  models.CLIPTextModel

The text model from CLIP without any head or projection on top.

**Kind** : static class of `models`

* * *

###  CLIPTextModel.from_pretrained() : <code> PreTrainedModel.from_pretrained
</code>

**Kind** : static method of `CLIPTextModel`

* * *

##  models.CLIPTextModelWithProjection

CLIP Text Model with a projection layer on top (a linear layer on top of the
pooled output)

**Example:** Compute text embeddings with `CLIPTextModelWithProjection`.

Copied

    
    
    import { AutoTokenizer, CLIPTextModelWithProjection } from '@huggingface/transformers';
    
    // Load tokenizer and text model
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');
    const text_model = await CLIPTextModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');
    
    // Run tokenization
    let texts = ['a photo of a car', 'a photo of a football match'];
    let text_inputs = tokenizer(texts, { padding: true, truncation: true });
    
    // Compute embeddings
    const { text_embeds } = await text_model(text_inputs);
    // Tensor {
    //   dims: [ 2, 512 ],
    //   type: 'float32',
    //   data: Float32Array(1024) [ ... ],
    //   size: 1024
    // }

**Kind** : static class of `models`

* * *

###  CLIPTextModelWithProjection.from_pretrained() : <code>
PreTrainedModel.from_pretrained </code>

**Kind** : static method of `CLIPTextModelWithProjection`

* * *

##  models.CLIPVisionModel

The vision model from CLIP without any head or projection on top.

**Kind** : static class of `models`

* * *

###  CLIPVisionModel.from_pretrained() : <code>
PreTrainedModel.from_pretrained </code>

**Kind** : static method of `CLIPVisionModel`

* * *

##  models.CLIPVisionModelWithProjection

CLIP Vision Model with a projection layer on top (a linear layer on top of the
pooled output)

**Example:** Compute vision embeddings with `CLIPVisionModelWithProjection`.

Copied

    
    
    import { AutoProcessor, CLIPVisionModelWithProjection, RawImage} from '@huggingface/transformers';
    
    // Load processor and vision model
    const processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');
    const vision_model = await CLIPVisionModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');
    
    // Read image and run processor
    let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/football-match.jpg');
    let image_inputs = await processor(image);
    
    // Compute embeddings
    const { image_embeds } = await vision_model(image_inputs);
    // Tensor {
    //   dims: [ 1, 512 ],
    //   type: 'float32',
    //   data: Float32Array(512) [ ... ],
    //   size: 512
    // }

**Kind** : static class of `models`

* * *

###  CLIPVisionModelWithProjection.from_pretrained() : <code>
PreTrainedModel.from_pretrained </code>

**Kind** : static method of `CLIPVisionModelWithProjection`

* * *

##  models.SiglipModel

SigLIP Text and Vision Model with a projection layers on top

**Example:** Perform zero-shot image classification with a `SiglipModel`.

Copied

    
    
    import { AutoTokenizer, AutoProcessor, SiglipModel, RawImage } from '@huggingface/transformers';
    
    // Load tokenizer, processor, and model
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/siglip-base-patch16-224');
    const processor = await AutoProcessor.from_pretrained('Xenova/siglip-base-patch16-224');
    const model = await SiglipModel.from_pretrained('Xenova/siglip-base-patch16-224');
    
    // Run tokenization
    const texts = ['a photo of 2 cats', 'a photo of 2 dogs'];
    const text_inputs = tokenizer(texts, { padding: 'max_length', truncation: true });
    
    // Read image and run processor
    const image = await RawImage.read('http://images.cocodataset.org/val2017/000000039769.jpg');
    const image_inputs = await processor(image);
    
    // Run model with both text and pixel inputs
    const output = await model({ ...text_inputs, ...image_inputs });
    // {
    //   logits_per_image: Tensor {
    //     dims: [ 1, 2 ],
    //     data: Float32Array(2) [ -1.6019744873046875, -10.720091819763184 ],
    //   },
    //   logits_per_text: Tensor {
    //     dims: [ 2, 1 ],
    //     data: Float32Array(2) [ -1.6019744873046875, -10.720091819763184 ],
    //   },
    //   text_embeds: Tensor {
    //     dims: [ 2, 768 ],
    //     data: Float32Array(1536) [ ... ],
    //   },
    //   image_embeds: Tensor {
    //     dims: [ 1, 768 ],
    //     data: Float32Array(768) [ ... ],
    //   }
    // }

**Kind** : static class of `models`

* * *

##  models.SiglipTextModel

The text model from SigLIP without any head or projection on top.

**Example:** Compute text embeddings with `SiglipTextModel`.

Copied

    
    
    import { AutoTokenizer, SiglipTextModel } from '@huggingface/transformers';
    
    // Load tokenizer and text model
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/siglip-base-patch16-224');
    const text_model = await SiglipTextModel.from_pretrained('Xenova/siglip-base-patch16-224');
    
    // Run tokenization
    const texts = ['a photo of 2 cats', 'a photo of 2 dogs'];
    const text_inputs = tokenizer(texts, { padding: 'max_length', truncation: true });
    
    // Compute embeddings
    const { pooler_output } = await text_model(text_inputs);
    // Tensor {
    //   dims: [ 2, 768 ],
    //   type: 'float32',
    //   data: Float32Array(1536) [ ... ],
    //   size: 1536
    // }

**Kind** : static class of `models`

* * *

###  SiglipTextModel.from_pretrained() : <code>
PreTrainedModel.from_pretrained </code>

**Kind** : static method of `SiglipTextModel`

* * *

##  models.SiglipVisionModel

The vision model from SigLIP without any head or projection on top.

**Example:** Compute vision embeddings with `SiglipVisionModel`.

Copied

    
    
    import { AutoProcessor, SiglipVisionModel, RawImage} from '@huggingface/transformers';
    
    // Load processor and vision model
    const processor = await AutoProcessor.from_pretrained('Xenova/siglip-base-patch16-224');
    const vision_model = await SiglipVisionModel.from_pretrained('Xenova/siglip-base-patch16-224');
    
    // Read image and run processor
    const image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/football-match.jpg');
    const image_inputs = await processor(image);
    
    // Compute embeddings
    const { pooler_output } = await vision_model(image_inputs);
    // Tensor {
    //   dims: [ 1, 768 ],
    //   type: 'float32',
    //   data: Float32Array(768) [ ... ],
    //   size: 768
    // }

**Kind** : static class of `models`

* * *

###  SiglipVisionModel.from_pretrained() : <code>
PreTrainedModel.from_pretrained </code>

**Kind** : static method of `SiglipVisionModel`

* * *

##  models.CLIPSegForImageSegmentation

CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-
shot image segmentation.

**Example:** Perform zero-shot image segmentation with a
`CLIPSegForImageSegmentation` model.

Copied

    
    
    import { AutoTokenizer, AutoProcessor, CLIPSegForImageSegmentation, RawImage } from '@huggingface/transformers';
    
    // Load tokenizer, processor, and model
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clipseg-rd64-refined');
    const processor = await AutoProcessor.from_pretrained('Xenova/clipseg-rd64-refined');
    const model = await CLIPSegForImageSegmentation.from_pretrained('Xenova/clipseg-rd64-refined');
    
    // Run tokenization
    const texts = ['a glass', 'something to fill', 'wood', 'a jar'];
    const text_inputs = tokenizer(texts, { padding: true, truncation: true });
    
    // Read image and run processor
    const image = await RawImage.read('https://github.com/timojl/clipseg/blob/master/example_image.jpg?raw=true');
    const image_inputs = await processor(image);
    
    // Run model with both text and pixel inputs
    const { logits } = await model({ ...text_inputs, ...image_inputs });
    // logits: Tensor {
    //   dims: [4, 352, 352],
    //   type: 'float32',
    //   data: Float32Array(495616) [ ... ],
    //   size: 495616
    // }

You can visualize the predictions as follows:

Copied

    
    
    const preds = logits
      .unsqueeze_(1)
      .sigmoid_()
      .mul_(255)
      .round_()
      .to('uint8');
    
    for (let i = 0; i < preds.dims[0]; ++i) {
      const img = RawImage.fromTensor(preds[i]);
      img.save(`prediction_${i}.png`);
    }

**Kind** : static class of `models`

* * *

##  models.GPT2LMHeadModel

GPT-2 language model head on top of the GPT-2 base model. This model is
suitable for text generation tasks.

**Kind** : static class of `models`

* * *

##  models.JAISModel

The bare JAIS Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.JAISLMHeadModel

The JAIS Model transformer with a language modeling head on top (linear layer
with weights tied to the input embeddings).

**Kind** : static class of `models`

* * *

##  models.CodeGenModel

CodeGenModel is a class representing a code generation model without a
language model head.

**Kind** : static class of `models`

* * *

##  models.CodeGenForCausalLM

CodeGenForCausalLM is a class that represents a code generation model based on
the GPT-2 architecture. It extends the `CodeGenPreTrainedModel` class.

**Kind** : static class of `models`

* * *

##  models.LlamaPreTrainedModel

The bare LLama Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.LlamaModel

The bare LLaMA Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.CoherePreTrainedModel

The bare Cohere Model outputting raw hidden-states without any specific head
on top.

**Kind** : static class of `models`

* * *

##  models.GemmaPreTrainedModel

The bare Gemma Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.GemmaModel

The bare Gemma Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.Gemma2PreTrainedModel

The bare Gemma2 Model outputting raw hidden-states without any specific head
on top.

**Kind** : static class of `models`

* * *

##  models.Gemma2Model

The bare Gemma2 Model outputting raw hidden-states without any specific head
on top.

**Kind** : static class of `models`

* * *

##  models.Qwen2PreTrainedModel

The bare Qwen2 Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.Qwen2Model

The bare Qwen2 Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.PhiModel

The bare Phi Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.Phi3Model

The bare Phi3 Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.BloomPreTrainedModel

The Bloom Model transformer with a language modeling head on top (linear layer
with weights tied to the input embeddings).

**Kind** : static class of `models`

* * *

##  models.BloomModel

The bare Bloom Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.BloomForCausalLM

The Bloom Model transformer with a language modeling head on top (linear layer
with weights tied to the input embeddings).

**Kind** : static class of `models`

* * *

##  models.MptModel

The bare Mpt Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.MptForCausalLM

The MPT Model transformer with a language modeling head on top (linear layer
with weights tied to the input embeddings).

**Kind** : static class of `models`

* * *

##  models.OPTModel

The bare OPT Model outputting raw hidden-states without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.OPTForCausalLM

The OPT Model transformer with a language modeling head on top (linear layer
with weights tied to the input embeddings).

**Kind** : static class of `models`

* * *

##  models.VitMatteForImageMatting

ViTMatte framework leveraging any vision backbone e.g. for ADE20k, CityScapes.

**Example:** Perform image matting with a `VitMatteForImageMatting` model.

Copied

    
    
    import { AutoProcessor, VitMatteForImageMatting, RawImage } from '@huggingface/transformers';
    
    // Load processor and model
    const processor = await AutoProcessor.from_pretrained('Xenova/vitmatte-small-distinctions-646');
    const model = await VitMatteForImageMatting.from_pretrained('Xenova/vitmatte-small-distinctions-646');
    
    // Load image and trimap
    const image = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/vitmatte_image.png');
    const trimap = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/vitmatte_trimap.png');
    
    // Prepare image + trimap for the model
    const inputs = await processor(image, trimap);
    
    // Predict alpha matte
    const { alphas } = await model(inputs);
    // Tensor {
    //   dims: [ 1, 1, 640, 960 ],
    //   type: 'float32',
    //   size: 614400,
    //   data: Float32Array(614400) [ 0.9894027709960938, 0.9970508813858032, ... ]
    // }

You can visualize the alpha matte as follows:

Copied

    
    
    import { Tensor, cat } from '@huggingface/transformers';
    
    // Visualize predicted alpha matte
    const imageTensor = image.toTensor();
    
    // Convert float (0-1) alpha matte to uint8 (0-255)
    const alphaChannel = alphas
      .squeeze(0)
      .mul_(255)
      .clamp_(0, 255)
      .round_()
      .to('uint8');
    
    // Concatenate original image with predicted alpha
    const imageData = cat([imageTensor, alphaChannel], 0);
    
    // Save output image
    const outputImage = RawImage.fromTensor(imageData);
    outputImage.save('output.png');

**Kind** : static class of `models`

* * *

###  vitMatteForImageMatting._call(model_inputs)

**Kind** : instance method of `VitMatteForImageMatting`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.DetrObjectDetectionOutput

**Kind** : static class of `models`

* * *

###  new DetrObjectDetectionOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| Classification logits (including no-object) for all
queries.  
output.pred_boxes| `Tensor`| Normalized boxes coordinates for all queries,
represented as (center_x, center_y, width, height). These values are
normalized in [0, 1], relative to the size of each individual image in the
batch (disregarding possible padding).  
  
* * *

##  models.DetrSegmentationOutput

**Kind** : static class of `models`

* * *

###  new DetrSegmentationOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| The output logits of the model.  
output.pred_boxes| `Tensor`| Predicted boxes.  
output.pred_masks| `Tensor`| Predicted masks.  
  
* * *

##  models.RTDetrObjectDetectionOutput

**Kind** : static class of `models`

* * *

###  new RTDetrObjectDetectionOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| Classification logits (including no-object) for all
queries.  
output.pred_boxes| `Tensor`| Normalized boxes coordinates for all queries,
represented as (center_x, center_y, width, height). These values are
normalized in [0, 1], relative to the size of each individual image in the
batch (disregarding possible padding).  
  
* * *

##  models.TableTransformerModel

The bare Table Transformer Model (consisting of a backbone and encoder-decoder
Transformer) outputting raw hidden-states without any specific head on top.

**Kind** : static class of `models`

* * *

##  models.TableTransformerForObjectDetection

Table Transformer Model (consisting of a backbone and encoder-decoder
Transformer) with object detection heads on top, for tasks such as COCO
detection.

**Kind** : static class of `models`

* * *

###  tableTransformerForObjectDetection._call(model_inputs)

**Kind** : instance method of `TableTransformerForObjectDetection`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.ResNetPreTrainedModel

An abstract class to handle weights initialization and a simple interface for
downloading and loading pretrained models.

**Kind** : static class of `models`

* * *

##  models.ResNetModel

The bare ResNet model outputting raw features without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.ResNetForImageClassification

ResNet Model with an image classification head on top (a linear layer on top
of the pooled features), e.g. for ImageNet.

**Kind** : static class of `models`

* * *

###  resNetForImageClassification._call(model_inputs)

**Kind** : instance method of `ResNetForImageClassification`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.Swin2SRModel

The bare Swin2SR Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.Swin2SRForImageSuperResolution

Swin2SR Model transformer with an upsampler head on top for image super
resolution and restoration.

**Example:** Super-resolution w/ `Xenova/swin2SR-classical-sr-x2-64`.

Copied

    
    
    import { AutoProcessor, Swin2SRForImageSuperResolution, RawImage } from '@huggingface/transformers';
    
    // Load processor and model
    const model_id = 'Xenova/swin2SR-classical-sr-x2-64';
    const processor = await AutoProcessor.from_pretrained(model_id);
    const model = await Swin2SRForImageSuperResolution.from_pretrained(model_id);
    
    // Prepare model inputs
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/butterfly.jpg';
    const image = await RawImage.fromURL(url);
    const inputs = await processor(image);
    
    // Run model
    const outputs = await model(inputs);
    
    // Convert Tensor to RawImage
    const output = outputs.reconstruction.squeeze().clamp_(0, 1).mul_(255).round_().to('uint8');
    const outputImage = RawImage.fromTensor(output);
    // RawImage {
    //   data: Uint8Array(786432) [ 41, 31, 24, ... ],
    //   width: 512,
    //   height: 512,
    //   channels: 3
    // }

**Kind** : static class of `models`

* * *

##  models.DPTModel

The bare DPT Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.DPTForDepthEstimation

DPT Model with a depth estimation head on top (consisting of 3 convolutional
layers) e.g. for KITTI, NYUv2.

**Example:** Depth estimation w/ `Xenova/dpt-hybrid-midas`.

Copied

    
    
    import { DPTForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@huggingface/transformers';
    
    // Load model and processor
    const model_id = 'Xenova/dpt-hybrid-midas';
    const model = await DPTForDepthEstimation.from_pretrained(model_id);
    const processor = await AutoProcessor.from_pretrained(model_id);
    
    // Load image from URL
    const url = 'http://images.cocodataset.org/val2017/000000039769.jpg';
    const image = await RawImage.fromURL(url);
    
    // Prepare image for the model
    const inputs = await processor(image);
    
    // Run model
    const { predicted_depth } = await model(inputs);
    
    // Interpolate to original size
    const prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);
    
    // Visualize the prediction
    const formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');
    const depth = RawImage.fromTensor(formatted);
    // RawImage {
    //   data: Uint8Array(307200) [ 85, 85, 84, ... ],
    //   width: 640,
    //   height: 480,
    //   channels: 1
    // }

**Kind** : static class of `models`

* * *

##  models.DepthAnythingForDepthEstimation

Depth Anything Model with a depth estimation head on top (consisting of 3
convolutional layers) e.g. for KITTI, NYUv2.

**Kind** : static class of `models`

* * *

##  models.GLPNModel

The bare GLPN encoder (Mix-Transformer) outputting raw hidden-states without
any specific head on top.

**Kind** : static class of `models`

* * *

##  models.GLPNForDepthEstimation

GLPN Model transformer with a lightweight depth estimation head on top e.g.
for KITTI, NYUv2.

**Example:** Depth estimation w/ `Xenova/glpn-kitti`.

Copied

    
    
    import { GLPNForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@huggingface/transformers';
    
    // Load model and processor
    const model_id = 'Xenova/glpn-kitti';
    const model = await GLPNForDepthEstimation.from_pretrained(model_id);
    const processor = await AutoProcessor.from_pretrained(model_id);
    
    // Load image from URL
    const url = 'http://images.cocodataset.org/val2017/000000039769.jpg';
    const image = await RawImage.fromURL(url);
    
    // Prepare image for the model
    const inputs = await processor(image);
    
    // Run model
    const { predicted_depth } = await model(inputs);
    
    // Interpolate to original size
    const prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);
    
    // Visualize the prediction
    const formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');
    const depth = RawImage.fromTensor(formatted);
    // RawImage {
    //   data: Uint8Array(307200) [ 207, 169, 154, ... ],
    //   width: 640,
    //   height: 480,
    //   channels: 1
    // }

**Kind** : static class of `models`

* * *

##  models.DonutSwinModel

The bare Donut Swin Model transformer outputting raw hidden-states without any
specific head on top.

**Example:** Step-by-step Document Parsing.

Copied

    
    
    import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@huggingface/transformers';
    
    // Choose model to use
    const model_id = 'Xenova/donut-base-finetuned-cord-v2';
    
    // Prepare image inputs
    const processor = await AutoProcessor.from_pretrained(model_id);
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/receipt.png';
    const image = await RawImage.read(url);
    const image_inputs = await processor(image);
    
    // Prepare decoder inputs
    const tokenizer = await AutoTokenizer.from_pretrained(model_id);
    const task_prompt = '<s_cord-v2>';
    const decoder_input_ids = tokenizer(task_prompt, {
      add_special_tokens: false,
    }).input_ids;
    
    // Create the model
    const model = await AutoModelForVision2Seq.from_pretrained(model_id);
    
    // Run inference
    const output = await model.generate(image_inputs.pixel_values, {
      decoder_input_ids,
      max_length: model.config.decoder.max_position_embeddings,
    });
    
    // Decode output
    const decoded = tokenizer.batch_decode(output)[0];
    // <s_cord-v2><s_menu><s_nm> CINNAMON SUGAR</s_nm><s_unitprice> 17,000</s_unitprice><s_cnt> 1 x</s_cnt><s_price> 17,000</s_price></s_menu><s_sub_total><s_subtotal_price> 17,000</s_subtotal_price></s_sub_total><s_total><s_total_price> 17,000</s_total_price><s_cashprice> 20,000</s_cashprice><s_changeprice> 3,000</s_changeprice></s_total></s>

**Example:** Step-by-step Document Visual Question Answering (DocVQA)

Copied

    
    
    import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@huggingface/transformers';
    
    // Choose model to use
    const model_id = 'Xenova/donut-base-finetuned-docvqa';
    
    // Prepare image inputs
    const processor = await AutoProcessor.from_pretrained(model_id);
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/invoice.png';
    const image = await RawImage.read(url);
    const image_inputs = await processor(image);
    
    // Prepare decoder inputs
    const tokenizer = await AutoTokenizer.from_pretrained(model_id);
    const question = 'What is the invoice number?';
    const task_prompt = `<s_docvqa><s_question>${question}</s_question><s_answer>`;
    const decoder_input_ids = tokenizer(task_prompt, {
      add_special_tokens: false,
    }).input_ids;
    
    // Create the model
    const model = await AutoModelForVision2Seq.from_pretrained(model_id);
    
    // Run inference
    const output = await model.generate(image_inputs.pixel_values, {
      decoder_input_ids,
      max_length: model.config.decoder.max_position_embeddings,
    });
    
    // Decode output
    const decoded = tokenizer.batch_decode(output)[0];
    // <s_docvqa><s_question> What is the invoice number?</s_question><s_answer> us-001</s_answer></s>

**Kind** : static class of `models`

* * *

##  models.ConvNextModel

The bare ConvNext model outputting raw features without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.ConvNextForImageClassification

ConvNext Model with an image classification head on top (a linear layer on top
of the pooled features), e.g. for ImageNet.

**Kind** : static class of `models`

* * *

###  convNextForImageClassification._call(model_inputs)

**Kind** : instance method of `ConvNextForImageClassification`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.ConvNextV2Model

The bare ConvNextV2 model outputting raw features without any specific head on
top.

**Kind** : static class of `models`

* * *

##  models.ConvNextV2ForImageClassification

ConvNextV2 Model with an image classification head on top (a linear layer on
top of the pooled features), e.g. for ImageNet.

**Kind** : static class of `models`

* * *

###  convNextV2ForImageClassification._call(model_inputs)

**Kind** : instance method of `ConvNextV2ForImageClassification`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.Dinov2Model

The bare DINOv2 Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.Dinov2ForImageClassification

Dinov2 Model transformer with an image classification head on top (a linear
layer on top of the final hidden state of the [CLS] token) e.g. for ImageNet.

**Kind** : static class of `models`

* * *

###  dinov2ForImageClassification._call(model_inputs)

**Kind** : instance method of `Dinov2ForImageClassification`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.YolosObjectDetectionOutput

**Kind** : static class of `models`

* * *

###  new YolosObjectDetectionOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| Classification logits (including no-object) for all
queries.  
output.pred_boxes| `Tensor`| Normalized boxes coordinates for all queries,
represented as (center_x, center_y, width, height). These values are
normalized in [0, 1], relative to the size of each individual image in the
batch (disregarding possible padding).  
  
* * *

##  models.SamModel

Segment Anything Model (SAM) for generating segmentation masks, given an input
image and optional 2D location and bounding boxes.

**Example:** Perform mask generation w/ `Xenova/sam-vit-base`.

Copied

    
    
    import { SamModel, AutoProcessor, RawImage } from '@huggingface/transformers';
    
    const model = await SamModel.from_pretrained('Xenova/sam-vit-base');
    const processor = await AutoProcessor.from_pretrained('Xenova/sam-vit-base');
    
    const img_url = 'https://huggingface.co/ybelkada/segment-anything/resolve/v3.0.0/assets/car.png';
    const raw_image = await RawImage.read(img_url);
    const input_points = [[[450, 600]]] // 2D localization of a window
    
    const inputs = await processor(raw_image, { input_points });
    const outputs = await model(inputs);
    
    const masks = await processor.post_process_masks(outputs.pred_masks, inputs.original_sizes, inputs.reshaped_input_sizes);
    // [
    //   Tensor {
    //     dims: [ 1, 3, 1764, 2646 ],
    //     type: 'bool',
    //     data: Uint8Array(14002632) [ ... ],
    //     size: 14002632
    //   }
    // ]
    const scores = outputs.iou_scores;
    // Tensor {
    //   dims: [ 1, 1, 3 ],
    //   type: 'float32',
    //   data: Float32Array(3) [
    //     0.8892380595207214,
    //     0.9311248064041138,
    //     0.983696699142456
    //   ],
    //   size: 3
    // }

**Kind** : static class of `models`

  * .SamModel
    * `.get_image_embeddings(model_inputs)` ‚áí `Promise.<{image_embeddings: Tensor, image_positional_embeddings: Tensor}>`
    * `.forward(model_inputs)` ‚áí `Promise.<Object>`
    * `._call(model_inputs)` ‚áí `Promise.<SamImageSegmentationOutput>`

* * *

###  samModel.get_image_embeddings(model_inputs) ‚áí <code> Promise. <
{image_embeddings: Tensor, image_positional_embeddings: Tensor} > </code>

Compute image embeddings and positional image embeddings, given the pixel
values of an image.

**Kind** : instance method of `SamModel`  
**Returns** : `Promise.<{image_embeddings: Tensor,
image_positional_embeddings: Tensor}>` \- The image embeddings and positional
image embeddings.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| Object containing the model inputs.  
model_inputs.pixel_values| `Tensor`| Pixel values obtained using a
`SamProcessor`.  
  
* * *

###  samModel.forward(model_inputs) ‚áí <code> Promise. < Object > </code>

**Kind** : instance method of `SamModel`  
**Returns** : `Promise.<Object>` \- The output of the model.

Param| Type| Description  
---|---|---  
model_inputs| `SamModelInputs`| Object containing the model inputs.  
  
* * *

###  samModel._call(model_inputs) ‚áí <code> Promise. <
SamImageSegmentationOutput > </code>

Runs the model with the provided inputs

**Kind** : instance method of `SamModel`  
**Returns** : `Promise.<SamImageSegmentationOutput>` \- Object containing
segmentation outputs

Param| Type| Description  
---|---|---  
model_inputs| `Object`| Model inputs  
  
* * *

##  models.SamImageSegmentationOutput

Base class for Segment-Anything model‚Äôs output.

**Kind** : static class of `models`

* * *

###  new SamImageSegmentationOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.iou_scores| `Tensor`| The output logits of the model.  
output.pred_masks| `Tensor`| Predicted boxes.  
  
* * *

##  models.Wav2Vec2Model

The bare Wav2Vec2 Model transformer outputting raw hidden-states without any
specific head on top.

**Example:** Load and run a `Wav2Vec2Model` for feature extraction.

Copied

    
    
    import { AutoProcessor, AutoModel, read_audio } from '@huggingface/transformers';
    
    // Read and preprocess audio
    const processor = await AutoProcessor.from_pretrained('Xenova/mms-300m');
    const audio = await read_audio('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/v3.0.0/mlk.flac', 16000);
    const inputs = await processor(audio);
    
    // Run model with inputs
    const model = await AutoModel.from_pretrained('Xenova/mms-300m');
    const output = await model(inputs);
    // {
    //   last_hidden_state: Tensor {
    //     dims: [ 1, 1144, 1024 ],
    //     type: 'float32',
    //     data: Float32Array(1171456) [ ... ],
    //     size: 1171456
    //   }
    // }

**Kind** : static class of `models`

* * *

##  models.Wav2Vec2ForAudioFrameClassification

Wav2Vec2 Model with a frame classification head on top for tasks like Speaker
Diarization.

**Kind** : static class of `models`

* * *

###  wav2Vec2ForAudioFrameClassification._call(model_inputs) ‚áí <code> Promise.
< TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `Wav2Vec2ForAudioFrameClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.PyAnnoteModel

The bare PyAnnote Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.PyAnnoteForAudioFrameClassification

PyAnnote Model with a frame classification head on top for tasks like Speaker
Diarization.

**Example:** Load and run a `PyAnnoteForAudioFrameClassification` for speaker
diarization.

Copied

    
    
    import { AutoProcessor, AutoModelForAudioFrameClassification, read_audio } from '@huggingface/transformers';
    
    // Load model and processor
    const model_id = 'onnx-community/pyannote-segmentation-3.0';
    const model = await AutoModelForAudioFrameClassification.from_pretrained(model_id);
    const processor = await AutoProcessor.from_pretrained(model_id);
    
    // Read and preprocess audio
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/mlk.wav';
    const audio = await read_audio(url, processor.feature_extractor.config.sampling_rate);
    const inputs = await processor(audio);
    
    // Run model with inputs
    const { logits } = await model(inputs);
    // {
    //   logits: Tensor {
    //     dims: [ 1, 767, 7 ],  // [batch_size, num_frames, num_classes]
    //     type: 'float32',
    //     data: Float32Array(5369) [ ... ],
    //     size: 5369
    //   }
    // }
    
    const result = processor.post_process_speaker_diarization(logits, audio.length);
    // [
    //   [
    //     { id: 0, start: 0, end: 1.0512535626298245, confidence: 0.8220156481664611 },
    //     { id: 2, start: 1.0512535626298245, end: 2.3398869619825127, confidence: 0.9008811707860472 },
    //     ...
    //   ]
    // ]
    
    // Display result
    console.table(result[0], ['start', 'end', 'id', 'confidence']);
    // ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    // ‚îÇ (index) ‚îÇ start              ‚îÇ end                ‚îÇ id ‚îÇ confidence          ‚îÇ
    // ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    // ‚îÇ 0       ‚îÇ 0                  ‚îÇ 1.0512535626298245 ‚îÇ 0  ‚îÇ 0.8220156481664611  ‚îÇ
    // ‚îÇ 1       ‚îÇ 1.0512535626298245 ‚îÇ 2.3398869619825127 ‚îÇ 2  ‚îÇ 0.9008811707860472  ‚îÇ
    // ‚îÇ 2       ‚îÇ 2.3398869619825127 ‚îÇ 3.5946089560890773 ‚îÇ 0  ‚îÇ 0.7521651315796233  ‚îÇ
    // ‚îÇ 3       ‚îÇ 3.5946089560890773 ‚îÇ 4.578039708226655  ‚îÇ 2  ‚îÇ 0.8491978128022479  ‚îÇ
    // ‚îÇ 4       ‚îÇ 4.578039708226655  ‚îÇ 4.594995410849717  ‚îÇ 0  ‚îÇ 0.2935352600416393  ‚îÇ
    // ‚îÇ 5       ‚îÇ 4.594995410849717  ‚îÇ 6.121008646925269  ‚îÇ 3  ‚îÇ 0.6788051309866024  ‚îÇ
    // ‚îÇ 6       ‚îÇ 6.121008646925269  ‚îÇ 6.256654267909762  ‚îÇ 0  ‚îÇ 0.37125512393851134 ‚îÇ
    // ‚îÇ 7       ‚îÇ 6.256654267909762  ‚îÇ 8.630452635138397  ‚îÇ 2  ‚îÇ 0.7467035186353542  ‚îÇ
    // ‚îÇ 8       ‚îÇ 8.630452635138397  ‚îÇ 10.088643060721703 ‚îÇ 0  ‚îÇ 0.7689364814666032  ‚îÇ
    // ‚îÇ 9       ‚îÇ 10.088643060721703 ‚îÇ 12.58113134631177  ‚îÇ 2  ‚îÇ 0.9123324509131324  ‚îÇ
    // ‚îÇ 10      ‚îÇ 12.58113134631177  ‚îÇ 13.005023911888312 ‚îÇ 0  ‚îÇ 0.4828358177572041  ‚îÇ
    // ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

**Kind** : static class of `models`

* * *

###  pyAnnoteForAudioFrameClassification._call(model_inputs) ‚áí <code> Promise.
< TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `PyAnnoteForAudioFrameClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.UniSpeechModel

The bare UniSpeech Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.UniSpeechForCTC

UniSpeech Model with a `language modeling` head on top for Connectionist
Temporal Classification (CTC).

**Kind** : static class of `models`

* * *

###  uniSpeechForCTC._call(model_inputs)

**Kind** : instance method of `UniSpeechForCTC`

Param| Type| Description  
---|---|---  
model_inputs| `Object`|  
model_inputs.input_values| `Tensor`| Float values of input raw speech
waveform.  
model_inputs.attention_mask| `Tensor`| Mask to avoid performing convolution
and attention on padding token indices. Mask values selected in [0, 1]  
  
* * *

##  models.UniSpeechForSequenceClassification

UniSpeech Model with a sequence classification head on top (a linear layer
over the pooled output).

**Kind** : static class of `models`

* * *

###  uniSpeechForSequenceClassification._call(model_inputs) ‚áí <code> Promise.
< SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `UniSpeechForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.UniSpeechSatModel

The bare UniSpeechSat Model transformer outputting raw hidden-states without
any specific head on top.

**Kind** : static class of `models`

* * *

##  models.UniSpeechSatForCTC

UniSpeechSat Model with a `language modeling` head on top for Connectionist
Temporal Classification (CTC).

**Kind** : static class of `models`

* * *

###  uniSpeechSatForCTC._call(model_inputs)

**Kind** : instance method of `UniSpeechSatForCTC`

Param| Type| Description  
---|---|---  
model_inputs| `Object`|  
model_inputs.input_values| `Tensor`| Float values of input raw speech
waveform.  
model_inputs.attention_mask| `Tensor`| Mask to avoid performing convolution
and attention on padding token indices. Mask values selected in [0, 1]  
  
* * *

##  models.UniSpeechSatForSequenceClassification

UniSpeechSat Model with a sequence classification head on top (a linear layer
over the pooled output).

**Kind** : static class of `models`

* * *

###  uniSpeechSatForSequenceClassification._call(model_inputs) ‚áí <code>
Promise. < SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `UniSpeechSatForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.UniSpeechSatForAudioFrameClassification

UniSpeechSat Model with a frame classification head on top for tasks like
Speaker Diarization.

**Kind** : static class of `models`

* * *

###  uniSpeechSatForAudioFrameClassification._call(model_inputs) ‚áí <code>
Promise. < TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `UniSpeechSatForAudioFrameClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.Wav2Vec2BertModel

The bare Wav2Vec2Bert Model transformer outputting raw hidden-states without
any specific head on top.

**Kind** : static class of `models`

* * *

##  models.Wav2Vec2BertForCTC

Wav2Vec2Bert Model with a `language modeling` head on top for Connectionist
Temporal Classification (CTC).

**Kind** : static class of `models`

* * *

###  wav2Vec2BertForCTC._call(model_inputs)

**Kind** : instance method of `Wav2Vec2BertForCTC`

Param| Type| Description  
---|---|---  
model_inputs| `Object`|  
model_inputs.input_features| `Tensor`| Float values of input mel-spectrogram.  
model_inputs.attention_mask| `Tensor`| Mask to avoid performing convolution
and attention on padding token indices. Mask values selected in [0, 1]  
  
* * *

##  models.Wav2Vec2BertForSequenceClassification

Wav2Vec2Bert Model with a sequence classification head on top (a linear layer
over the pooled output).

**Kind** : static class of `models`

* * *

###  wav2Vec2BertForSequenceClassification._call(model_inputs) ‚áí <code>
Promise. < SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `Wav2Vec2BertForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.HubertModel

The bare Hubert Model transformer outputting raw hidden-states without any
specific head on top.

**Example:** Load and run a `HubertModel` for feature extraction.

Copied

    
    
    import { AutoProcessor, AutoModel, read_audio } from '@huggingface/transformers';
    
    // Read and preprocess audio
    const processor = await AutoProcessor.from_pretrained('Xenova/hubert-base-ls960');
    const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/jfk.wav', 16000);
    const inputs = await processor(audio);
    
    // Load and run model with inputs
    const model = await AutoModel.from_pretrained('Xenova/hubert-base-ls960');
    const output = await model(inputs);
    // {
    //   last_hidden_state: Tensor {
    //     dims: [ 1, 549, 768 ],
    //     type: 'float32',
    //     data: Float32Array(421632) [0.0682469978928566, 0.08104046434164047, -0.4975186586380005, ...],
    //     size: 421632
    //   }
    // }

**Kind** : static class of `models`

* * *

##  models.HubertForCTC

Hubert Model with a `language modeling` head on top for Connectionist Temporal
Classification (CTC).

**Kind** : static class of `models`

* * *

###  hubertForCTC._call(model_inputs)

**Kind** : instance method of `HubertForCTC`

Param| Type| Description  
---|---|---  
model_inputs| `Object`|  
model_inputs.input_values| `Tensor`| Float values of input raw speech
waveform.  
model_inputs.attention_mask| `Tensor`| Mask to avoid performing convolution
and attention on padding token indices. Mask values selected in [0, 1]  
  
* * *

##  models.HubertForSequenceClassification

Hubert Model with a sequence classification head on top (a linear layer over
the pooled output) for tasks like SUPERB Keyword Spotting.

**Kind** : static class of `models`

* * *

###  hubertForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `HubertForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.WavLMPreTrainedModel

An abstract class to handle weights initialization and a simple interface for
downloading and loading pretrained models.

**Kind** : static class of `models`

* * *

##  models.WavLMModel

The bare WavLM Model transformer outputting raw hidden-states without any
specific head on top.

**Example:** Load and run a `WavLMModel` for feature extraction.

Copied

    
    
    import { AutoProcessor, AutoModel, read_audio } from '@huggingface/transformers';
    
    // Read and preprocess audio
    const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base');
    const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/jfk.wav', 16000);
    const inputs = await processor(audio);
    
    // Run model with inputs
    const model = await AutoModel.from_pretrained('Xenova/wavlm-base');
    const output = await model(inputs);
    // {
    //   last_hidden_state: Tensor {
    //     dims: [ 1, 549, 768 ],
    //     type: 'float32',
    //     data: Float32Array(421632) [-0.349443256855011, -0.39341306686401367,  0.022836603224277496, ...],
    //     size: 421632
    //   }
    // }

**Kind** : static class of `models`

* * *

##  models.WavLMForCTC

WavLM Model with a `language modeling` head on top for Connectionist Temporal
Classification (CTC).

**Kind** : static class of `models`

* * *

###  wavLMForCTC._call(model_inputs)

**Kind** : instance method of `WavLMForCTC`

Param| Type| Description  
---|---|---  
model_inputs| `Object`|  
model_inputs.input_values| `Tensor`| Float values of input raw speech
waveform.  
model_inputs.attention_mask| `Tensor`| Mask to avoid performing convolution
and attention on padding token indices. Mask values selected in [0, 1]  
  
* * *

##  models.WavLMForSequenceClassification

WavLM Model with a sequence classification head on top (a linear layer over
the pooled output).

**Kind** : static class of `models`

* * *

###  wavLMForSequenceClassification._call(model_inputs) ‚áí <code> Promise. <
SequenceClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `WavLMForSequenceClassification`  
**Returns** : `Promise.<SequenceClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.WavLMForXVector

WavLM Model with an XVector feature extraction head on top for tasks like
Speaker Verification.

**Example:** Extract speaker embeddings with `WavLMForXVector`.

Copied

    
    
    import { AutoProcessor, AutoModel, read_audio } from '@huggingface/transformers';
    
    // Read and preprocess audio
    const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base-plus-sv');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/jfk.wav';
    const audio = await read_audio(url, 16000);
    const inputs = await processor(audio);
    
    // Run model with inputs
    const model = await AutoModel.from_pretrained('Xenova/wavlm-base-plus-sv');
    const outputs = await model(inputs);
    // {
    //   logits: Tensor {
    //     dims: [ 1, 512 ],
    //     type: 'float32',
    //     data: Float32Array(512) [0.5847219228744507, ...],
    //     size: 512
    //   },
    //   embeddings: Tensor {
    //     dims: [ 1, 512 ],
    //     type: 'float32',
    //     data: Float32Array(512) [-0.09079201519489288, ...],
    //     size: 512
    //   }
    // }

**Kind** : static class of `models`

* * *

###  wavLMForXVector._call(model_inputs) ‚áí <code> Promise. < XVectorOutput >
</code>

Calls the model on new inputs.

**Kind** : instance method of `WavLMForXVector`  
**Returns** : `Promise.<XVectorOutput>` \- An object containing the model‚Äôs
output logits and speaker embeddings.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.WavLMForAudioFrameClassification

WavLM Model with a frame classification head on top for tasks like Speaker
Diarization.

**Example:** Perform speaker diarization with
`WavLMForAudioFrameClassification`.

Copied

    
    
    import { AutoProcessor, AutoModelForAudioFrameClassification, read_audio } from '@huggingface/transformers';
    
    // Read and preprocess audio
    const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base-plus-sd');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/jfk.wav';
    const audio = await read_audio(url, 16000);
    const inputs = await processor(audio);
    
    // Run model with inputs
    const model = await AutoModelForAudioFrameClassification.from_pretrained('Xenova/wavlm-base-plus-sd');
    const { logits } = await model(inputs);
    // {
    //   logits: Tensor {
    //     dims: [ 1, 549, 2 ],  // [batch_size, num_frames, num_speakers]
    //     type: 'float32',
    //     data: Float32Array(1098) [-3.5301010608673096, ...],
    //     size: 1098
    //   }
    // }
    
    const labels = logits[0].sigmoid().tolist().map(
        frames => frames.map(speaker => speaker > 0.5 ? 1 : 0)
    );
    console.log(labels); // labels is a one-hot array of shape (num_frames, num_speakers)
    // [
    //     [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0],
    //     [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0],
    //     [0, 0], [0, 1], [0, 1], [0, 1], [0, 1], [0, 1],
    //     ...
    // ]

**Kind** : static class of `models`

* * *

###  wavLMForAudioFrameClassification._call(model_inputs) ‚áí <code> Promise. <
TokenClassifierOutput > </code>

Calls the model on new inputs.

**Kind** : instance method of `WavLMForAudioFrameClassification`  
**Returns** : `Promise.<TokenClassifierOutput>` \- An object containing the
model‚Äôs output logits for sequence classification.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.SpeechT5PreTrainedModel

An abstract class to handle weights initialization and a simple interface for
downloading and loading pretrained models.

**Kind** : static class of `models`

* * *

##  models.SpeechT5Model

The bare SpeechT5 Encoder-Decoder Model outputting raw hidden-states without
any specific pre- or post-nets.

**Kind** : static class of `models`

* * *

##  models.SpeechT5ForSpeechToText

SpeechT5 Model with a speech encoder and a text decoder.

**Example:** Generate speech from text with `SpeechT5ForSpeechToText`.

Copied

    
    
    import { AutoTokenizer, AutoProcessor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, Tensor } from '@huggingface/transformers';
    
    // Load the tokenizer and processor
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/speecht5_tts');
    const processor = await AutoProcessor.from_pretrained('Xenova/speecht5_tts');
    
    // Load the models
    // NOTE: We use the full-precision versions as they are more accurate
    const model = await SpeechT5ForTextToSpeech.from_pretrained('Xenova/speecht5_tts', { dtype: 'fp32' });
    const vocoder = await SpeechT5HifiGan.from_pretrained('Xenova/speecht5_hifigan', { dtype: 'fp32' });
    
    // Load speaker embeddings from URL
    const speaker_embeddings_data = new Float32Array(
        await (await fetch('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/speaker_embeddings.bin')).arrayBuffer()
    );
    const speaker_embeddings = new Tensor(
        'float32',
        speaker_embeddings_data,
        [1, speaker_embeddings_data.length]
    )
    
    // Run tokenization
    const { input_ids } = tokenizer('Hello, my dog is cute');
    
    // Generate waveform
    const { waveform } = await model.generate_speech(input_ids, speaker_embeddings, { vocoder });
    console.log(waveform)
    // Tensor {
    //   dims: [ 26112 ],
    //   type: 'float32',
    //   size: 26112,
    //   data: Float32Array(26112) [ -0.00043630177970044315, -0.00018082228780258447, ... ],
    // }

**Kind** : static class of `models`

* * *

##  models.SpeechT5ForTextToSpeech

SpeechT5 Model with a text encoder and a speech decoder.

**Kind** : static class of `models`

* * *

###  speechT5ForTextToSpeech.generate_speech(input_values, speaker_embeddings,
options) ‚áí <code> Promise. < SpeechOutput > </code>

Converts a sequence of input tokens into a sequence of mel spectrograms, which
are subsequently turned into a speech waveform using a vocoder.

**Kind** : instance method of `SpeechT5ForTextToSpeech`  
**Returns** : `Promise.<SpeechOutput>` \- A promise which resolves to an
object containing the spectrogram, waveform, and cross-attention tensors.

Param| Type| Default| Description  
---|---|---|---  
input_values| `Tensor`| | Indices of input sequence tokens in the vocabulary.  
speaker_embeddings| `Tensor`| | Tensor containing the speaker embeddings.  
options| `Object`| | Optional parameters for generating speech.  
[options.threshold]| `number`| `0.5`| The generated sequence ends when the
predicted stop token probability exceeds this value.  
[options.minlenratio]| `number`| `0.0`| Used to calculate the minimum required
length for the output sequence.  
[options.maxlenratio]| `number`| `20.0`| Used to calculate the maximum allowed
length for the output sequence.  
[options.vocoder]| `Object`| ``| The vocoder that converts the mel spectrogram
into a speech waveform. If `null`, the output is the mel spectrogram.  
[options.output_cross_attentions]| `boolean`| `false`| Whether or not to
return the attentions tensors of the decoder's cross-attention layers.  
  
* * *

##  models.SpeechT5HifiGan

HiFi-GAN vocoder.

See [SpeechT5ForSpeechToText](./models#module_models.SpeechT5ForSpeechToText)
for example usage.

**Kind** : static class of `models`

* * *

##  models.TrOCRForCausalLM

The TrOCR Decoder with a language modeling head.

**Kind** : static class of `models`

* * *

##  models.MistralPreTrainedModel

The bare Mistral Model outputting raw hidden-states without any specific head
on top.

**Kind** : static class of `models`

* * *

##  models.Starcoder2PreTrainedModel

The bare Starcoder2 Model outputting raw hidden-states without any specific
head on top.

**Kind** : static class of `models`

* * *

##  models.FalconPreTrainedModel

The bare Falcon Model outputting raw hidden-states without any specific head
on top.

**Kind** : static class of `models`

* * *

##  models.ClapTextModelWithProjection

CLAP Text Model with a projection layer on top (a linear layer on top of the
pooled output).

**Example:** Compute text embeddings with `ClapTextModelWithProjection`.

Copied

    
    
    import { AutoTokenizer, ClapTextModelWithProjection } from '@huggingface/transformers';
    
    // Load tokenizer and text model
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clap-htsat-unfused');
    const text_model = await ClapTextModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');
    
    // Run tokenization
    const texts = ['a sound of a cat', 'a sound of a dog'];
    const text_inputs = tokenizer(texts, { padding: true, truncation: true });
    
    // Compute embeddings
    const { text_embeds } = await text_model(text_inputs);
    // Tensor {
    //   dims: [ 2, 512 ],
    //   type: 'float32',
    //   data: Float32Array(1024) [ ... ],
    //   size: 1024
    // }

**Kind** : static class of `models`

* * *

###  ClapTextModelWithProjection.from_pretrained() : <code>
PreTrainedModel.from_pretrained </code>

**Kind** : static method of `ClapTextModelWithProjection`

* * *

##  models.ClapAudioModelWithProjection

CLAP Audio Model with a projection layer on top (a linear layer on top of the
pooled output).

**Example:** Compute audio embeddings with `ClapAudioModelWithProjection`.

Copied

    
    
    import { AutoProcessor, ClapAudioModelWithProjection, read_audio } from '@huggingface/transformers';
    
    // Load processor and audio model
    const processor = await AutoProcessor.from_pretrained('Xenova/clap-htsat-unfused');
    const audio_model = await ClapAudioModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');
    
    // Read audio and run processor
    const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/cat_meow.wav');
    const audio_inputs = await processor(audio);
    
    // Compute embeddings
    const { audio_embeds } = await audio_model(audio_inputs);
    // Tensor {
    //   dims: [ 1, 512 ],
    //   type: 'float32',
    //   data: Float32Array(512) [ ... ],
    //   size: 512
    // }

**Kind** : static class of `models`

* * *

###  ClapAudioModelWithProjection.from_pretrained() : <code>
PreTrainedModel.from_pretrained </code>

**Kind** : static method of `ClapAudioModelWithProjection`

* * *

##  models.VitsModel

The complete VITS model, for text-to-speech synthesis.

**Example:** Generate speech from text with `VitsModel`.

Copied

    
    
    import { AutoTokenizer, VitsModel } from '@huggingface/transformers';
    
    // Load the tokenizer and model
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/mms-tts-eng');
    const model = await VitsModel.from_pretrained('Xenova/mms-tts-eng');
    
    // Run tokenization
    const inputs = tokenizer('I love transformers');
    
    // Generate waveform
    const { waveform } = await model(inputs);
    // Tensor {
    //   dims: [ 1, 35328 ],
    //   type: 'float32',
    //   data: Float32Array(35328) [ ... ],
    //   size: 35328,
    // }

**Kind** : static class of `models`

* * *

###  vitsModel._call(model_inputs) ‚áí <code> Promise. < VitsModelOutput >
</code>

Calls the model on new inputs.

**Kind** : instance method of `VitsModel`  
**Returns** : `Promise.<VitsModelOutput>` \- The outputs for the VITS model.

Param| Type| Description  
---|---|---  
model_inputs| `Object`| The inputs to the model.  
  
* * *

##  models.SegformerModel

The bare SegFormer encoder (Mix-Transformer) outputting raw hidden-states
without any specific head on top.

**Kind** : static class of `models`

* * *

##  models.SegformerForImageClassification

SegFormer Model transformer with an image classification head on top (a linear
layer on top of the final hidden states) e.g. for ImageNet.

**Kind** : static class of `models`

* * *

##  models.SegformerForSemanticSegmentation

SegFormer Model transformer with an all-MLP decode head on top e.g. for
ADE20k, CityScapes.

**Kind** : static class of `models`

* * *

##  models.StableLmModel

The bare StableLm Model transformer outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.StableLmForCausalLM

StableLm Model with a `language modeling` head on top for Causal Language
Modeling (with past).

**Kind** : static class of `models`

* * *

##  models.EfficientNetModel

The bare EfficientNet model outputting raw features without any specific head
on top.

**Kind** : static class of `models`

* * *

##  models.EfficientNetForImageClassification

EfficientNet Model with an image classification head on top (a linear layer on
top of the pooled features).

**Kind** : static class of `models`

* * *

###  efficientNetForImageClassification._call(model_inputs)

**Kind** : instance method of `EfficientNetForImageClassification`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.MusicgenModel

The bare Musicgen decoder model outputting raw hidden-states without any
specific head on top.

**Kind** : static class of `models`

* * *

##  models.MusicgenForCausalLM

The MusicGen decoder model with a language modelling head on top.

**Kind** : static class of `models`

* * *

##  models.MusicgenForConditionalGeneration

The composite MusicGen model with a text encoder, audio encoder and Musicgen
decoder, for music generation tasks with one or both of text and audio
prompts.

**Example:** Generate music from text with `Xenova/musicgen-small`.

Copied

    
    
    import { AutoTokenizer, MusicgenForConditionalGeneration } from '@huggingface/transformers';
    
    // Load tokenizer and model
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/musicgen-small');
    const model = await MusicgenForConditionalGeneration.from_pretrained(
      'Xenova/musicgen-small', { dtype: 'fp32' }
    );
    
    // Prepare text input
    const prompt = '80s pop track with bassy drums and synth';
    const inputs = tokenizer(prompt);
    
    // Generate audio
    const audio_values = await model.generate({
      ...inputs,
      max_new_tokens: 512,
      do_sample: true,
      guidance_scale: 3,
    });
    
    // (Optional) Write the output to a WAV file
    import wavefile from 'wavefile';
    import fs from 'fs';
    
    const wav = new wavefile.WaveFile();
    wav.fromScratch(1, model.config.audio_encoder.sampling_rate, '32f', audio_values.data);
    fs.writeFileSync('musicgen_out.wav', wav.toBuffer());

**Kind** : static class of `models`

  * .MusicgenForConditionalGeneration
    * `._apply_and_filter_by_delay_pattern_mask(outputs)` ‚áí `Tensor`
    * `.generate(options)` ‚áí `Promise.<(ModelOutput|Tensor)>`

* * *

###
musicgenForConditionalGeneration._apply_and_filter_by_delay_pattern_mask(outputs)
‚áí <code> Tensor </code>

Apply the pattern mask to the final ids, then revert the pattern delay mask by
filtering the pad token id in a single step.

**Kind** : instance method of `MusicgenForConditionalGeneration`  
**Returns** : `Tensor` \- The filtered output tensor.

Param| Type| Description  
---|---|---  
outputs| `Tensor`| The output tensor from the model.  
  
* * *

###  musicgenForConditionalGeneration.generate(options) ‚áí <code> Promise. <
(ModelOutput|Tensor) > </code>

Generates sequences of token ids for models with a language modeling head.

**Kind** : instance method of `MusicgenForConditionalGeneration`  
**Returns** : `Promise.<(ModelOutput|Tensor)>` \- The output of the model,
which can contain the generated token ids, attentions, and scores.

Param| Type  
---|---  
options| `*`  
  
* * *

##  models.MobileNetV1Model

The bare MobileNetV1 model outputting raw hidden-states without any specific
head on top.

**Kind** : static class of `models`

* * *

##  models.MobileNetV1ForImageClassification

MobileNetV1 model with an image classification head on top (a linear layer on
top of the pooled features), e.g. for ImageNet.

**Kind** : static class of `models`

* * *

###  mobileNetV1ForImageClassification._call(model_inputs)

**Kind** : instance method of `MobileNetV1ForImageClassification`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.MobileNetV2Model

The bare MobileNetV2 model outputting raw hidden-states without any specific
head on top.

**Kind** : static class of `models`

* * *

##  models.MobileNetV2ForImageClassification

MobileNetV2 model with an image classification head on top (a linear layer on
top of the pooled features), e.g. for ImageNet.

**Kind** : static class of `models`

* * *

###  mobileNetV2ForImageClassification._call(model_inputs)

**Kind** : instance method of `MobileNetV2ForImageClassification`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.MobileNetV3Model

The bare MobileNetV3 model outputting raw hidden-states without any specific
head on top.

**Kind** : static class of `models`

* * *

##  models.MobileNetV3ForImageClassification

MobileNetV3 model with an image classification head on top (a linear layer on
top of the pooled features), e.g. for ImageNet.

**Kind** : static class of `models`

* * *

###  mobileNetV3ForImageClassification._call(model_inputs)

**Kind** : instance method of `MobileNetV3ForImageClassification`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.MobileNetV4Model

The bare MobileNetV4 model outputting raw hidden-states without any specific
head on top.

**Kind** : static class of `models`

* * *

##  models.MobileNetV4ForImageClassification

MobileNetV4 model with an image classification head on top (a linear layer on
top of the pooled features), e.g. for ImageNet.

**Kind** : static class of `models`

* * *

###  mobileNetV4ForImageClassification._call(model_inputs)

**Kind** : instance method of `MobileNetV4ForImageClassification`

Param| Type  
---|---  
model_inputs| `any`  
  
* * *

##  models.DecisionTransformerModel

The model builds upon the GPT2 architecture to perform autoregressive
prediction of actions in an offline RL setting. Refer to the paper for more
details: <https://arxiv.org/abs/2106.01345>

**Kind** : static class of `models`

* * *

##  models.PretrainedMixin

Base class of all AutoModels. Contains the `from_pretrained` function which is
used to instantiate pretrained models.

**Kind** : static class of `models`

  * .PretrainedMixin
    *  _instance_
      * `.MODEL_CLASS_MAPPINGS` : `*`
      * `.BASE_IF_FAIL`
    * _static_
      * `.from_pretrained()` : `*`

* * *

###  pretrainedMixin.MODEL_CLASS_MAPPINGS : <code> * </code>

Mapping from model type to model class.

**Kind** : instance property of `PretrainedMixin`

* * *

###  pretrainedMixin.BASE_IF_FAIL

Whether to attempt to instantiate the base class (`PretrainedModel`) if the
model type is not found in the mapping.

**Kind** : instance property of `PretrainedMixin`

* * *

###  PretrainedMixin.from_pretrained() : <code> * </code>

**Kind** : static method of `PretrainedMixin`

* * *

##  models.AutoModel

Helper class which is used to instantiate pretrained models with the
`from_pretrained` function. The chosen model class is determined by the type
specified in the model config.

**Kind** : static class of `models`

* * *

###  autoModel.MODEL_CLASS_MAPPINGS : <code> * </code>

**Kind** : instance property of `AutoModel`

* * *

##  models.AutoModelForSequenceClassification

Helper class which is used to instantiate pretrained sequence classification
models with the `from_pretrained` function. The chosen model class is
determined by the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForTokenClassification

Helper class which is used to instantiate pretrained token classification
models with the `from_pretrained` function. The chosen model class is
determined by the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForSeq2SeqLM

Helper class which is used to instantiate pretrained sequence-to-sequence
models with the `from_pretrained` function. The chosen model class is
determined by the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForSpeechSeq2Seq

Helper class which is used to instantiate pretrained sequence-to-sequence
speech-to-text models with the `from_pretrained` function. The chosen model
class is determined by the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForTextToSpectrogram

Helper class which is used to instantiate pretrained sequence-to-sequence
text-to-spectrogram models with the `from_pretrained` function. The chosen
model class is determined by the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForTextToWaveform

Helper class which is used to instantiate pretrained text-to-waveform models
with the `from_pretrained` function. The chosen model class is determined by
the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForCausalLM

Helper class which is used to instantiate pretrained causal language models
with the `from_pretrained` function. The chosen model class is determined by
the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForMaskedLM

Helper class which is used to instantiate pretrained masked language models
with the `from_pretrained` function. The chosen model class is determined by
the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForQuestionAnswering

Helper class which is used to instantiate pretrained question answering models
with the `from_pretrained` function. The chosen model class is determined by
the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForVision2Seq

Helper class which is used to instantiate pretrained vision-to-sequence models
with the `from_pretrained` function. The chosen model class is determined by
the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForImageClassification

Helper class which is used to instantiate pretrained image classification
models with the `from_pretrained` function. The chosen model class is
determined by the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForImageSegmentation

Helper class which is used to instantiate pretrained image segmentation models
with the `from_pretrained` function. The chosen model class is determined by
the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForSemanticSegmentation

Helper class which is used to instantiate pretrained image segmentation models
with the `from_pretrained` function. The chosen model class is determined by
the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForUniversalSegmentation

Helper class which is used to instantiate pretrained universal image
segmentation models with the `from_pretrained` function. The chosen model
class is determined by the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForObjectDetection

Helper class which is used to instantiate pretrained object detection models
with the `from_pretrained` function. The chosen model class is determined by
the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.AutoModelForMaskGeneration

Helper class which is used to instantiate pretrained mask generation models
with the `from_pretrained` function. The chosen model class is determined by
the type specified in the model config.

**Kind** : static class of `models`

* * *

##  models.Seq2SeqLMOutput

**Kind** : static class of `models`

* * *

###  new Seq2SeqLMOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| The output logits of the model.  
output.past_key_values| `Tensor`| An tensor of key/value pairs that represent
the previous state of the model.  
output.encoder_outputs| `Tensor`| The output of the encoder in a sequence-to-
sequence model.  
[output.decoder_attentions]| `Tensor`| Attentions weights of the decoder,
after the attention softmax, used to compute the weighted average in the self-
attention heads.  
[output.cross_attentions]| `Tensor`| Attentions weights of the decoder's
cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.  
  
* * *

##  models.SequenceClassifierOutput

Base class for outputs of sentence classification models.

**Kind** : static class of `models`

* * *

###  new SequenceClassifierOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| classification (or regression if
config.num_labels==1) scores (before SoftMax).  
  
* * *

##  models.XVectorOutput

Base class for outputs of XVector models.

**Kind** : static class of `models`

* * *

###  new XVectorOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| Classification hidden states before AMSoftmax, of
shape `(batch_size, config.xvector_output_dim)`.  
output.embeddings| `Tensor`| Utterance embeddings used for vector similarity-
based retrieval, of shape `(batch_size, config.xvector_output_dim)`.  
  
* * *

##  models.TokenClassifierOutput

Base class for outputs of token classification models.

**Kind** : static class of `models`

* * *

###  new TokenClassifierOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| Classification scores (before SoftMax).  
  
* * *

##  models.MaskedLMOutput

Base class for masked language models outputs.

**Kind** : static class of `models`

* * *

###  new MaskedLMOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| Prediction scores of the language modeling head
(scores for each vocabulary token before SoftMax).  
  
* * *

##  models.QuestionAnsweringModelOutput

Base class for outputs of question answering models.

**Kind** : static class of `models`

* * *

###  new QuestionAnsweringModelOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.start_logits| `Tensor`| Span-start scores (before SoftMax).  
output.end_logits| `Tensor`| Span-end scores (before SoftMax).  
  
* * *

##  models.CausalLMOutput

Base class for causal language model (or autoregressive) outputs.

**Kind** : static class of `models`

* * *

###  new CausalLMOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| Prediction scores of the language modeling head
(scores for each vocabulary token before softmax).  
  
* * *

##  models.CausalLMOutputWithPast

Base class for causal language model (or autoregressive) outputs.

**Kind** : static class of `models`

* * *

###  new CausalLMOutputWithPast(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.logits| `Tensor`| Prediction scores of the language modeling head
(scores for each vocabulary token before softmax).  
output.past_key_values| `Tensor`| Contains pre-computed hidden-states (key and
values in the self-attention blocks) that can be used (see `past_key_values`
input) to speed up sequential decoding.  
  
* * *

##  models.ImageMattingOutput

**Kind** : static class of `models`

* * *

###  new ImageMattingOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.alphas| `Tensor`| Estimated alpha values, of shape `(batch_size,
num_channels, height, width)`.  
  
* * *

##  models.VitsModelOutput

Describes the outputs for the VITS model.

**Kind** : static class of `models`

* * *

###  new VitsModelOutput(output)

Param| Type| Description  
---|---|---  
output| `Object`| The output of the model.  
output.waveform| `Tensor`| The final audio waveform predicted by the model, of
shape `(batch_size, sequence_length)`.  
output.spectrogram| `Tensor`| The log-mel spectrogram predicted at the output
of the flow model. This spectrogram is passed to the Hi-Fi GAN decoder model
to obtain the final audio waveform.  
  
* * *

##  models~SamModelInputs : <code> Object </code>

Object containing the model inputs.

**Kind** : inner typedef of `models`  
**Properties**

Name| Type| Description  
---|---|---  
pixel_values| `Tensor`| Pixel values as a Tensor with shape `(batch_size,
num_channels, height, width)`. These can be obtained using a `SamProcessor`.  
[input_points]| `Tensor`| Input 2D spatial points with shape `(batch_size,
num_points, 2)`. This is used by the prompt encoder to encode the prompt.  
[input_labels]| `Tensor`| Input labels for the points, as a Tensor of shape
`(batch_size, point_batch_size, num_points)`. This is used by the prompt
encoder to encode the prompt. There are 4 types of labels:

  * `1`: the point is a point that contains the object of interest
  * `0`: the point is a point that does not contain the object of interest
  * `-1`: the point corresponds to the background
  * `-10`: the point is a padding point, thus should be ignored by the prompt encoder

  
[input_boxes]| `Tensor`| Input bounding boxes with shape `(batch_size,
num_boxes, 4)`.  
[image_embeddings]| `Tensor`| Image embeddings used by the mask decoder.  
[image_positional_embeddings]| `Tensor`| Image positional embeddings used by
the mask decoder.  
  
* * *

##  models~SpeechOutput : <code> Object </code>

**Kind** : inner typedef of `models`  
**Properties**

Name| Type| Description  
---|---|---  
[spectrogram]| `Tensor`| The predicted log-mel spectrogram of shape
`(output_sequence_length, config.num_mel_bins)`. Returned when no `vocoder` is
provided  
[waveform]| `Tensor`| The predicted waveform of shape `(num_frames,)`.
Returned when a `vocoder` is provided.  
[cross_attentions]| `Tensor`| The outputs of the decoder's cross-attention
layers of shape `(config.decoder_layers, config.decoder_attention_heads,
output_sequence_length, input_sequence_length)`. returned when
`output_cross_attentions` is `true`.  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/models.md)

[‚ÜêPipelines](/docs/transformers.js/v3.0.0/en/api/pipelines)
[Tokenizers‚Üí](/docs/transformers.js/v3.0.0/en/api/tokenizers)

models models.PreTrainedModel new PreTrainedModel(config, sessions, configs) preTrainedModel.custom_config : ` * ` preTrainedModel.generation_config ‚áí ` GenerationConfig ` | ` null ` preTrainedModel.dispose() ‚áí ` Promise. < Array < unknown > > ` preTrainedModel._call(model_inputs) ‚áí ` Promise. < Object > ` preTrainedModel.forward(model_inputs) ‚áí ` Promise. < Object > ` preTrainedModel._get_logits_warper(generation_config) ‚áí ` LogitsProcessorList ` preTrainedModel._prepare_generation_config(generation_config, kwargs) ‚áí ` GenerationConfig ` preTrainedModel._get_stopping_criteria(generation_config, [stopping_criteria]) preTrainedModel._validate_model_class() preTrainedModel._update_model_kwargs_for_generation(inputs) ‚áí ` Object ` preTrainedModel._prepare_model_inputs(params) ‚áí ` Object ` preTrainedModel._prepare_decoder_input_ids_for_generation(param0) preTrainedModel.generate(options) ‚áí ` Promise. < (ModelOutput|Tensor) > ` preTrainedModel.getPastKeyValues(decoderResults, pastKeyValues) ‚áí ` Object ` preTrainedModel.getAttentions(model_output) ‚áí ` * ` preTrainedModel.addPastKeyValues(decoderFeeds, pastKeyValues) PreTrainedModel.from_pretrained(pretrained_model_name_or_path, options) ‚áí ` Promise. < PreTrainedModel > ` models.BaseModelOutput new BaseModelOutput(output) models.BertForMaskedLM bertForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.BertForSequenceClassification bertForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.BertForTokenClassification bertForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.BertForQuestionAnswering bertForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.RoFormerModel models.RoFormerForMaskedLM roFormerForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.RoFormerForSequenceClassification roFormerForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.RoFormerForTokenClassification roFormerForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.RoFormerForQuestionAnswering roFormerForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.ConvBertModel models.ConvBertForMaskedLM convBertForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.ConvBertForSequenceClassification convBertForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.ConvBertForTokenClassification convBertForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.ConvBertForQuestionAnswering convBertForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.ElectraModel models.ElectraForMaskedLM electraForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.ElectraForSequenceClassification electraForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.ElectraForTokenClassification electraForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.ElectraForQuestionAnswering electraForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.CamembertModel models.CamembertForMaskedLM camembertForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.CamembertForSequenceClassification camembertForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.CamembertForTokenClassification camembertForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.CamembertForQuestionAnswering camembertForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.DebertaModel models.DebertaForMaskedLM debertaForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.DebertaForSequenceClassification debertaForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.DebertaForTokenClassification debertaForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.DebertaForQuestionAnswering debertaForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.DebertaV2Model models.DebertaV2ForMaskedLM debertaV2ForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.DebertaV2ForSequenceClassification debertaV2ForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.DebertaV2ForTokenClassification debertaV2ForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.DebertaV2ForQuestionAnswering debertaV2ForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.DistilBertForSequenceClassification distilBertForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.DistilBertForTokenClassification distilBertForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.DistilBertForQuestionAnswering distilBertForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.DistilBertForMaskedLM distilBertForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.EsmModel models.EsmForMaskedLM esmForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.EsmForSequenceClassification esmForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.EsmForTokenClassification esmForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.MobileBertForMaskedLM mobileBertForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.MobileBertForSequenceClassification mobileBertForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.MobileBertForQuestionAnswering mobileBertForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.MPNetModel models.MPNetForMaskedLM mpNetForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.MPNetForSequenceClassification mpNetForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.MPNetForTokenClassification mpNetForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.MPNetForQuestionAnswering mpNetForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.T5ForConditionalGeneration models.LongT5PreTrainedModel models.LongT5Model models.LongT5ForConditionalGeneration models.MT5ForConditionalGeneration models.BartModel models.BartForConditionalGeneration models.BartForSequenceClassification bartForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.MBartModel models.MBartForConditionalGeneration models.MBartForSequenceClassification mBartForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.BlenderbotModel models.BlenderbotForConditionalGeneration models.BlenderbotSmallModel models.BlenderbotSmallForConditionalGeneration models.RobertaForMaskedLM robertaForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.RobertaForSequenceClassification robertaForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.RobertaForTokenClassification robertaForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.RobertaForQuestionAnswering robertaForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.XLMPreTrainedModel models.XLMModel models.XLMWithLMHeadModel xlmWithLMHeadModel._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.XLMForSequenceClassification xlmForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.XLMForTokenClassification xlmForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.XLMForQuestionAnswering xlmForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.XLMRobertaForMaskedLM xlmRobertaForMaskedLM._call(model_inputs) ‚áí ` Promise. < MaskedLMOutput > ` models.XLMRobertaForSequenceClassification xlmRobertaForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.XLMRobertaForTokenClassification xlmRobertaForTokenClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.XLMRobertaForQuestionAnswering xlmRobertaForQuestionAnswering._call(model_inputs) ‚áí ` Promise. < QuestionAnsweringModelOutput > ` models.ASTModel models.ASTForAudioClassification models.WhisperModel models.WhisperForConditionalGeneration whisperForConditionalGeneration._retrieve_init_tokens(generation_config) whisperForConditionalGeneration.generate(options) ‚áí ` Promise. < (ModelOutput|Tensor) > ` whisperForConditionalGeneration._extract_token_timestamps(generate_outputs, alignment_heads, [num_frames], [time_precision]) ‚áí ` Tensor ` models.VisionEncoderDecoderModel models.LlavaForConditionalGeneration models.CLIPModel models.CLIPTextModel CLIPTextModel.from_pretrained() : ` PreTrainedModel.from_pretrained ` models.CLIPTextModelWithProjection CLIPTextModelWithProjection.from_pretrained() : ` PreTrainedModel.from_pretrained ` models.CLIPVisionModel CLIPVisionModel.from_pretrained() : ` PreTrainedModel.from_pretrained ` models.CLIPVisionModelWithProjection CLIPVisionModelWithProjection.from_pretrained() : ` PreTrainedModel.from_pretrained ` models.SiglipModel models.SiglipTextModel SiglipTextModel.from_pretrained() : ` PreTrainedModel.from_pretrained ` models.SiglipVisionModel SiglipVisionModel.from_pretrained() : ` PreTrainedModel.from_pretrained ` models.CLIPSegForImageSegmentation models.GPT2LMHeadModel models.JAISModel models.JAISLMHeadModel models.CodeGenModel models.CodeGenForCausalLM models.LlamaPreTrainedModel models.LlamaModel models.CoherePreTrainedModel models.GemmaPreTrainedModel models.GemmaModel models.Gemma2PreTrainedModel models.Gemma2Model models.Qwen2PreTrainedModel models.Qwen2Model models.PhiModel models.Phi3Model models.BloomPreTrainedModel models.BloomModel models.BloomForCausalLM models.MptModel models.MptForCausalLM models.OPTModel models.OPTForCausalLM models.VitMatteForImageMatting vitMatteForImageMatting._call(model_inputs) models.DetrObjectDetectionOutput new DetrObjectDetectionOutput(output) models.DetrSegmentationOutput new DetrSegmentationOutput(output) models.RTDetrObjectDetectionOutput new RTDetrObjectDetectionOutput(output) models.TableTransformerModel models.TableTransformerForObjectDetection tableTransformerForObjectDetection._call(model_inputs) models.ResNetPreTrainedModel models.ResNetModel models.ResNetForImageClassification resNetForImageClassification._call(model_inputs) models.Swin2SRModel models.Swin2SRForImageSuperResolution models.DPTModel models.DPTForDepthEstimation models.DepthAnythingForDepthEstimation models.GLPNModel models.GLPNForDepthEstimation models.DonutSwinModel models.ConvNextModel models.ConvNextForImageClassification convNextForImageClassification._call(model_inputs) models.ConvNextV2Model models.ConvNextV2ForImageClassification convNextV2ForImageClassification._call(model_inputs) models.Dinov2Model models.Dinov2ForImageClassification dinov2ForImageClassification._call(model_inputs) models.YolosObjectDetectionOutput new YolosObjectDetectionOutput(output) models.SamModel samModel.get_image_embeddings(model_inputs) ‚áí ` Promise. < {image_embeddings: Tensor, image_positional_embeddings: Tensor} > ` samModel.forward(model_inputs) ‚áí ` Promise. < Object > ` samModel._call(model_inputs) ‚áí ` Promise. < SamImageSegmentationOutput > ` models.SamImageSegmentationOutput new SamImageSegmentationOutput(output) models.Wav2Vec2Model models.Wav2Vec2ForAudioFrameClassification wav2Vec2ForAudioFrameClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.PyAnnoteModel models.PyAnnoteForAudioFrameClassification pyAnnoteForAudioFrameClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.UniSpeechModel models.UniSpeechForCTC uniSpeechForCTC._call(model_inputs) models.UniSpeechForSequenceClassification uniSpeechForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.UniSpeechSatModel models.UniSpeechSatForCTC uniSpeechSatForCTC._call(model_inputs) models.UniSpeechSatForSequenceClassification uniSpeechSatForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.UniSpeechSatForAudioFrameClassification uniSpeechSatForAudioFrameClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.Wav2Vec2BertModel models.Wav2Vec2BertForCTC wav2Vec2BertForCTC._call(model_inputs) models.Wav2Vec2BertForSequenceClassification wav2Vec2BertForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.HubertModel models.HubertForCTC hubertForCTC._call(model_inputs) models.HubertForSequenceClassification hubertForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.WavLMPreTrainedModel models.WavLMModel models.WavLMForCTC wavLMForCTC._call(model_inputs) models.WavLMForSequenceClassification wavLMForSequenceClassification._call(model_inputs) ‚áí ` Promise. < SequenceClassifierOutput > ` models.WavLMForXVector wavLMForXVector._call(model_inputs) ‚áí ` Promise. < XVectorOutput > ` models.WavLMForAudioFrameClassification wavLMForAudioFrameClassification._call(model_inputs) ‚áí ` Promise. < TokenClassifierOutput > ` models.SpeechT5PreTrainedModel models.SpeechT5Model models.SpeechT5ForSpeechToText models.SpeechT5ForTextToSpeech speechT5ForTextToSpeech.generate_speech(input_values, speaker_embeddings, options) ‚áí ` Promise. < SpeechOutput > ` models.SpeechT5HifiGan models.TrOCRForCausalLM models.MistralPreTrainedModel models.Starcoder2PreTrainedModel models.FalconPreTrainedModel models.ClapTextModelWithProjection ClapTextModelWithProjection.from_pretrained() : ` PreTrainedModel.from_pretrained ` models.ClapAudioModelWithProjection ClapAudioModelWithProjection.from_pretrained() : ` PreTrainedModel.from_pretrained ` models.VitsModel vitsModel._call(model_inputs) ‚áí ` Promise. < VitsModelOutput > ` models.SegformerModel models.SegformerForImageClassification models.SegformerForSemanticSegmentation models.StableLmModel models.StableLmForCausalLM models.EfficientNetModel models.EfficientNetForImageClassification efficientNetForImageClassification._call(model_inputs) models.MusicgenModel models.MusicgenForCausalLM models.MusicgenForConditionalGeneration musicgenForConditionalGeneration._apply_and_filter_by_delay_pattern_mask(outputs) ‚áí ` Tensor ` musicgenForConditionalGeneration.generate(options) ‚áí ` Promise. < (ModelOutput|Tensor) > ` models.MobileNetV1Model models.MobileNetV1ForImageClassification mobileNetV1ForImageClassification._call(model_inputs) models.MobileNetV2Model models.MobileNetV2ForImageClassification mobileNetV2ForImageClassification._call(model_inputs) models.MobileNetV3Model models.MobileNetV3ForImageClassification mobileNetV3ForImageClassification._call(model_inputs) models.MobileNetV4Model models.MobileNetV4ForImageClassification mobileNetV4ForImageClassification._call(model_inputs) models.DecisionTransformerModel models.PretrainedMixin pretrainedMixin.MODEL_CLASS_MAPPINGS : ` * ` pretrainedMixin.BASE_IF_FAIL PretrainedMixin.from_pretrained() : ` * ` models.AutoModel autoModel.MODEL_CLASS_MAPPINGS : ` * ` models.AutoModelForSequenceClassification models.AutoModelForTokenClassification models.AutoModelForSeq2SeqLM models.AutoModelForSpeechSeq2Seq models.AutoModelForTextToSpectrogram models.AutoModelForTextToWaveform models.AutoModelForCausalLM models.AutoModelForMaskedLM models.AutoModelForQuestionAnswering models.AutoModelForVision2Seq models.AutoModelForImageClassification models.AutoModelForImageSegmentation models.AutoModelForSemanticSegmentation models.AutoModelForUniversalSegmentation models.AutoModelForObjectDetection models.AutoModelForMaskGeneration models.Seq2SeqLMOutput new Seq2SeqLMOutput(output) models.SequenceClassifierOutput new SequenceClassifierOutput(output) models.XVectorOutput new XVectorOutput(output) models.TokenClassifierOutput new TokenClassifierOutput(output) models.MaskedLMOutput new MaskedLMOutput(output) models.QuestionAnsweringModelOutput new QuestionAnsweringModelOutput(output) models.CausalLMOutput new CausalLMOutput(output) models.CausalLMOutputWithPast new CausalLMOutputWithPast(output) models.ImageMattingOutput new ImageMattingOutput(output) models.VitsModelOutput new VitsModelOutput(output) models~SamModelInputs : ` Object ` models~SpeechOutput : ` Object `

The documentation page API/NORMALIZER doesn‚Äôt exist in v3.0.0, but exists on
the main version. Click [here](/docs/transformers.js/main/en/api/Normalizer)
to redirect to the main version of the documentation.

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

pipelines

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  pipelines

Pipelines provide a high-level, easy to use, API for running machine learning
models.

**Example:** Instantiate pipeline using the `pipeline` function.

Copied

    
    
    import { pipeline } from '@huggingface/transformers';
    
    const classifier = await pipeline('sentiment-analysis');
    const output = await classifier('I love transformers!');
    // [{'label': 'POSITIVE', 'score': 0.999817686}]

  * pipelines
    *  _static_
      * .Pipeline ‚áê `Callable`
        * `new Pipeline(options)`
        * `.dispose()` : `DisposeType`
        * `._call(...args)`
      * .TextClassificationPipeline
        * `new TextClassificationPipeline(options)`
        * `._call()` : `TextClassificationPipelineCallback`
      * .TokenClassificationPipeline
        * `new TokenClassificationPipeline(options)`
        * `._call()` : `TokenClassificationPipelineCallback`
      * .QuestionAnsweringPipeline
        * `new QuestionAnsweringPipeline(options)`
        * `._call()` : `QuestionAnsweringPipelineCallback`
      * .FillMaskPipeline
        * `new FillMaskPipeline(options)`
        * `._call()` : `FillMaskPipelineCallback`
      * .Text2TextGenerationPipeline
        * `new Text2TextGenerationPipeline(options)`
        * `._key` : `‚Äôgenerated_text‚Äô`
        * `._call()` : `Text2TextGenerationPipelineCallback`
      * .SummarizationPipeline
        * `new SummarizationPipeline(options)`
        * `._key` : `‚Äôsummary_text‚Äô`
      * .TranslationPipeline
        * `new TranslationPipeline(options)`
        * `._key` : `‚Äôtranslation_text‚Äô`
      * .TextGenerationPipeline
        * `new TextGenerationPipeline(options)`
        * `._call()` : `TextGenerationPipelineCallback`
      * .ZeroShotClassificationPipeline
        * `new ZeroShotClassificationPipeline(options)`
        * `.model` : `any`
        * `._call()` : `ZeroShotClassificationPipelineCallback`
      * .FeatureExtractionPipeline
        * `new FeatureExtractionPipeline(options)`
        * `._call()` : `FeatureExtractionPipelineCallback`
      * .ImageFeatureExtractionPipeline
        * `new ImageFeatureExtractionPipeline(options)`
        * `._call()` : `ImageFeatureExtractionPipelineCallback`
      * .AudioClassificationPipeline
        * `new AudioClassificationPipeline(options)`
        * `._call()` : `AudioClassificationPipelineCallback`
      * .ZeroShotAudioClassificationPipeline
        * `new ZeroShotAudioClassificationPipeline(options)`
        * `._call()` : `ZeroShotAudioClassificationPipelineCallback`
      * .AutomaticSpeechRecognitionPipeline
        * `new AutomaticSpeechRecognitionPipeline(options)`
        * `._call()` : `AutomaticSpeechRecognitionPipelineCallback`
      * .ImageToTextPipeline
        * `new ImageToTextPipeline(options)`
        * `._call()` : `ImageToTextPipelineCallback`
      * .ImageClassificationPipeline
        * `new ImageClassificationPipeline(options)`
        * `._call()` : `ImageClassificationPipelineCallback`
      * .ImageSegmentationPipeline
        * `new ImageSegmentationPipeline(options)`
        * `._call()` : `ImageSegmentationPipelineCallback`
      * .ZeroShotImageClassificationPipeline
        * `new ZeroShotImageClassificationPipeline(options)`
        * `._call()` : `ZeroShotImageClassificationPipelineCallback`
      * .ObjectDetectionPipeline
        * `new ObjectDetectionPipeline(options)`
        * `._call()` : `ObjectDetectionPipelineCallback`
      * .ZeroShotObjectDetectionPipeline
        * `new ZeroShotObjectDetectionPipeline(options)`
        * `._call()` : `ZeroShotObjectDetectionPipelineCallback`
      * .DocumentQuestionAnsweringPipeline
        * `new DocumentQuestionAnsweringPipeline(options)`
        * `._call()` : `DocumentQuestionAnsweringPipelineCallback`
      * .TextToAudioPipeline
        * `new TextToAudioPipeline(options)`
        * `._call()` : `TextToAudioPipelineCallback`
      * .ImageToImagePipeline
        * `new ImageToImagePipeline(options)`
        * `._call()` : `ImageToImagePipelineCallback`
      * .DepthEstimationPipeline
        * `new DepthEstimationPipeline(options)`
        * `._call()` : `DepthEstimationPipelineCallback`
      * `.pipeline(task, [model], [options])` ‚áí `*`
    * _inner_
      * `~ImagePipelineInputs` : `string` | `RawImage` | `URL`
      * `~AudioPipelineInputs` : `string` | `URL` | `Float32Array` | `Float64Array`
      * `~BoundingBox` : `Object`
      * `~Disposable` ‚áí `Promise.<void>`
      * `~TextPipelineConstructorArgs` : `Object`
      * `~ImagePipelineConstructorArgs` : `Object`
      * `~TextImagePipelineConstructorArgs` : `Object`
      * `~TextClassificationPipelineType` ‚áí `Promise.<(TextClassificationOutput|Array<TextClassificationOutput>)>`
      * `~TokenClassificationPipelineType` ‚áí `Promise.<(TokenClassificationOutput|Array<TokenClassificationOutput>)>`
      * `~QuestionAnsweringPipelineType` ‚áí `Promise.<(QuestionAnsweringOutput|Array<QuestionAnsweringOutput>)>`
      * `~FillMaskPipelineType` ‚áí `Promise.<(FillMaskOutput|Array<FillMaskOutput>)>`
      * `~Text2TextGenerationPipelineType` ‚áí `Promise.<(Text2TextGenerationOutput|Array<Text2TextGenerationOutput>)>`
      * `~SummarizationPipelineType` ‚áí `Promise.<(SummarizationOutput|Array<SummarizationOutput>)>`
      * `~TranslationPipelineType` ‚áí `Promise.<(TranslationOutput|Array<TranslationOutput>)>`
      * `~TextGenerationPipelineType` ‚áí `Promise.<(TextGenerationOutput|Array<TextGenerationOutput>)>`
      * `~ZeroShotClassificationPipelineType` ‚áí `Promise.<(ZeroShotClassificationOutput|Array<ZeroShotClassificationOutput>)>`
      * `~FeatureExtractionPipelineType` ‚áí `Promise.<Tensor>`
      * `~ImageFeatureExtractionPipelineType` ‚áí `Promise.<Tensor>`
      * `~AudioClassificationPipelineType` ‚áí `Promise.<(AudioClassificationOutput|Array<AudioClassificationOutput>)>`
      * `~ZeroShotAudioClassificationPipelineType` ‚áí `Promise.<(Array<ZeroShotAudioClassificationOutput>|Array<Array<ZeroShotAudioClassificationOutput>>)>`
      * `~Chunk` : `Object`
      * `~AutomaticSpeechRecognitionPipelineType` ‚áí `Promise.<(AutomaticSpeechRecognitionOutput|Array<AutomaticSpeechRecognitionOutput>)>`
      * `~ImageToTextPipelineType` ‚áí `Promise.<(ImageToTextOutput|Array<ImageToTextOutput>)>`
      * `~ImageClassificationPipelineType` ‚áí `Promise.<(ImageClassificationOutput|Array<ImageClassificationOutput>)>`
      * `~ImageSegmentationPipelineType` ‚áí `Promise.<Array<ImageSegmentationPipelineOutput>>`
      * `~ZeroShotImageClassificationPipelineType` ‚áí `Promise.<(Array<ZeroShotImageClassificationOutput>|Array<Array<ZeroShotImageClassificationOutput>>)>`
      * `~ObjectDetectionPipelineType` ‚áí `Promise.<(ObjectDetectionPipelineOutput|Array<ObjectDetectionPipelineOutput>)>`
      * `~ZeroShotObjectDetectionPipelineType` ‚áí `Promise.<(Array<ZeroShotObjectDetectionOutput>|Array<Array<ZeroShotObjectDetectionOutput>>)>`
      * `~DocumentQuestionAnsweringPipelineType` ‚áí `Promise.<(DocumentQuestionAnsweringOutput|Array<DocumentQuestionAnsweringOutput>)>`
      * `~TextToAudioPipelineConstructorArgs` : `Object`
      * `~TextToAudioPipelineType` ‚áí `Promise.<TextToAudioOutput>`
      * `~ImageToImagePipelineType` ‚áí `Promise.<(RawImage|Array<RawImage>)>`
      * `~DepthEstimationPipelineType` ‚áí `Promise.<(DepthEstimationPipelineOutput|Array<DepthEstimationPipelineOutput>)>`
      * `~AllTasks` : `*`

* * *

##  pipelines.Pipeline ‚áê <code> Callable </code>

The Pipeline class is the class from which all pipelines inherit. Refer to
this class for methods shared across different pipelines.

**Kind** : static class of `pipelines`  
**Extends** : `Callable`

  * .Pipeline ‚áê `Callable`
    * `new Pipeline(options)`
    * `.dispose()` : `DisposeType`
    * `._call(...args)`

* * *

###  new Pipeline(options)

Create a new Pipeline.

Param| Type| Default| Description  
---|---|---|---  
options| `Object`| | An object containing the following properties:  
[options.task]| `string`| | The task of the pipeline. Useful for specifying subtasks.  
[options.model]| `PreTrainedModel`| | The model used by the pipeline.  
[options.tokenizer]| `PreTrainedTokenizer`| ``| The tokenizer used by the
pipeline (if any).  
[options.processor]| `Processor`| ``| The processor used by the pipeline (if
any).  
  
* * *

###  pipeline.dispose() : <code> DisposeType </code>

**Kind** : instance method of `Pipeline`

* * *

###  pipeline._call(...args)

This method should be implemented in subclasses to provide the functionality
of the callable object.

**Kind** : instance method of `Pipeline`  
**Overrides** : `_call`  
**Throws** :

  * `Error` If the subclass does not implement the `_call` method.

Param| Type  
---|---  
...args| `Array.<any>`  
  
* * *

##  pipelines.TextClassificationPipeline

Text classification pipeline using any `ModelForSequenceClassification`.

**Example:** Sentiment-analysis w/ `Xenova/distilbert-base-uncased-finetuned-
sst-2-english`.

Copied

    
    
    const classifier = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english');
    const output = await classifier('I love transformers!');
    // [{ label: 'POSITIVE', score: 0.999788761138916 }]

**Example:** Multilingual sentiment-analysis w/ `Xenova/bert-base-
multilingual-uncased-sentiment` (and return top 5 classes).

Copied

    
    
    const classifier = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment');
    const output = await classifier('Le meilleur film de tous les temps.', { top_k: 5 });
    // [
    //   { label: '5 stars', score: 0.9610759615898132 },
    //   { label: '4 stars', score: 0.03323351591825485 },
    //   { label: '3 stars', score: 0.0036155181005597115 },
    //   { label: '1 star', score: 0.0011325967498123646 },
    //   { label: '2 stars', score: 0.0009423971059732139 }
    // ]

**Example:** Toxic comment classification w/ `Xenova/toxic-bert` (and return
all classes).

Copied

    
    
    const classifier = await pipeline('text-classification', 'Xenova/toxic-bert');
    const output = await classifier('I hate you!', { top_k: null });
    // [
    //   { label: 'toxic', score: 0.9593140482902527 },
    //   { label: 'insult', score: 0.16187334060668945 },
    //   { label: 'obscene', score: 0.03452680632472038 },
    //   { label: 'identity_hate', score: 0.0223250575363636 },
    //   { label: 'threat', score: 0.019197041168808937 },
    //   { label: 'severe_toxic', score: 0.005651099607348442 }
    // ]

**Kind** : static class of `pipelines`

  * .TextClassificationPipeline
    * `new TextClassificationPipeline(options)`
    * `._call()` : `TextClassificationPipelineCallback`

* * *

###  new TextClassificationPipeline(options)

Create a new TextClassificationPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  textClassificationPipeline._call() : <code>
TextClassificationPipelineCallback </code>

**Kind** : instance method of `TextClassificationPipeline`

* * *

##  pipelines.TokenClassificationPipeline

Named Entity Recognition pipeline using any `ModelForTokenClassification`.

**Example:** Perform named entity recognition with `Xenova/bert-base-NER`.

Copied

    
    
    const classifier = await pipeline('token-classification', 'Xenova/bert-base-NER');
    const output = await classifier('My name is Sarah and I live in London');
    // [
    //   { entity: 'B-PER', score: 0.9980202913284302, index: 4, word: 'Sarah' },
    //   { entity: 'B-LOC', score: 0.9994474053382874, index: 9, word: 'London' }
    // ]

**Example:** Perform named entity recognition with `Xenova/bert-base-NER` (and
return all labels).

Copied

    
    
    const classifier = await pipeline('token-classification', 'Xenova/bert-base-NER');
    const output = await classifier('Sarah lives in the United States of America', { ignore_labels: [] });
    // [
    //   { entity: 'B-PER', score: 0.9966587424278259, index: 1, word: 'Sarah' },
    //   { entity: 'O', score: 0.9987385869026184, index: 2, word: 'lives' },
    //   { entity: 'O', score: 0.9990072846412659, index: 3, word: 'in' },
    //   { entity: 'O', score: 0.9988298416137695, index: 4, word: 'the' },
    //   { entity: 'B-LOC', score: 0.9995510578155518, index: 5, word: 'United' },
    //   { entity: 'I-LOC', score: 0.9990395307540894, index: 6, word: 'States' },
    //   { entity: 'I-LOC', score: 0.9986724853515625, index: 7, word: 'of' },
    //   { entity: 'I-LOC', score: 0.9975294470787048, index: 8, word: 'America' }
    // ]

**Kind** : static class of `pipelines`

  * .TokenClassificationPipeline
    * `new TokenClassificationPipeline(options)`
    * `._call()` : `TokenClassificationPipelineCallback`

* * *

###  new TokenClassificationPipeline(options)

Create a new TokenClassificationPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  tokenClassificationPipeline._call() : <code>
TokenClassificationPipelineCallback </code>

**Kind** : instance method of `TokenClassificationPipeline`

* * *

##  pipelines.QuestionAnsweringPipeline

Question Answering pipeline using any `ModelForQuestionAnswering`.

**Example:** Run question answering with `Xenova/distilbert-base-uncased-
distilled-squad`.

Copied

    
    
    const answerer = await pipeline('question-answering', 'Xenova/distilbert-base-uncased-distilled-squad');
    const question = 'Who was Jim Henson?';
    const context = 'Jim Henson was a nice puppet.';
    const output = await answerer(question, context);
    // {
    //   answer: "a nice puppet",
    //   score: 0.5768911502526741
    // }

**Kind** : static class of `pipelines`

  * .QuestionAnsweringPipeline
    * `new QuestionAnsweringPipeline(options)`
    * `._call()` : `QuestionAnsweringPipelineCallback`

* * *

###  new QuestionAnsweringPipeline(options)

Create a new QuestionAnsweringPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  questionAnsweringPipeline._call() : <code>
QuestionAnsweringPipelineCallback </code>

**Kind** : instance method of `QuestionAnsweringPipeline`

* * *

##  pipelines.FillMaskPipeline

Masked language modeling prediction pipeline using any `ModelWithLMHead`.

**Example:** Perform masked language modelling (a.k.a. ‚Äúfill-mask‚Äù) with
`Xenova/bert-base-uncased`.

Copied

    
    
    const unmasker = await pipeline('fill-mask', 'Xenova/bert-base-cased');
    const output = await unmasker('The goal of life is [MASK].');
    // [
    //   { token_str: 'survival', score: 0.06137419492006302, token: 8115, sequence: 'The goal of life is survival.' },
    //   { token_str: 'love', score: 0.03902450203895569, token: 1567, sequence: 'The goal of life is love.' },
    //   { token_str: 'happiness', score: 0.03253183513879776, token: 9266, sequence: 'The goal of life is happiness.' },
    //   { token_str: 'freedom', score: 0.018736306577920914, token: 4438, sequence: 'The goal of life is freedom.' },
    //   { token_str: 'life', score: 0.01859794743359089, token: 1297, sequence: 'The goal of life is life.' }
    // ]

**Example:** Perform masked language modelling (a.k.a. ‚Äúfill-mask‚Äù) with
`Xenova/bert-base-cased` (and return top result).

Copied

    
    
    const unmasker = await pipeline('fill-mask', 'Xenova/bert-base-cased');
    const output = await unmasker('The Milky Way is a [MASK] galaxy.', { top_k: 1 });
    // [{ token_str: 'spiral', score: 0.6299987435340881, token: 14061, sequence: 'The Milky Way is a spiral galaxy.' }]

**Kind** : static class of `pipelines`

  * .FillMaskPipeline
    * `new FillMaskPipeline(options)`
    * `._call()` : `FillMaskPipelineCallback`

* * *

###  new FillMaskPipeline(options)

Create a new FillMaskPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  fillMaskPipeline._call() : <code> FillMaskPipelineCallback </code>

**Kind** : instance method of `FillMaskPipeline`

* * *

##  pipelines.Text2TextGenerationPipeline

Text2TextGenerationPipeline class for generating text using a model that
performs text-to-text generation tasks.

**Example:** Text-to-text generation w/ `Xenova/LaMini-Flan-T5-783M`.

Copied

    
    
    const generator = await pipeline('text2text-generation', 'Xenova/LaMini-Flan-T5-783M');
    const output = await generator('how can I become more healthy?', {
      max_new_tokens: 100,
    });
    // [{ generated_text: "To become more healthy, you can: 1. Eat a balanced diet with plenty of fruits, vegetables, whole grains, lean proteins, and healthy fats. 2. Stay hydrated by drinking plenty of water. 3. Get enough sleep and manage stress levels. 4. Avoid smoking and excessive alcohol consumption. 5. Regularly exercise and maintain a healthy weight. 6. Practice good hygiene and sanitation. 7. Seek medical attention if you experience any health issues." }]

**Kind** : static class of `pipelines`

  * .Text2TextGenerationPipeline
    * `new Text2TextGenerationPipeline(options)`
    * `._key` : `‚Äôgenerated_text‚Äô`
    * `._call()` : `Text2TextGenerationPipelineCallback`

* * *

###  new Text2TextGenerationPipeline(options)

Create a new Text2TextGenerationPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  text2TextGenerationPipeline._key : <code> ‚Äô generated_text ‚Äô </code>

**Kind** : instance property of `Text2TextGenerationPipeline`

* * *

###  text2TextGenerationPipeline._call() : <code>
Text2TextGenerationPipelineCallback </code>

**Kind** : instance method of `Text2TextGenerationPipeline`

* * *

##  pipelines.SummarizationPipeline

A pipeline for summarization tasks, inheriting from
Text2TextGenerationPipeline.

**Example:** Summarization w/ `Xenova/distilbart-cnn-6-6`.

Copied

    
    
    const generator = await pipeline('summarization', 'Xenova/distilbart-cnn-6-6');
    const text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, ' +
      'and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. ' +
      'During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest ' +
      'man-made structure in the world, a title it held for 41 years until the Chrysler Building in New ' +
      'York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to ' +
      'the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the ' +
      'Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second ' +
      'tallest free-standing structure in France after the Millau Viaduct.';
    const output = await generator(text, {
      max_new_tokens: 100,
    });
    // [{ summary_text: ' The Eiffel Tower is about the same height as an 81-storey building and the tallest structure in Paris. It is the second tallest free-standing structure in France after the Millau Viaduct.' }]

**Kind** : static class of `pipelines`

  * .SummarizationPipeline
    * `new SummarizationPipeline(options)`
    * `._key` : `‚Äôsummary_text‚Äô`

* * *

###  new SummarizationPipeline(options)

Create a new SummarizationPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  summarizationPipeline._key : <code> ‚Äô summary_text ‚Äô </code>

**Kind** : instance property of `SummarizationPipeline`

* * *

##  pipelines.TranslationPipeline

Translates text from one language to another.

**Example:** Multilingual translation w/ `Xenova/nllb-200-distilled-600M`.

See
[here](https://github.com/facebookresearch/flores/blob/v3.0.0/flores200/README.md#languages-
in-flores-200) for the full list of languages and their corresponding codes.

Copied

    
    
    const translator = await pipeline('translation', 'Xenova/nllb-200-distilled-600M');
    const output = await translator('‡§ú‡•Ä‡§µ‡§® ‡§è‡§ï ‡§ö‡•â‡§ï‡§≤‡•á‡§ü ‡§¨‡•â‡§ï‡•ç‡§∏ ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§π‡•à‡•§', {
      src_lang: 'hin_Deva', // Hindi
      tgt_lang: 'fra_Latn', // French
    });
    // [{ translation_text: 'La vie est comme une bo√Æte √† chocolat.' }]

**Example:** Multilingual translation w/ `Xenova/m2m100_418M`.

See [here](https://huggingface.co/facebook/m2m100_418M#languages-covered) for
the full list of languages and their corresponding codes.

Copied

    
    
    const translator = await pipeline('translation', 'Xenova/m2m100_418M');
    const output = await translator('ÁîüÊ¥ªÂ∞±ÂÉè‰∏ÄÁõíÂ∑ßÂÖãÂäõ„ÄÇ', {
      src_lang: 'zh', // Chinese
      tgt_lang: 'en', // English
    });
    // [{ translation_text: 'Life is like a box of chocolate.' }]

**Example:** Multilingual translation w/ `Xenova/mbart-large-50-many-to-many-
mmt`.

See [here](https://huggingface.co/facebook/mbart-large-50-many-to-many-
mmt#languages-covered) for the full list of languages and their corresponding
codes.

Copied

    
    
    const translator = await pipeline('translation', 'Xenova/mbart-large-50-many-to-many-mmt');
    const output = await translator('‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§ï‡•á ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§ï‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§∏‡•Ä‡§∞‡§ø‡§Ø‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§à ‡§∏‡•à‡§®‡•ç‡§Ø ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à', {
      src_lang: 'hi_IN', // Hindi
      tgt_lang: 'fr_XX', // French
    });
    // [{ translation_text: 'Le chef des Nations affirme qu 'il n 'y a military solution in Syria.' }]

**Kind** : static class of `pipelines`

  * .TranslationPipeline
    * `new TranslationPipeline(options)`
    * `._key` : `‚Äôtranslation_text‚Äô`

* * *

###  new TranslationPipeline(options)

Create a new TranslationPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  translationPipeline._key : <code> ‚Äô translation_text ‚Äô </code>

**Kind** : instance property of `TranslationPipeline`

* * *

##  pipelines.TextGenerationPipeline

Language generation pipeline using any `ModelWithLMHead` or
`ModelForCausalLM`. This pipeline predicts the words that will follow a
specified text prompt. NOTE: For the full list of generation parameters, see
[`GenerationConfig`](./utils/generation#module_utils/generation.GenerationConfig).

**Example:** Text generation with `Xenova/distilgpt2` (default settings).

Copied

    
    
    const generator = await pipeline('text-generation', 'Xenova/distilgpt2');
    const text = 'I enjoy walking with my cute dog,';
    const output = await generator(text);
    // [{ generated_text: "I enjoy walking with my cute dog, and I love to play with the other dogs." }]

**Example:** Text generation with `Xenova/distilgpt2` (custom settings).

Copied

    
    
    const generator = await pipeline('text-generation', 'Xenova/distilgpt2');
    const text = 'Once upon a time, there was';
    const output = await generator(text, {
      temperature: 2,
      max_new_tokens: 10,
      repetition_penalty: 1.5,
      no_repeat_ngram_size: 2,
      num_beams: 2,
      num_return_sequences: 2,
    });
    // [{
    //   "generated_text": "Once upon a time, there was an abundance of information about the history and activities that"
    // }, {
    //   "generated_text": "Once upon a time, there was an abundance of information about the most important and influential"
    // }]

**Example:** Run code generation with `Xenova/codegen-350M-mono`.

Copied

    
    
    const generator = await pipeline('text-generation', 'Xenova/codegen-350M-mono');
    const text = 'def fib(n):';
    const output = await generator(text, {
      max_new_tokens: 44,
    });
    // [{
    //   generated_text: 'def fib(n):\n' +
    //     '    if n == 0:\n' +
    //     '        return 0\n' +
    //     '    elif n == 1:\n' +
    //     '        return 1\n' +
    //     '    else:\n' +
    //     '        return fib(n-1) + fib(n-2)\n'
    // }]

**Kind** : static class of `pipelines`

  * .TextGenerationPipeline
    * `new TextGenerationPipeline(options)`
    * `._call()` : `TextGenerationPipelineCallback`

* * *

###  new TextGenerationPipeline(options)

Create a new TextGenerationPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  textGenerationPipeline._call() : <code> TextGenerationPipelineCallback
</code>

**Kind** : instance method of `TextGenerationPipeline`

* * *

##  pipelines.ZeroShotClassificationPipeline

NLI-based zero-shot classification pipeline using a
`ModelForSequenceClassification` trained on NLI (natural language inference)
tasks. Equivalent of `text-classification` pipelines, but these models don‚Äôt
require a hardcoded number of potential classes, they can be chosen at
runtime. It usually means it‚Äôs slower but it is **much** more flexible.

**Example:** Zero shot classification with `Xenova/mobilebert-uncased-mnli`.

Copied

    
    
    const classifier = await pipeline('zero-shot-classification', 'Xenova/mobilebert-uncased-mnli');
    const text = 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.';
    const labels = [ 'mobile', 'billing', 'website', 'account access' ];
    const output = await classifier(text, labels);
    // {
    //   sequence: 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',
    //   labels: [ 'mobile', 'website', 'billing', 'account access' ],
    //   scores: [ 0.5562091040482018, 0.1843621307860853, 0.13942646639336376, 0.12000229877234923 ]
    // }

**Example:** Zero shot classification with `Xenova/nli-deberta-v3-xsmall`
(multi-label).

Copied

    
    
    const classifier = await pipeline('zero-shot-classification', 'Xenova/nli-deberta-v3-xsmall');
    const text = 'I have a problem with my iphone that needs to be resolved asap!';
    const labels = [ 'urgent', 'not urgent', 'phone', 'tablet', 'computer' ];
    const output = await classifier(text, labels, { multi_label: true });
    // {
    //   sequence: 'I have a problem with my iphone that needs to be resolved asap!',
    //   labels: [ 'urgent', 'phone', 'computer', 'tablet', 'not urgent' ],
    //   scores: [ 0.9958870956360275, 0.9923963400697035, 0.002333537946160235, 0.0015134138567598765, 0.0010699384208377163 ]
    // }

**Kind** : static class of `pipelines`

  * .ZeroShotClassificationPipeline
    * `new ZeroShotClassificationPipeline(options)`
    * `.model` : `any`
    * `._call()` : `ZeroShotClassificationPipelineCallback`

* * *

###  new ZeroShotClassificationPipeline(options)

Create a new ZeroShotClassificationPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  zeroShotClassificationPipeline.model : <code> any </code>

**Kind** : instance property of `ZeroShotClassificationPipeline`

* * *

###  zeroShotClassificationPipeline._call() : <code>
ZeroShotClassificationPipelineCallback </code>

**Kind** : instance method of `ZeroShotClassificationPipeline`

* * *

##  pipelines.FeatureExtractionPipeline

Feature extraction pipeline using no model head. This pipeline extracts the
hidden states from the base transformer, which can be used as features in
downstream tasks.

**Example:** Run feature extraction with `bert-base-uncased` (without
pooling/normalization).

Copied

    
    
    const extractor = await pipeline('feature-extraction', 'Xenova/bert-base-uncased', { revision: 'default' });
    const output = await extractor('This is a simple test.');
    // Tensor {
    //   type: 'float32',
    //   data: Float32Array [0.05939924716949463, 0.021655935794115067, ...],
    //   dims: [1, 8, 768]
    // }

**Example:** Run feature extraction with `bert-base-uncased` (with
pooling/normalization).

Copied

    
    
    const extractor = await pipeline('feature-extraction', 'Xenova/bert-base-uncased', { revision: 'default' });
    const output = await extractor('This is a simple test.', { pooling: 'mean', normalize: true });
    // Tensor {
    //   type: 'float32',
    //   data: Float32Array [0.03373778983950615, -0.010106077417731285, ...],
    //   dims: [1, 768]
    // }

**Example:** Calculating embeddings with `sentence-transformers` models.

Copied

    
    
    const extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
    const output = await extractor('This is a simple test.', { pooling: 'mean', normalize: true });
    // Tensor {
    //   type: 'float32',
    //   data: Float32Array [0.09094982594251633, -0.014774246141314507, ...],
    //   dims: [1, 384]
    // }

**Example:** Calculating binary embeddings with `sentence-transformers`
models.

Copied

    
    
    const extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');
    const output = await extractor('This is a simple test.', { pooling: 'mean', quantize: true, precision: 'binary' });
    // Tensor {
    //   type: 'int8',
    //   data: Int8Array¬†[49, 108, 24, ...],
    //   dims: [1, 48]
    // }

**Kind** : static class of `pipelines`

  * .FeatureExtractionPipeline
    * `new FeatureExtractionPipeline(options)`
    * `._call()` : `FeatureExtractionPipelineCallback`

* * *

###  new FeatureExtractionPipeline(options)

Create a new FeatureExtractionPipeline.

Param| Type| Description  
---|---|---  
options| `TextPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  featureExtractionPipeline._call() : <code>
FeatureExtractionPipelineCallback </code>

**Kind** : instance method of `FeatureExtractionPipeline`

* * *

##  pipelines.ImageFeatureExtractionPipeline

Image feature extraction pipeline using no model head. This pipeline extracts
the hidden states from the base transformer, which can be used as features in
downstream tasks.

**Example:** Perform image feature extraction with `Xenova/vit-base-
patch16-224-in21k`.

Copied

    
    
    const image_feature_extractor = await pipeline('image-feature-extraction', 'Xenova/vit-base-patch16-224-in21k');
    const url = 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/v3.0.0/cats.png';
    const features = await image_feature_extractor(url);
    // Tensor {
    //   dims: [ 1, 197, 768 ],
    //   type: 'float32',
    //   data: Float32Array(151296) [ ... ],
    //   size: 151296
    // }

**Example:** Compute image embeddings with `Xenova/clip-vit-base-patch32`.

Copied

    
    
    const image_feature_extractor = await pipeline('image-feature-extraction', 'Xenova/clip-vit-base-patch32');
    const url = 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/v3.0.0/cats.png';
    const features = await image_feature_extractor(url);
    // Tensor {
    //   dims: [ 1, 512 ],
    //   type: 'float32',
    //   data: Float32Array(512) [ ... ],
    //   size: 512
    // }

**Kind** : static class of `pipelines`

  * .ImageFeatureExtractionPipeline
    * `new ImageFeatureExtractionPipeline(options)`
    * `._call()` : `ImageFeatureExtractionPipelineCallback`

* * *

###  new ImageFeatureExtractionPipeline(options)

Create a new ImageFeatureExtractionPipeline.

Param| Type| Description  
---|---|---  
options| `ImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  imageFeatureExtractionPipeline._call() : <code>
ImageFeatureExtractionPipelineCallback </code>

**Kind** : instance method of `ImageFeatureExtractionPipeline`

* * *

##  pipelines.AudioClassificationPipeline

Audio classification pipeline using any `AutoModelForAudioClassification`.
This pipeline predicts the class of a raw waveform or an audio file.

**Example:** Perform audio classification with `Xenova/wav2vec2-large-
xlsr-53-gender-recognition-librispeech`.

Copied

    
    
    const classifier = await pipeline('audio-classification', 'Xenova/wav2vec2-large-xlsr-53-gender-recognition-librispeech');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/jfk.wav';
    const output = await classifier(url);
    // [
    //   { label: 'male', score: 0.9981542229652405 },
    //   { label: 'female', score: 0.001845747814513743 }
    // ]

**Example:** Perform audio classification with `Xenova/ast-finetuned-
audioset-10-10-0.4593` and return top 4 results.

Copied

    
    
    const classifier = await pipeline('audio-classification', 'Xenova/ast-finetuned-audioset-10-10-0.4593');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/cat_meow.wav';
    const output = await classifier(url, { top_k: 4 });
    // [
    //   { label: 'Meow', score: 0.5617874264717102 },
    //   { label: 'Cat', score: 0.22365376353263855 },
    //   { label: 'Domestic animals, pets', score: 0.1141069084405899 },
    //   { label: 'Animal', score: 0.08985692262649536 },
    // ]

**Kind** : static class of `pipelines`

  * .AudioClassificationPipeline
    * `new AudioClassificationPipeline(options)`
    * `._call()` : `AudioClassificationPipelineCallback`

* * *

###  new AudioClassificationPipeline(options)

Create a new AudioClassificationPipeline.

Param| Type| Description  
---|---|---  
options| `AudioPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  audioClassificationPipeline._call() : <code>
AudioClassificationPipelineCallback </code>

**Kind** : instance method of `AudioClassificationPipeline`

* * *

##  pipelines.ZeroShotAudioClassificationPipeline

Zero shot audio classification pipeline using `ClapModel`. This pipeline
predicts the class of an audio when you provide an audio and a set of
`candidate_labels`.

**Example** : Perform zero-shot audio classification with `Xenova/clap-htsat-
unfused`.

Copied

    
    
    const classifier = await pipeline('zero-shot-audio-classification', 'Xenova/clap-htsat-unfused');
    const audio = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/dog_barking.wav';
    const candidate_labels = ['dog', 'vaccum cleaner'];
    const scores = await classifier(audio, candidate_labels);
    // [
    //   { score: 0.9993992447853088, label: 'dog' },
    //   { score: 0.0006007603369653225, label: 'vaccum cleaner' }
    // ]

**Kind** : static class of `pipelines`

  * .ZeroShotAudioClassificationPipeline
    * `new ZeroShotAudioClassificationPipeline(options)`
    * `._call()` : `ZeroShotAudioClassificationPipelineCallback`

* * *

###  new ZeroShotAudioClassificationPipeline(options)

Create a new ZeroShotAudioClassificationPipeline.

Param| Type| Description  
---|---|---  
options| `TextAudioPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  zeroShotAudioClassificationPipeline._call() : <code>
ZeroShotAudioClassificationPipelineCallback </code>

**Kind** : instance method of `ZeroShotAudioClassificationPipeline`

* * *

##  pipelines.AutomaticSpeechRecognitionPipeline

Pipeline that aims at extracting spoken text contained within some audio.

**Example:** Transcribe English.

Copied

    
    
    const transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/jfk.wav';
    const output = await transcriber(url);
    // { text: " And so my fellow Americans ask not what your country can do for you, ask what you can do for your country." }

**Example:** Transcribe English w/ timestamps.

Copied

    
    
    const transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/jfk.wav';
    const output = await transcriber(url, { return_timestamps: true });
    // {
    //   text: " And so my fellow Americans ask not what your country can do for you, ask what you can do for your country."
    //   chunks: [
    //     { timestamp: [0, 8],  text: " And so my fellow Americans ask not what your country can do for you" }
    //     { timestamp: [8, 11], text: " ask what you can do for your country." }
    //   ]
    // }

**Example:** Transcribe English w/ word-level timestamps.

Copied

    
    
    const transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/jfk.wav';
    const output = await transcriber(url, { return_timestamps: 'word' });
    // {
    //   "text": " And so my fellow Americans ask not what your country can do for you ask what you can do for your country.",
    //   "chunks": [
    //     { "text": " And", "timestamp": [0, 0.78] },
    //     { "text": " so", "timestamp": [0.78, 1.06] },
    //     { "text": " my", "timestamp": [1.06, 1.46] },
    //     ...
    //     { "text": " for", "timestamp": [9.72, 9.92] },
    //     { "text": " your", "timestamp": [9.92, 10.22] },
    //     { "text": " country.", "timestamp": [10.22, 13.5] }
    //   ]
    // }

**Example:** Transcribe French.

Copied

    
    
    const transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-small');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/french-audio.mp3';
    const output = await transcriber(url, { language: 'french', task: 'transcribe' });
    // { text: " J'adore, j'aime, je n'aime pas, je d√©teste." }

**Example:** Translate French to English.

Copied

    
    
    const transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-small');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/french-audio.mp3';
    const output = await transcriber(url, { language: 'french', task: 'translate' });
    // { text: " I love, I like, I don't like, I hate." }

**Example:** Transcribe/translate audio longer than 30 seconds.

Copied

    
    
    const transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/ted_60.wav';
    const output = await transcriber(url, { chunk_length_s: 30, stride_length_s: 5 });
    // { text: " So in college, I was a government major, which means [...] So I'd start off light and I'd bump it up" }

**Kind** : static class of `pipelines`

  * .AutomaticSpeechRecognitionPipeline
    * `new AutomaticSpeechRecognitionPipeline(options)`
    * `._call()` : `AutomaticSpeechRecognitionPipelineCallback`

* * *

###  new AutomaticSpeechRecognitionPipeline(options)

Create a new AutomaticSpeechRecognitionPipeline.

Param| Type| Description  
---|---|---  
options| `TextAudioPipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  automaticSpeechRecognitionPipeline._call() : <code>
AutomaticSpeechRecognitionPipelineCallback </code>

**Kind** : instance method of `AutomaticSpeechRecognitionPipeline`

* * *

##  pipelines.ImageToTextPipeline

Image To Text pipeline using a `AutoModelForVision2Seq`. This pipeline
predicts a caption for a given image.

**Example:** Generate a caption for an image w/ `Xenova/vit-gpt2-image-
captioning`.

Copied

    
    
    const captioner = await pipeline('image-to-text', 'Xenova/vit-gpt2-image-captioning');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/cats.jpg';
    const output = await captioner(url);
    // [{ generated_text: 'a cat laying on a couch with another cat' }]

**Example:** Optical Character Recognition (OCR) w/ `Xenova/trocr-small-
handwritten`.

Copied

    
    
    const captioner = await pipeline('image-to-text', 'Xenova/trocr-small-handwritten');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/handwriting.jpg';
    const output = await captioner(url);
    // [{ generated_text: 'Mr. Brown commented icily.' }]

**Kind** : static class of `pipelines`

  * .ImageToTextPipeline
    * `new ImageToTextPipeline(options)`
    * `._call()` : `ImageToTextPipelineCallback`

* * *

###  new ImageToTextPipeline(options)

Create a new ImageToTextPipeline.

Param| Type| Description  
---|---|---  
options| `TextImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  imageToTextPipeline._call() : <code> ImageToTextPipelineCallback </code>

**Kind** : instance method of `ImageToTextPipeline`

* * *

##  pipelines.ImageClassificationPipeline

Image classification pipeline using any `AutoModelForImageClassification`.
This pipeline predicts the class of an image.

**Example:** Classify an image.

Copied

    
    
    const classifier = await pipeline('image-classification', 'Xenova/vit-base-patch16-224');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/tiger.jpg';
    const output = await classifier(url);
    // [
    //   { label: 'tiger, Panthera tigris', score: 0.632695734500885 },
    // ]

**Example:** Classify an image and return top `n` classes.

Copied

    
    
    const classifier = await pipeline('image-classification', 'Xenova/vit-base-patch16-224');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/tiger.jpg';
    const output = await classifier(url, { top_k: 3 });
    // [
    //   { label: 'tiger, Panthera tigris', score: 0.632695734500885 },
    //   { label: 'tiger cat', score: 0.3634825646877289 },
    //   { label: 'lion, king of beasts, Panthera leo', score: 0.00045060308184474707 },
    // ]

**Example:** Classify an image and return all classes.

Copied

    
    
    const classifier = await pipeline('image-classification', 'Xenova/vit-base-patch16-224');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/tiger.jpg';
    const output = await classifier(url, { top_k: 0 });
    // [
    //   { label: 'tiger, Panthera tigris', score: 0.632695734500885 },
    //   { label: 'tiger cat', score: 0.3634825646877289 },
    //   { label: 'lion, king of beasts, Panthera leo', score: 0.00045060308184474707 },
    //   { label: 'jaguar, panther, Panthera onca, Felis onca', score: 0.00035465499968267977 },
    //   ...
    // ]

**Kind** : static class of `pipelines`

  * .ImageClassificationPipeline
    * `new ImageClassificationPipeline(options)`
    * `._call()` : `ImageClassificationPipelineCallback`

* * *

###  new ImageClassificationPipeline(options)

Create a new ImageClassificationPipeline.

Param| Type| Description  
---|---|---  
options| `ImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  imageClassificationPipeline._call() : <code>
ImageClassificationPipelineCallback </code>

**Kind** : instance method of `ImageClassificationPipeline`

* * *

##  pipelines.ImageSegmentationPipeline

Image segmentation pipeline using any `AutoModelForXXXSegmentation`. This
pipeline predicts masks of objects and their classes.

**Example:** Perform image segmentation with `Xenova/detr-resnet-50-panoptic`.

Copied

    
    
    const segmenter = await pipeline('image-segmentation', 'Xenova/detr-resnet-50-panoptic');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/cats.jpg';
    const output = await segmenter(url);
    // [
    //   { label: 'remote', score: 0.9984649419784546, mask: RawImage { ... } },
    //   { label: 'cat', score: 0.9994316101074219, mask: RawImage { ... } }
    // ]

**Kind** : static class of `pipelines`

  * .ImageSegmentationPipeline
    * `new ImageSegmentationPipeline(options)`
    * `._call()` : `ImageSegmentationPipelineCallback`

* * *

###  new ImageSegmentationPipeline(options)

Create a new ImageSegmentationPipeline.

Param| Type| Description  
---|---|---  
options| `ImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  imageSegmentationPipeline._call() : <code>
ImageSegmentationPipelineCallback </code>

**Kind** : instance method of `ImageSegmentationPipeline`

* * *

##  pipelines.ZeroShotImageClassificationPipeline

Zero shot image classification pipeline. This pipeline predicts the class of
an image when you provide an image and a set of `candidate_labels`.

**Example:** Zero shot image classification w/ `Xenova/clip-vit-base-patch32`.

Copied

    
    
    const classifier = await pipeline('zero-shot-image-classification', 'Xenova/clip-vit-base-patch32');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/tiger.jpg';
    const output = await classifier(url, ['tiger', 'horse', 'dog']);
    // [
    //   { score: 0.9993917942047119, label: 'tiger' },
    //   { score: 0.0003519294841680676, label: 'horse' },
    //   { score: 0.0002562698791734874, label: 'dog' }
    // ]

**Kind** : static class of `pipelines`

  * .ZeroShotImageClassificationPipeline
    * `new ZeroShotImageClassificationPipeline(options)`
    * `._call()` : `ZeroShotImageClassificationPipelineCallback`

* * *

###  new ZeroShotImageClassificationPipeline(options)

Create a new ZeroShotImageClassificationPipeline.

Param| Type| Description  
---|---|---  
options| `TextImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  zeroShotImageClassificationPipeline._call() : <code>
ZeroShotImageClassificationPipelineCallback </code>

**Kind** : instance method of `ZeroShotImageClassificationPipeline`

* * *

##  pipelines.ObjectDetectionPipeline

Object detection pipeline using any `AutoModelForObjectDetection`. This
pipeline predicts bounding boxes of objects and their classes.

**Example:** Run object-detection with `Xenova/detr-resnet-50`.

Copied

    
    
    const detector = await pipeline('object-detection', 'Xenova/detr-resnet-50');
    const img = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/cats.jpg';
    const output = await detector(img, { threshold: 0.9 });
    // [{
    //   score: 0.9976370930671692,
    //   label: "remote",
    //   box: { xmin: 31, ymin: 68, xmax: 190, ymax: 118 }
    // },
    // ...
    // {
    //   score: 0.9984092116355896,
    //   label: "cat",
    //   box: { xmin: 331, ymin: 19, xmax: 649, ymax: 371 }
    // }]

**Kind** : static class of `pipelines`

  * .ObjectDetectionPipeline
    * `new ObjectDetectionPipeline(options)`
    * `._call()` : `ObjectDetectionPipelineCallback`

* * *

###  new ObjectDetectionPipeline(options)

Create a new ObjectDetectionPipeline.

Param| Type| Description  
---|---|---  
options| `ImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  objectDetectionPipeline._call() : <code> ObjectDetectionPipelineCallback
</code>

**Kind** : instance method of `ObjectDetectionPipeline`

* * *

##  pipelines.ZeroShotObjectDetectionPipeline

Zero-shot object detection pipeline. This pipeline predicts bounding boxes of
objects when you provide an image and a set of `candidate_labels`.

**Example:** Zero-shot object detection w/ `Xenova/owlvit-base-patch32`.

Copied

    
    
    const detector = await pipeline('zero-shot-object-detection', 'Xenova/owlvit-base-patch32');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/astronaut.png';
    const candidate_labels = ['human face', 'rocket', 'helmet', 'american flag'];
    const output = await detector(url, candidate_labels);
    // [
    //   {
    //     score: 0.24392342567443848,
    //     label: 'human face',
    //     box: { xmin: 180, ymin: 67, xmax: 274, ymax: 175 }
    //   },
    //   {
    //     score: 0.15129457414150238,
    //     label: 'american flag',
    //     box: { xmin: 0, ymin: 4, xmax: 106, ymax: 513 }
    //   },
    //   {
    //     score: 0.13649864494800568,
    //     label: 'helmet',
    //     box: { xmin: 277, ymin: 337, xmax: 511, ymax: 511 }
    //   },
    //   {
    //     score: 0.10262022167444229,
    //     label: 'rocket',
    //     box: { xmin: 352, ymin: -1, xmax: 463, ymax: 287 }
    //   }
    // ]

**Example:** Zero-shot object detection w/ `Xenova/owlvit-base-patch32`
(returning top 4 matches and setting a threshold).

Copied

    
    
    const detector = await pipeline('zero-shot-object-detection', 'Xenova/owlvit-base-patch32');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/beach.png';
    const candidate_labels = ['hat', 'book', 'sunglasses', 'camera'];
    const output = await detector(url, candidate_labels, { top_k: 4, threshold: 0.05 });
    // [
    //   {
    //     score: 0.1606510728597641,
    //     label: 'sunglasses',
    //     box: { xmin: 347, ymin: 229, xmax: 429, ymax: 264 }
    //   },
    //   {
    //     score: 0.08935828506946564,
    //     label: 'hat',
    //     box: { xmin: 38, ymin: 174, xmax: 258, ymax: 364 }
    //   },
    //   {
    //     score: 0.08530698716640472,
    //     label: 'camera',
    //     box: { xmin: 187, ymin: 350, xmax: 260, ymax: 411 }
    //   },
    //   {
    //     score: 0.08349756896495819,
    //     label: 'book',
    //     box: { xmin: 261, ymin: 280, xmax: 494, ymax: 425 }
    //   }
    // ]

**Kind** : static class of `pipelines`

  * .ZeroShotObjectDetectionPipeline
    * `new ZeroShotObjectDetectionPipeline(options)`
    * `._call()` : `ZeroShotObjectDetectionPipelineCallback`

* * *

###  new ZeroShotObjectDetectionPipeline(options)

Create a new ZeroShotObjectDetectionPipeline.

Param| Type| Description  
---|---|---  
options| `TextImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  zeroShotObjectDetectionPipeline._call() : <code>
ZeroShotObjectDetectionPipelineCallback </code>

**Kind** : instance method of `ZeroShotObjectDetectionPipeline`

* * *

##  pipelines.DocumentQuestionAnsweringPipeline

Document Question Answering pipeline using any
`AutoModelForDocumentQuestionAnswering`. The inputs/outputs are similar to the
(extractive) question answering pipeline; however, the pipeline takes an image
(and optional OCR‚Äôd words/boxes) as input instead of text context.

**Example:** Answer questions about a document with `Xenova/donut-base-
finetuned-docvqa`.

Copied

    
    
    const qa_pipeline = await pipeline('document-question-answering', 'Xenova/donut-base-finetuned-docvqa');
    const image = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/invoice.png';
    const question = 'What is the invoice number?';
    const output = await qa_pipeline(image, question);
    // [{ answer: 'us-001' }]

**Kind** : static class of `pipelines`

  * .DocumentQuestionAnsweringPipeline
    * `new DocumentQuestionAnsweringPipeline(options)`
    * `._call()` : `DocumentQuestionAnsweringPipelineCallback`

* * *

###  new DocumentQuestionAnsweringPipeline(options)

Create a new DocumentQuestionAnsweringPipeline.

Param| Type| Description  
---|---|---  
options| `TextImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  documentQuestionAnsweringPipeline._call() : <code>
DocumentQuestionAnsweringPipelineCallback </code>

**Kind** : instance method of `DocumentQuestionAnsweringPipeline`

* * *

##  pipelines.TextToAudioPipeline

Text-to-audio generation pipeline using any `AutoModelForTextToWaveform` or
`AutoModelForTextToSpectrogram`. This pipeline generates an audio file from an
input text and optional other conditional inputs.

**Example:** Generate audio from text with `Xenova/speecht5_tts`.

Copied

    
    
    const synthesizer = await pipeline('text-to-speech', 'Xenova/speecht5_tts', { quantized: false });
    const speaker_embeddings = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/speaker_embeddings.bin';
    const out = await synthesizer('Hello, my dog is cute', { speaker_embeddings });
    // {
    //   audio: Float32Array(26112) [-0.00005657337896991521, 0.00020583874720614403, ...],
    //   sampling_rate: 16000
    // }

You can then save the audio to a .wav file with the `wavefile` package:

Copied

    
    
    import wavefile from 'wavefile';
    import fs from 'fs';
    
    const wav = new wavefile.WaveFile();
    wav.fromScratch(1, out.sampling_rate, '32f', out.audio);
    fs.writeFileSync('out.wav', wav.toBuffer());

**Example:** Multilingual speech generation with `Xenova/mms-tts-fra`. See
[here](https://huggingface.co/models?pipeline_tag=text-to-
speech&other=vits&sort=trending) for the full list of available languages
(1107).

Copied

    
    
    const synthesizer = await pipeline('text-to-speech', 'Xenova/mms-tts-fra');
    const out = await synthesizer('Bonjour');
    // {
    //   audio: Float32Array(23808) [-0.00037693005288019776, 0.0003325853613205254, ...],
    //   sampling_rate: 16000
    // }

**Kind** : static class of `pipelines`

  * .TextToAudioPipeline
    * `new TextToAudioPipeline(options)`
    * `._call()` : `TextToAudioPipelineCallback`

* * *

###  new TextToAudioPipeline(options)

Create a new TextToAudioPipeline.

Param| Type| Description  
---|---|---  
options| `TextToAudioPipelineConstructorArgs`| An object used to instantiate
the pipeline.  
  
* * *

###  textToAudioPipeline._call() : <code> TextToAudioPipelineCallback </code>

**Kind** : instance method of `TextToAudioPipeline`

* * *

##  pipelines.ImageToImagePipeline

Image to Image pipeline using any `AutoModelForImageToImage`. This pipeline
generates an image based on a previous image input.

**Example:** Super-resolution w/ `Xenova/swin2SR-classical-sr-x2-64`

Copied

    
    
    const upscaler = await pipeline('image-to-image', 'Xenova/swin2SR-classical-sr-x2-64');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/butterfly.jpg';
    const output = await upscaler(url);
    // RawImage {
    //   data: Uint8Array(786432) [ 41, 31, 24,  43, ... ],
    //   width: 512,
    //   height: 512,
    //   channels: 3
    // }

**Kind** : static class of `pipelines`

  * .ImageToImagePipeline
    * `new ImageToImagePipeline(options)`
    * `._call()` : `ImageToImagePipelineCallback`

* * *

###  new ImageToImagePipeline(options)

Create a new ImageToImagePipeline.

Param| Type| Description  
---|---|---  
options| `ImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  imageToImagePipeline._call() : <code> ImageToImagePipelineCallback
</code>

**Kind** : instance method of `ImageToImagePipeline`

* * *

##  pipelines.DepthEstimationPipeline

Depth estimation pipeline using any `AutoModelForDepthEstimation`. This
pipeline predicts the depth of an image.

**Example:** Depth estimation w/ `Xenova/dpt-hybrid-midas`

Copied

    
    
    const depth_estimator = await pipeline('depth-estimation', 'Xenova/dpt-hybrid-midas');
    const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/cats.jpg';
    const out = await depth_estimator(url);
    // {
    //   predicted_depth: Tensor {
    //     dims: [ 384, 384 ],
    //     type: 'float32',
    //     data: Float32Array(147456) [ 542.859130859375, 545.2833862304688, 546.1649169921875, ... ],
    //     size: 147456
    //   },
    //   depth: RawImage {
    //     data: Uint8Array(307200) [ 86, 86, 86, ... ],
    //     width: 640,
    //     height: 480,
    //     channels: 1
    //   }
    // }

**Kind** : static class of `pipelines`

  * .DepthEstimationPipeline
    * `new DepthEstimationPipeline(options)`
    * `._call()` : `DepthEstimationPipelineCallback`

* * *

###  new DepthEstimationPipeline(options)

Create a new DepthEstimationPipeline.

Param| Type| Description  
---|---|---  
options| `ImagePipelineConstructorArgs`| An object used to instantiate the
pipeline.  
  
* * *

###  depthEstimationPipeline._call() : <code> DepthEstimationPipelineCallback
</code>

**Kind** : instance method of `DepthEstimationPipeline`

* * *

##  pipelines.pipeline(task, [model], [options]) ‚áí <code> * </code>

Utility factory method to build a `Pipeline` object.

**Kind** : static method of `pipelines`  
**Returns** : `*` \- A Pipeline object for the specified task.  
**Throws** :

  * `Error` If an unsupported pipeline is requested.

Param| Type| Default| Description  
---|---|---|---  
task| `T`| | The task defining which pipeline will be returned. Currently accepted tasks are:

  * `"audio-classification"`: will return a `AudioClassificationPipeline`.
  * `"automatic-speech-recognition"`: will return a `AutomaticSpeechRecognitionPipeline`.
  * `"depth-estimation"`: will return a `DepthEstimationPipeline`.
  * `"document-question-answering"`: will return a `DocumentQuestionAnsweringPipeline`.
  * `"feature-extraction"`: will return a `FeatureExtractionPipeline`.
  * `"fill-mask"`: will return a `FillMaskPipeline`.
  * `"image-classification"`: will return a `ImageClassificationPipeline`.
  * `"image-segmentation"`: will return a `ImageSegmentationPipeline`.
  * `"image-to-text"`: will return a `ImageToTextPipeline`.
  * `"object-detection"`: will return a `ObjectDetectionPipeline`.
  * `"question-answering"`: will return a `QuestionAnsweringPipeline`.
  * `"summarization"`: will return a `SummarizationPipeline`.
  * `"text2text-generation"`: will return a `Text2TextGenerationPipeline`.
  * `"text-classification"` (alias "sentiment-analysis" available): will return a `TextClassificationPipeline`.
  * `"text-generation"`: will return a `TextGenerationPipeline`.
  * `"token-classification"` (alias "ner" available): will return a `TokenClassificationPipeline`.
  * `"translation"`: will return a `TranslationPipeline`.
  * `"translation_xx_to_yy"`: will return a `TranslationPipeline`.
  * `"zero-shot-classification"`: will return a `ZeroShotClassificationPipeline`.
  * `"zero-shot-audio-classification"`: will return a `ZeroShotAudioClassificationPipeline`.
  * `"zero-shot-image-classification"`: will return a `ZeroShotImageClassificationPipeline`.
  * `"zero-shot-object-detection"`: will return a `ZeroShotObjectDetectionPipeline`.

  
[model]| `string`| `null`| The name of the pre-trained model to use. If not
specified, the default model for the task will be used.  
[options]| `*`| | Optional parameters for the pipeline.  
  
* * *

##  pipelines~ImagePipelineInputs : <code> string </code> | <code> RawImage </code> | <code> URL </code>

**Kind** : inner typedef of `pipelines`

* * *

##  pipelines~AudioPipelineInputs : <code> string </code> | <code> URL </code> | <code> Float32Array </code> | <code> Float64Array </code>

**Kind** : inner typedef of `pipelines`

* * *

##  pipelines~BoundingBox : <code> Object </code>

**Kind** : inner typedef of `pipelines`  
**Properties**

Name| Type| Description  
---|---|---  
xmin| `number`| The minimum x coordinate of the bounding box.  
ymin| `number`| The minimum y coordinate of the bounding box.  
xmax| `number`| The maximum x coordinate of the bounding box.  
ymax| `number`| The maximum y coordinate of the bounding box.  
  
* * *

##  pipelines~Disposable ‚áí <code> Promise. < void > </code>

**Kind** : inner typedef of `pipelines`  
**Returns** : `Promise.<void>` \- A promise that resolves when the item has
been disposed.  
**Properties**

Name| Type| Description  
---|---|---  
dispose| `DisposeType`| A promise that resolves when the pipeline has been
disposed.  
  
* * *

##  pipelines~TextPipelineConstructorArgs : <code> Object </code>

An object used to instantiate a text-based pipeline.

**Kind** : inner typedef of `pipelines`  
**Properties**

Name| Type| Description  
---|---|---  
task| `string`| The task of the pipeline. Useful for specifying subtasks.  
model| `PreTrainedModel`| The model used by the pipeline.  
tokenizer| `PreTrainedTokenizer`| The tokenizer used by the pipeline.  
  
* * *

##  pipelines~ImagePipelineConstructorArgs : <code> Object </code>

An object used to instantiate an audio-based pipeline.

**Kind** : inner typedef of `pipelines`  
**Properties**

Name| Type| Description  
---|---|---  
task| `string`| The task of the pipeline. Useful for specifying subtasks.  
model| `PreTrainedModel`| The model used by the pipeline.  
processor| `Processor`| The processor used by the pipeline.  
  
* * *

##  pipelines~TextImagePipelineConstructorArgs : <code> Object </code>

An object used to instantiate a text- and audio-based pipeline.

**Kind** : inner typedef of `pipelines`  
**Properties**

Name| Type| Description  
---|---|---  
task| `string`| The task of the pipeline. Useful for specifying subtasks.  
model| `PreTrainedModel`| The model used by the pipeline.  
tokenizer| `PreTrainedTokenizer`| The tokenizer used by the pipeline.  
processor| `Processor`| The processor used by the pipeline.  
  
* * *

##  pipelines~TextClassificationPipelineType ‚áí <code> Promise. <
(TextClassificationOutput|Array < TextClassificationOutput > ) > </code>

Parameters specific to text classification pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(TextClassificationOutput|Array<TextClassificationOutput>)>` \- An
array or object containing the predicted labels and scores.

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>`| The input text(s) to be classified.  
[options]| `TextClassificationPipelineOptions`| The options to use for text
classification.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
label| `string`| | The label predicted.  
score| `number`| | The corresponding probability.  
[top_k]| `number`| `1`| The number of top predictions to be returned.  
  
* * *

##  pipelines~TokenClassificationPipelineType ‚áí <code> Promise. <
(TokenClassificationOutput|Array < TokenClassificationOutput > ) > </code>

Parameters specific to token classification pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(TokenClassificationOutput|Array<TokenClassificationOutput>)>` \-
The result.

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>`| One or several texts (or one list of texts) for token classification.  
[options]| `TokenClassificationPipelineOptions`| The options to use for token
classification.  
  
**Properties**

Name| Type| Description  
---|---|---  
word| `string`| The token/word classified. This is obtained by decoding the
selected tokens.  
score| `number`| The corresponding probability for `entity`.  
entity| `string`| The entity predicted for that token/word.  
index| `number`| The index of the corresponding token in the sentence.  
[start]| `number`| The index of the start of the corresponding entity in the
sentence.  
[end]| `number`| The index of the end of the corresponding entity in the
sentence.  
[ignore_labels]| `Array.<string>`| A list of labels to ignore.  
  
* * *

##  pipelines~QuestionAnsweringPipelineType ‚áí <code> Promise. <
(QuestionAnsweringOutput|Array < QuestionAnsweringOutput > ) > </code>

Parameters specific to question answering pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(QuestionAnsweringOutput|Array<QuestionAnsweringOutput>)>` \- An
array or object containing the predicted answers and scores.

Param| Type| Description  
---|---|---  
question| `string` | `Array<string>`| One or several question(s) (must be used in conjunction with the `context` argument).  
context| `string` | `Array<string>`| One or several context(s) associated with the question(s) (must be used in conjunction with the `question` argument).  
[options]| `QuestionAnsweringPipelineOptions`| The options to use for question
answering.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
score| `number`| | The probability associated to the answer.  
[start]| `number`| | The character start index of the answer (in the tokenized version of the input).  
[end]| `number`| | The character end index of the answer (in the tokenized version of the input).  
answer| `string`| | The answer to the question.  
[top_k]| `number`| `1`| The number of top answer predictions to be returned.  
  
* * *

##  pipelines~FillMaskPipelineType ‚áí <code> Promise. < (FillMaskOutput|Array <
FillMaskOutput > ) > </code>

Parameters specific to fill mask pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** : `Promise.<(FillMaskOutput|Array<FillMaskOutput>)>` \- An array
of objects containing the score, predicted token, predicted token string, and
the sequence with the predicted token filled in, or an array of such arrays
(one for each input text). If only one input text is given, the output will be
an array of objects.  
**Throws** :

  * `Error` When the mask token is not found in the input text.

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>`| One or several texts (or one list of prompts) with masked tokens.  
[options]| `FillMaskPipelineOptions`| The options to use for masked language
modelling.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
sequence| `string`| | The corresponding input with the mask token prediction.  
score| `number`| | The corresponding probability.  
token| `number`| | The predicted token id (to replace the masked one).  
token_str| `string`| | The predicted token (to replace the masked one).  
[top_k]| `number`| `5`| When passed, overrides the number of predictions to
return.  
  
* * *

##  pipelines~Text2TextGenerationPipelineType ‚áí <code> Promise. <
(Text2TextGenerationOutput|Array < Text2TextGenerationOutput > ) > </code>

**Kind** : inner typedef of `pipelines`

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>`| Input text for the encoder.  
[options]| `*`| Additional keyword arguments to pass along to the generate
method of the model.  
  
**Properties**

Name| Type| Description  
---|---|---  
generated_text| `string`| The generated text.  
  
* * *

##  pipelines~SummarizationPipelineType ‚áí <code> Promise. <
(SummarizationOutput|Array < SummarizationOutput > ) > </code>

**Kind** : inner typedef of `pipelines`

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>`| One or several articles (or one list of articles) to summarize.  
[options]| `*`| Additional keyword arguments to pass along to the generate
method of the model.  
  
**Properties**

Name| Type| Description  
---|---|---  
summary_text| `string`| The summary text.  
  
* * *

##  pipelines~TranslationPipelineType ‚áí <code> Promise. <
(TranslationOutput|Array < TranslationOutput > ) > </code>

**Kind** : inner typedef of `pipelines`

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>`| Texts to be translated.  
[options]| `*`| Additional keyword arguments to pass along to the generate
method of the model.  
  
**Properties**

Name| Type| Description  
---|---|---  
translation_text| `string`| The translated text.  
  
* * *

##  pipelines~TextGenerationPipelineType ‚áí <code> Promise. <
(TextGenerationOutput|Array < TextGenerationOutput > ) > </code>

Parameters specific to text-generation pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** : `Promise.<(TextGenerationOutput|Array<TextGenerationOutput>)>`
\- An array or object containing the generated texts.

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>` | `Chat` | `Array<Chat>`| One or several prompts (or one list of prompts) to complete.  
[options]| `Partial.<TextGenerationConfig>`| Additional keyword arguments to
pass along to the generate method of the model.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
generated_text| `string` | `Chat`| | The generated text.  
[add_special_tokens]| `boolean`| | Whether or not to add special tokens when tokenizing the sequences.  
[return_full_text]| `boolean`| `true`| If set to `false` only added text is
returned, otherwise the full text is returned.  
  
* * *

##  pipelines~ZeroShotClassificationPipelineType ‚áí <code> Promise. <
(ZeroShotClassificationOutput|Array < ZeroShotClassificationOutput > ) >
</code>

Parameters specific to zero-shot classification pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(ZeroShotClassificationOutput|Array<ZeroShotClassificationOutput>)>`
\- An array or object containing the predicted labels and scores.

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>`| The sequence(s) to classify, will be truncated if the model input is too large.  
candidate_labels| `string` | `Array<string>`| The set of possible class labels to classify each sequence into. Can be a single label, a string of comma-separated labels, or a list of labels.  
[options]| `ZeroShotClassificationPipelineOptions`| The options to use for
zero-shot classification.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
sequence| `string`| | The sequence for which this is the output.  
labels| `Array.<string>`| | The labels sorted by order of likelihood.  
scores| `Array.<number>`| | The probabilities for each of the labels.  
[hypothesis_template]| `string`| `""This example is {}.""`| The template used
to turn each candidate label into an NLI-style hypothesis. The candidate label
will replace the &#123;} placeholder.  
[multi_label]| `boolean`| `false`| Whether or not multiple candidate labels
can be true. If `false`, the scores are normalized such that the sum of the
label likelihoods for each sequence is 1. If `true`, the labels are considered
independent and probabilities are normalized for each candidate by doing a
softmax of the entailment score vs. the contradiction score.  
  
* * *

##  pipelines~FeatureExtractionPipelineType ‚áí <code> Promise. < Tensor >
</code>

Parameters specific to feature extraction pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** : `Promise.<Tensor>` \- The features computed by the model.

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>`| One or several texts (or one list of texts) to get the features of.  
[options]| `FeatureExtractionPipelineOptions`| The options to use for feature
extraction.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
[pooling]| `'none'` | `'mean'` | `'cls'`| `"none"`| The pooling method to use.  
[normalize]| `boolean`| `false`| Whether or not to normalize the embeddings in
the last dimension.  
[quantize]| `boolean`| `false`| Whether or not to quantize the embeddings.  
[precision]| `'binary'` | `'ubinary'`| `'binary'`| The precision to use for quantization.  
  
* * *

##  pipelines~ImageFeatureExtractionPipelineType ‚áí <code> Promise. < Tensor >
</code>

Parameters specific to image feature extraction pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** : `Promise.<Tensor>` \- The image features computed by the model.

Param| Type| Description  
---|---|---  
images| `ImagePipelineInputs`| One or several images (or one list of images)
to get the features of.  
[options]| `ImageFeatureExtractionPipelineOptions`| The options to use for
image feature extraction.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
[pool]| `boolean`| ``| Whether or not to return the pooled output. If set to
`false`, the model will return the raw hidden states.  
  
* * *

##  pipelines~AudioClassificationPipelineType ‚áí <code> Promise. <
(AudioClassificationOutput|Array < AudioClassificationOutput > ) > </code>

Parameters specific to audio classification pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(AudioClassificationOutput|Array<AudioClassificationOutput>)>` \- An
array or object containing the predicted labels and scores.

Param| Type| Description  
---|---|---  
audio| `AudioPipelineInputs`| The input audio file(s) to be classified. The
input is either:

  * `string` or `URL` that is the filename/URL of the audio file, the file will be read at the processor's sampling rate to get the waveform using the [`AudioContext`](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext) API. If `AudioContext` is not available, you should pass the raw waveform in as a Float32Array of shape `(n, )`.
  * `Float32Array` or `Float64Array` of shape `(n, )`, representing the raw audio at the correct sampling rate (no further check will be done).

  
[options]| `AudioClassificationPipelineOptions`| The options to use for audio
classification.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
label| `string`| | The label predicted.  
score| `number`| | The corresponding probability.  
[top_k]| `number`| `5`| The number of top labels that will be returned by the
pipeline. If the provided number is `null` or higher than the number of labels
available in the model configuration, it will default to the number of labels.  
  
* * *

##  pipelines~ZeroShotAudioClassificationPipelineType ‚áí <code> Promise. <
(Array < ZeroShotAudioClassificationOutput > |Array < Array <
ZeroShotAudioClassificationOutput > > ) > </code>

Parameters specific to zero-shot audio classification pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(Array<ZeroShotAudioClassificationOutput>|Array<Array<ZeroShotAudioClassificationOutput>>)>`
\- An array of objects containing the predicted labels and scores.

Param| Type| Description  
---|---|---  
audio| `AudioPipelineInputs`| The input audio file(s) to be classified. The
input is either:

  * `string` or `URL` that is the filename/URL of the audio file, the file will be read at the processor's sampling rate to get the waveform using the [`AudioContext`](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext) API. If `AudioContext` is not available, you should pass the raw waveform in as a Float32Array of shape `(n, )`.
  * `Float32Array` or `Float64Array` of shape `(n, )`, representing the raw audio at the correct sampling rate (no further check will be done).

  
candidate_labels| `Array.<string>`| The candidate labels for this audio.  
[options]| `ZeroShotAudioClassificationPipelineOptions`| The options to use
for zero-shot audio classification.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
label| `string`| | The label identified by the model. It is one of the suggested `candidate_label`.  
score| `number`| | The score attributed by the model for that label (between 0 and 1).  
[hypothesis_template]| `string`| `""This is a sound of {}.""`| The sentence
used in conjunction with `candidate_labels` to attempt the audio
classification by replacing the placeholder with the candidate_labels. Then
likelihood is estimated by using `logits_per_audio`.  
  
* * *

##  pipelines~Chunk : <code> Object </code>

**Kind** : inner typedef of `pipelines`  
**Properties**

Name| Type| Description  
---|---|---  
timestamp| `*`| The start and end timestamp of the chunk in seconds.  
text| `string`| The recognized text.  
  
* * *

##  pipelines~AutomaticSpeechRecognitionPipelineType ‚áí <code> Promise. <
(AutomaticSpeechRecognitionOutput|Array < AutomaticSpeechRecognitionOutput > )
> </code>

Parameters specific to automatic-speech-recognition pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(AutomaticSpeechRecognitionOutput|Array<AutomaticSpeechRecognitionOutput>)>`
\- An object containing the transcription text and optionally timestamps if
`return_timestamps` is `true`.

Param| Type| Description  
---|---|---  
audio| `AudioPipelineInputs`| The input audio file(s) to be transcribed. The
input is either:

  * `string` or `URL` that is the filename/URL of the audio file, the file will be read at the processor's sampling rate to get the waveform using the [`AudioContext`](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext) API. If `AudioContext` is not available, you should pass the raw waveform in as a Float32Array of shape `(n, )`.
  * `Float32Array` or `Float64Array` of shape `(n, )`, representing the raw audio at the correct sampling rate (no further check will be done).

  
[options]| `Partial.<AutomaticSpeechRecognitionConfig>`| Additional keyword
arguments to pass along to the generate method of the model.  
  
**Properties**

Name| Type| Description  
---|---|---  
text| `string`| The recognized text.  
[chunks]| `Array.<Chunk>`| When using `return_timestamps`, the `chunks` will
become a list containing all the various text chunks identified by the model.  
[return_timestamps]| `boolean` | `'word'`| Whether to return timestamps or not. Default is `false`.  
[chunk_length_s]| `number`| The length of audio chunks to process in seconds.
Default is 0 (no chunking).  
[stride_length_s]| `number`| The length of overlap between consecutive audio
chunks in seconds. If not provided, defaults to `chunk_length_s / 6`.  
[force_full_sequences]| `boolean`| Whether to force outputting full sequences
or not. Default is `false`.  
[language]| `string`| The source language. Default is `null`, meaning it
should be auto-detected. Use this to potentially improve performance if the
source language is known.  
[task]| `string`| The task to perform. Default is `null`, meaning it should be
auto-detected.  
[num_frames]| `number`| The number of frames in the input audio.  
  
* * *

##  pipelines~ImageToTextPipelineType ‚áí <code> Promise. <
(ImageToTextOutput|Array < ImageToTextOutput > ) > </code>

**Kind** : inner typedef of `pipelines`  
**Returns** : `Promise.<(ImageToTextOutput|Array<ImageToTextOutput>)>` \- An
object (or array of objects) containing the generated text(s).

Param| Type| Description  
---|---|---  
texts| `ImagePipelineInputs`| The images to be captioned.  
[options]| `*`| Additional keyword arguments to pass along to the generate
method of the model.  
  
**Properties**

Name| Type| Description  
---|---|---  
generated_text| `string`| The generated text.  
  
* * *

##  pipelines~ImageClassificationPipelineType ‚áí <code> Promise. <
(ImageClassificationOutput|Array < ImageClassificationOutput > ) > </code>

Parameters specific to image classification pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(ImageClassificationOutput|Array<ImageClassificationOutput>)>` \- An
array or object containing the predicted labels and scores.

Param| Type| Description  
---|---|---  
images| `ImagePipelineInputs`| The input images(s) to be classified.  
[options]| `ImageClassificationPipelineOptions`| The options to use for image
classification.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
label| `string`| | The label identified by the model.  
score| `number`| | The score attributed by the model for that label.  
[top_k]| `number`| `1`| The number of top labels that will be returned by the
pipeline.  
  
* * *

##  pipelines~ImageSegmentationPipelineType ‚áí <code> Promise. < Array <
ImageSegmentationPipelineOutput > > </code>

Parameters specific to image segmentation pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** : `Promise.<Array<ImageSegmentationPipelineOutput>>` \- The
annotated segments.

Param| Type| Description  
---|---|---  
images| `ImagePipelineInputs`| The input images.  
[options]| `ImageSegmentationPipelineOptions`| The options to use for image
segmentation.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
label| `string`| | The label of the segment.  
score| `number` | `null`| | The score of the segment.  
mask| `RawImage`| | The mask of the segment.  
[threshold]| `number`| `0.5`| Probability threshold to filter out predicted
masks.  
[mask_threshold]| `number`| `0.5`| Threshold to use when turning the predicted
masks into binary values.  
[overlap_mask_area_threshold]| `number`| `0.8`| Mask overlap threshold to
eliminate small, disconnected segments.  
[subtask]| `null` | `string`| ``| Segmentation task to be performed. One of [`panoptic`, `instance`, and `semantic`], depending on model capabilities. If not set, the pipeline will attempt to resolve (in that order).  
[label_ids_to_fuse]| `Array.<number>`| ``| List of label ids to fuse. If not
set, do not fuse any labels.  
[target_sizes]| `Array.<Array<number>>`| ``| List of target sizes for the
input images. If not set, use the original image sizes.  
  
* * *

##  pipelines~ZeroShotImageClassificationPipelineType ‚áí <code> Promise. <
(Array < ZeroShotImageClassificationOutput > |Array < Array <
ZeroShotImageClassificationOutput > > ) > </code>

Parameters specific to zero-shot image classification pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(Array<ZeroShotImageClassificationOutput>|Array<Array<ZeroShotImageClassificationOutput>>)>`
\- An array of objects containing the predicted labels and scores.

Param| Type| Description  
---|---|---  
images| `ImagePipelineInputs`| The input images.  
candidate_labels| `Array.<string>`| The candidate labels for this image.  
[options]| `ZeroShotImageClassificationPipelineOptions`| The options to use
for zero-shot image classification.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
label| `string`| | The label identified by the model. It is one of the suggested `candidate_label`.  
score| `number`| | The score attributed by the model for that label (between 0 and 1).  
[hypothesis_template]| `string`| `""This is a photo of {}""`| The sentence
used in conjunction with `candidate_labels` to attempt the image
classification by replacing the placeholder with the candidate_labels. Then
likelihood is estimated by using `logits_per_image`.  
  
* * *

##  pipelines~ObjectDetectionPipelineType ‚áí <code> Promise. <
(ObjectDetectionPipelineOutput|Array < ObjectDetectionPipelineOutput > ) >
</code>

Parameters specific to object detection pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(ObjectDetectionPipelineOutput|Array<ObjectDetectionPipelineOutput>)>`
\- A list of objects or a list of list of objects.

Param| Type| Description  
---|---|---  
images| `ImagePipelineInputs`| The input images.  
[options]| `ObjectDetectionPipelineOptions`| The options to use for object
detection.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
label| `string`| | The class label identified by the model.  
score| `number`| | The score attributed by the model for that label.  
box| `BoundingBox`| | The bounding box of detected object in image's original size, or as a percentage if `percentage` is set to true.  
[threshold]| `number`| `0.9`| The threshold used to filter boxes by score.  
[percentage]| `boolean`| `false`| Whether to return the boxes coordinates in
percentage (true) or in pixels (false).  
  
* * *

##  pipelines~ZeroShotObjectDetectionPipelineType ‚áí <code> Promise. < (Array <
ZeroShotObjectDetectionOutput > |Array < Array < ZeroShotObjectDetectionOutput
> > ) > </code>

Parameters specific to zero-shot object detection pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(Array<ZeroShotObjectDetectionOutput>|Array<Array<ZeroShotObjectDetectionOutput>>)>`
\- An array of objects containing the predicted labels, scores, and bounding
boxes.

Param| Type| Description  
---|---|---  
images| `ImagePipelineInputs`| The input images.  
candidate_labels| `Array.<string>`| What the model should recognize in the
image.  
[options]| `ZeroShotObjectDetectionPipelineOptions`| The options to use for
zero-shot object detection.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
label| `string`| | Text query corresponding to the found object.  
score| `number`| | Score corresponding to the object (between 0 and 1).  
box| `BoundingBox`| | Bounding box of the detected object in image's original size, or as a percentage if `percentage` is set to true.  
[threshold]| `number`| `0.1`| The probability necessary to make a prediction.  
[top_k]| `number`| ``| The number of top predictions that will be returned by
the pipeline. If the provided number is `null` or higher than the number of
predictions available, it will default to the number of predictions.  
[percentage]| `boolean`| `false`| Whether to return the boxes coordinates in
percentage (true) or in pixels (false).  
  
* * *

##  pipelines~DocumentQuestionAnsweringPipelineType ‚áí <code> Promise. <
(DocumentQuestionAnsweringOutput|Array < DocumentQuestionAnsweringOutput > ) >
</code>

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(DocumentQuestionAnsweringOutput|Array<DocumentQuestionAnsweringOutput>)>`
\- An object (or array of objects) containing the answer(s).

Param| Type| Description  
---|---|---  
image| `ImageInput`| The image of the document to use.  
question| `string`| A question to ask of the document.  
[options]| `*`| Additional keyword arguments to pass along to the generate
method of the model.  
  
**Properties**

Name| Type| Description  
---|---|---  
answer| `string`| The generated text.  
  
* * *

##  pipelines~TextToAudioPipelineConstructorArgs : <code> Object </code>

**Kind** : inner typedef of `pipelines`  
**Properties**

Name| Type| Description  
---|---|---  
[vocoder]| `PreTrainedModel`| The vocoder used by the pipeline (if the model
uses one). If not provided, use the default HifiGan vocoder.  
  
* * *

##  pipelines~TextToAudioPipelineType ‚áí <code> Promise. < TextToAudioOutput >
</code>

Parameters specific to text-to-audio pipelines.

**Kind** : inner typedef of `pipelines`  
**Returns** : `Promise.<TextToAudioOutput>` \- An object containing the
generated audio and sampling rate.

Param| Type| Description  
---|---|---  
texts| `string` | `Array<string>`| The text(s) to generate.  
options| `TextToAudioPipelineOptions`| Parameters passed to the model
generation/forward method.  
  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
audio| `Float32Array`| | The generated audio waveform.  
sampling_rate| `number`| | The sampling rate of the generated audio waveform.  
[speaker_embeddings]| `Tensor` | `Float32Array` | `string` | `URL`| ``| The speaker embeddings (if the model requires it).  
  
* * *

##  pipelines~ImageToImagePipelineType ‚áí <code> Promise. < (RawImage|Array <
RawImage > ) > </code>

**Kind** : inner typedef of `pipelines`  
**Returns** : `Promise.<(RawImage|Array<RawImage>)>` \- The transformed image
or list of images.

Param| Type| Description  
---|---|---  
images| `ImagePipelineInputs`| The images to transform.  
  
* * *

##  pipelines~DepthEstimationPipelineType ‚áí <code> Promise. <
(DepthEstimationPipelineOutput|Array < DepthEstimationPipelineOutput > ) >
</code>

**Kind** : inner typedef of `pipelines`  
**Returns** :
`Promise.<(DepthEstimationPipelineOutput|Array<DepthEstimationPipelineOutput>)>`
\- An image or a list of images containing result(s).

Param| Type| Description  
---|---|---  
images| `ImagePipelineInputs`| The images to compute depth for.  
  
**Properties**

Name| Type| Description  
---|---|---  
predicted_depth| `Tensor`| The raw depth map predicted by the model.  
depth| `RawImage`| The processed depth map as an image (with the same size as
the input image).  
  
* * *

##  pipelines~AllTasks : <code> * </code>

All possible pipeline types.

**Kind** : inner typedef of `pipelines`

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/pipelines.md)

[‚ÜêIndex](/docs/transformers.js/v3.0.0/en/api/transformers)
[Models‚Üí](/docs/transformers.js/v3.0.0/en/api/models)

pipelines pipelines.Pipeline ‚áê ` Callable ` new Pipeline(options) pipeline.dispose() : ` DisposeType ` pipeline._call(...args) pipelines.TextClassificationPipeline new TextClassificationPipeline(options) textClassificationPipeline._call() : ` TextClassificationPipelineCallback ` pipelines.TokenClassificationPipeline new TokenClassificationPipeline(options) tokenClassificationPipeline._call() : ` TokenClassificationPipelineCallback ` pipelines.QuestionAnsweringPipeline new QuestionAnsweringPipeline(options) questionAnsweringPipeline._call() : ` QuestionAnsweringPipelineCallback ` pipelines.FillMaskPipeline new FillMaskPipeline(options) fillMaskPipeline._call() : ` FillMaskPipelineCallback ` pipelines.Text2TextGenerationPipeline new Text2TextGenerationPipeline(options) text2TextGenerationPipeline._key : ` ‚Äô generated_text ‚Äô ` text2TextGenerationPipeline._call() : ` Text2TextGenerationPipelineCallback ` pipelines.SummarizationPipeline new SummarizationPipeline(options) summarizationPipeline._key : ` ‚Äô summary_text ‚Äô ` pipelines.TranslationPipeline new TranslationPipeline(options) translationPipeline._key : ` ‚Äô translation_text ‚Äô ` pipelines.TextGenerationPipeline new TextGenerationPipeline(options) textGenerationPipeline._call() : ` TextGenerationPipelineCallback ` pipelines.ZeroShotClassificationPipeline new ZeroShotClassificationPipeline(options) zeroShotClassificationPipeline.model : ` any ` zeroShotClassificationPipeline._call() : ` ZeroShotClassificationPipelineCallback ` pipelines.FeatureExtractionPipeline new FeatureExtractionPipeline(options) featureExtractionPipeline._call() : ` FeatureExtractionPipelineCallback ` pipelines.ImageFeatureExtractionPipeline new ImageFeatureExtractionPipeline(options) imageFeatureExtractionPipeline._call() : ` ImageFeatureExtractionPipelineCallback ` pipelines.AudioClassificationPipeline new AudioClassificationPipeline(options) audioClassificationPipeline._call() : ` AudioClassificationPipelineCallback ` pipelines.ZeroShotAudioClassificationPipeline new ZeroShotAudioClassificationPipeline(options) zeroShotAudioClassificationPipeline._call() : ` ZeroShotAudioClassificationPipelineCallback ` pipelines.AutomaticSpeechRecognitionPipeline new AutomaticSpeechRecognitionPipeline(options) automaticSpeechRecognitionPipeline._call() : ` AutomaticSpeechRecognitionPipelineCallback ` pipelines.ImageToTextPipeline new ImageToTextPipeline(options) imageToTextPipeline._call() : ` ImageToTextPipelineCallback ` pipelines.ImageClassificationPipeline new ImageClassificationPipeline(options) imageClassificationPipeline._call() : ` ImageClassificationPipelineCallback ` pipelines.ImageSegmentationPipeline new ImageSegmentationPipeline(options) imageSegmentationPipeline._call() : ` ImageSegmentationPipelineCallback ` pipelines.ZeroShotImageClassificationPipeline new ZeroShotImageClassificationPipeline(options) zeroShotImageClassificationPipeline._call() : ` ZeroShotImageClassificationPipelineCallback ` pipelines.ObjectDetectionPipeline new ObjectDetectionPipeline(options) objectDetectionPipeline._call() : ` ObjectDetectionPipelineCallback ` pipelines.ZeroShotObjectDetectionPipeline new ZeroShotObjectDetectionPipeline(options) zeroShotObjectDetectionPipeline._call() : ` ZeroShotObjectDetectionPipelineCallback ` pipelines.DocumentQuestionAnsweringPipeline new DocumentQuestionAnsweringPipeline(options) documentQuestionAnsweringPipeline._call() : ` DocumentQuestionAnsweringPipelineCallback ` pipelines.TextToAudioPipeline new TextToAudioPipeline(options) textToAudioPipeline._call() : ` TextToAudioPipelineCallback ` pipelines.ImageToImagePipeline new ImageToImagePipeline(options) imageToImagePipeline._call() : ` ImageToImagePipelineCallback ` pipelines.DepthEstimationPipeline new DepthEstimationPipeline(options) depthEstimationPipeline._call() : ` DepthEstimationPipelineCallback ` pipelines.pipeline(task, [model], [options]) ‚áí ` * ` pipelines~ImagePipelineInputs : ` string ` | ` RawImage ` | ` URL ` pipelines~AudioPipelineInputs : ` string ` | ` URL ` | ` Float32Array ` | ` Float64Array ` pipelines~BoundingBox : ` Object ` pipelines~Disposable ‚áí ` Promise. < void > ` pipelines~TextPipelineConstructorArgs : ` Object ` pipelines~ImagePipelineConstructorArgs : ` Object ` pipelines~TextImagePipelineConstructorArgs : ` Object ` pipelines~TextClassificationPipelineType ‚áí ` Promise. < (TextClassificationOutput|Array < TextClassificationOutput > ) > ` pipelines~TokenClassificationPipelineType ‚áí ` Promise. < (TokenClassificationOutput|Array < TokenClassificationOutput > ) > ` pipelines~QuestionAnsweringPipelineType ‚áí ` Promise. < (QuestionAnsweringOutput|Array < QuestionAnsweringOutput > ) > ` pipelines~FillMaskPipelineType ‚áí ` Promise. < (FillMaskOutput|Array < FillMaskOutput > ) > ` pipelines~Text2TextGenerationPipelineType ‚áí ` Promise. < (Text2TextGenerationOutput|Array < Text2TextGenerationOutput > ) > ` pipelines~SummarizationPipelineType ‚áí ` Promise. < (SummarizationOutput|Array < SummarizationOutput > ) > ` pipelines~TranslationPipelineType ‚áí ` Promise. < (TranslationOutput|Array < TranslationOutput > ) > ` pipelines~TextGenerationPipelineType ‚áí ` Promise. < (TextGenerationOutput|Array < TextGenerationOutput > ) > ` pipelines~ZeroShotClassificationPipelineType ‚áí ` Promise. < (ZeroShotClassificationOutput|Array < ZeroShotClassificationOutput > ) > ` pipelines~FeatureExtractionPipelineType ‚áí ` Promise. < Tensor > ` pipelines~ImageFeatureExtractionPipelineType ‚áí ` Promise. < Tensor > ` pipelines~AudioClassificationPipelineType ‚áí ` Promise. < (AudioClassificationOutput|Array < AudioClassificationOutput > ) > ` pipelines~ZeroShotAudioClassificationPipelineType ‚áí ` Promise. < (Array < ZeroShotAudioClassificationOutput > |Array < Array < ZeroShotAudioClassificationOutput > > ) > ` pipelines~Chunk : ` Object ` pipelines~AutomaticSpeechRecognitionPipelineType ‚áí ` Promise. < (AutomaticSpeechRecognitionOutput|Array < AutomaticSpeechRecognitionOutput > ) > ` pipelines~ImageToTextPipelineType ‚áí ` Promise. < (ImageToTextOutput|Array < ImageToTextOutput > ) > ` pipelines~ImageClassificationPipelineType ‚áí ` Promise. < (ImageClassificationOutput|Array < ImageClassificationOutput > ) > ` pipelines~ImageSegmentationPipelineType ‚áí ` Promise. < Array < ImageSegmentationPipelineOutput > > ` pipelines~ZeroShotImageClassificationPipelineType ‚áí ` Promise. < (Array < ZeroShotImageClassificationOutput > |Array < Array < ZeroShotImageClassificationOutput > > ) > ` pipelines~ObjectDetectionPipelineType ‚áí ` Promise. < (ObjectDetectionPipelineOutput|Array < ObjectDetectionPipelineOutput > ) > ` pipelines~ZeroShotObjectDetectionPipelineType ‚áí ` Promise. < (Array < ZeroShotObjectDetectionOutput > |Array < Array < ZeroShotObjectDetectionOutput > > ) > ` pipelines~DocumentQuestionAnsweringPipelineType ‚áí ` Promise. < (DocumentQuestionAnsweringOutput|Array < DocumentQuestionAnsweringOutput > ) > ` pipelines~TextToAudioPipelineConstructorArgs : ` Object ` pipelines~TextToAudioPipelineType ‚áí ` Promise. < TextToAudioOutput > ` pipelines~ImageToImagePipelineType ‚áí ` Promise. < (RawImage|Array < RawImage > ) > ` pipelines~DepthEstimationPipelineType ‚áí ` Promise. < (DepthEstimationPipelineOutput|Array < DepthEstimationPipelineOutput > ) > ` pipelines~AllTasks : ` * `

The documentation page API/POSTPROCESSOR doesn‚Äôt exist in v3.0.0, but exists
on the main version. Click
[here](/docs/transformers.js/main/en/api/PostProcessor) to redirect to the
main version of the documentation.

The documentation page API/PRETOKENIZER doesn‚Äôt exist in v3.0.0, but exists on
the main version. Click [here](/docs/transformers.js/main/en/api/PreTokenizer)
to redirect to the main version of the documentation.

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

processors

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  processors

Processors are used to prepare non-textual inputs (e.g., image or audio) for a
model.

**Example:** Using a `WhisperProcessor` to prepare an audio input for a model.

Copied

    
    
    import { AutoProcessor, read_audio } from '@huggingface/transformers';
    
    let processor = await AutoProcessor.from_pretrained('openai/whisper-tiny.en');
    let audio = await read_audio('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/v3.0.0/mlk.flac', 16000);
    let { input_features } = await processor(audio);
    // Tensor {
    //   data: Float32Array(240000) [0.4752984642982483, 0.5597258806228638, 0.56434166431427, ...],
    //   dims: [1, 80, 3000],
    //   type: 'float32',
    //   size: 240000,
    // }

  * processors
    *  _static_
      * .FeatureExtractor ‚áê `Callable`
        * `new FeatureExtractor(config)`
        * `._call(...args)`
      * .ImageFeatureExtractor ‚áê `FeatureExtractor`
        * `new ImageFeatureExtractor(config)`
        * `.thumbnail(image, size, [resample])` ‚áí `Promise.<RawImage>`
        * `.crop_margin(image, gray_threshold)` ‚áí `Promise.<RawImage>`
        * `.pad_image(pixelData, imgDims, padSize, options)` ‚áí `*`
        * `.rescale(pixelData)` ‚áí `void`
        * `.get_resize_output_image_size(image, size)` ‚áí `*`
        * `.resize(image)` ‚áí `Promise.<RawImage>`
        * `.preprocess(image, overrides)` ‚áí `Promise.<PreprocessedImage>`
        * `._call(images, ...args)` ‚áí `Promise.<ImageFeatureExtractorResult>`
      * .DetrFeatureExtractor ‚áê `ImageFeatureExtractor`
        * `._call(images)` ‚áí `Promise.<DetrFeatureExtractorResult>`
        * `.post_process_object_detection()` : `*`
        * `.post_process_panoptic_segmentation()` : `*`
      * .Processor ‚áê `Callable`
        * `new Processor(feature_extractor)`
        * `._call(input, ...args)` ‚áí `Promise.<any>`
      * .WhisperProcessor ‚áê `Processor`
        * `._call(audio)` ‚áí `Promise.<any>`
      * .AutoProcessor
        * `.from_pretrained(pretrained_model_name_or_path, options)` ‚áí `Promise.<Processor>`
      * `.data` : `Float32Array`
    * _inner_
      * `~center_to_corners_format(arr)` ‚áí `Array.<number>`
      * `~post_process_semantic_segmentation(outputs, [target_sizes])` ‚áí `*`
        * `~labels` : `Array.<number>`
      * `~post_process_panoptic_segmentation(outputs, [threshold], [mask_threshold], [overlap_mask_area_threshold], [label_ids_to_fuse], [target_sizes])` ‚áí `Array.<{segmentation: Tensor, segments_info: Array<{id: number, label_id: number, score: number}>}>`
      * `~post_process_instance_segmentation(outputs, [threshold], [target_sizes])` ‚áí `Array.<{segmentation: Tensor, segments_info: Array<{id: number, label_id: number, score: number}>}>`
      * `~enforce_size_divisibility(size, divisor)` ‚áí `*`
      * `~HeightWidth` : `*`
      * `~ImageFeatureExtractorResult` : `object`
      * `~PreprocessedImage` : `object`
      * `~DetrFeatureExtractorResult` : `object`
      * `~SamImageProcessorResult` : `object`

* * *

##  processors.FeatureExtractor ‚áê <code> Callable </code>

Base class for feature extractors.

**Kind** : static class of `processors`  
**Extends** : `Callable`

  * .FeatureExtractor ‚áê `Callable`
    * `new FeatureExtractor(config)`
    * `._call(...args)`

* * *

###  new FeatureExtractor(config)

Constructs a new FeatureExtractor instance.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration for the feature extractor.  
  
* * *

###  featureExtractor._call(...args)

This method should be implemented in subclasses to provide the functionality
of the callable object.

**Kind** : instance method of `FeatureExtractor`  
**Overrides** : `_call`  
**Throws** :

  * `Error` If the subclass does not implement the `_call` method.

Param| Type  
---|---  
...args| `Array.<any>`  
  
* * *

##  processors.ImageFeatureExtractor ‚áê <code> FeatureExtractor </code>

Feature extractor for image models.

**Kind** : static class of `processors`  
**Extends** : `FeatureExtractor`

  * .ImageFeatureExtractor ‚áê `FeatureExtractor`
    * `new ImageFeatureExtractor(config)`
    * `.thumbnail(image, size, [resample])` ‚áí `Promise.<RawImage>`
    * `.crop_margin(image, gray_threshold)` ‚áí `Promise.<RawImage>`
    * `.pad_image(pixelData, imgDims, padSize, options)` ‚áí `*`
    * `.rescale(pixelData)` ‚áí `void`
    * `.get_resize_output_image_size(image, size)` ‚áí `*`
    * `.resize(image)` ‚áí `Promise.<RawImage>`
    * `.preprocess(image, overrides)` ‚áí `Promise.<PreprocessedImage>`
    * `._call(images, ...args)` ‚áí `Promise.<ImageFeatureExtractorResult>`

* * *

###  new ImageFeatureExtractor(config)

Constructs a new ImageFeatureExtractor instance.

Param| Type| Default| Description  
---|---|---|---  
config| `Object`| | The configuration for the feature extractor.  
config.image_mean| `Array.<number>`| | The mean values for image normalization.  
config.image_std| `Array.<number>`| | The standard deviation values for image normalization.  
config.do_rescale| `boolean`| | Whether to rescale the image pixel values to the [0,1] range.  
config.rescale_factor| `number`| | The factor to use for rescaling the image pixel values.  
config.do_normalize| `boolean`| | Whether to normalize the image pixel values.  
config.do_resize| `boolean`| | Whether to resize the image.  
config.resample| `number`| | What method to use for resampling.  
config.size| `number` | `Object`| | The size to resize the image to.  
[config.do_flip_channel_order]| `boolean`| `false`| Whether to flip the color
channels from RGB to BGR. Can be overridden by the `do_flip_channel_order`
parameter in the `preprocess` method.  
  
* * *

###  imageFeatureExtractor.thumbnail(image, size, [resample]) ‚áí <code>
Promise. < RawImage > </code>

Resize the image to make a thumbnail. The image is resized so that no
dimension is larger than any corresponding dimension of the specified size.

**Kind** : instance method of `ImageFeatureExtractor`  
**Returns** : `Promise.<RawImage>` \- The resized image.

Param| Type| Default| Description  
---|---|---|---  
image| `RawImage`| | The image to be resized.  
size| `Object`| | The size `{"height": h, "width": w}` to resize the image to.  
[resample]| `string` | `0` | `1` | `2` | `3` | `4` | `5`| `2`| The resampling filter to use.  
  
* * *

###  imageFeatureExtractor.crop_margin(image, gray_threshold) ‚áí <code>
Promise. < RawImage > </code>

Crops the margin of the image. Gray pixels are considered margin (i.e., pixels
with a value below the threshold).

**Kind** : instance method of `ImageFeatureExtractor`  
**Returns** : `Promise.<RawImage>` \- The cropped image.

Param| Type| Default| Description  
---|---|---|---  
image| `RawImage`| | The image to be cropped.  
gray_threshold| `number`| `200`| Value below which pixels are considered to be
gray.  
  
* * *

###  imageFeatureExtractor.pad_image(pixelData, imgDims, padSize, options) ‚áí
<code> * </code>

Pad the image by a certain amount.

**Kind** : instance method of `ImageFeatureExtractor`  
**Returns** : `*` \- The padded pixel data and image dimensions.

Param| Type| Default| Description  
---|---|---|---  
pixelData| `Float32Array`| | The pixel data to pad.  
imgDims| `Array.<number>`| | The dimensions of the image (height, width, channels).  
padSize| `*`| | The dimensions of the padded image.  
options| `Object`| | The options for padding.  
[options.mode]| `'constant'` | `'symmetric'`| `'constant'`| The type of padding to add.  
[options.center]| `boolean`| `false`| Whether to center the image.  
[options.constant_values]| `number`| `0`| The constant value to use for
padding.  
  
* * *

###  imageFeatureExtractor.rescale(pixelData) ‚áí <code> void </code>

Rescale the image‚Äô pixel values by `this.rescale_factor`.

**Kind** : instance method of `ImageFeatureExtractor`

Param| Type| Description  
---|---|---  
pixelData| `Float32Array`| The pixel data to rescale.  
  
* * *

###  imageFeatureExtractor.get_resize_output_image_size(image, size) ‚áí <code>
* </code>

Find the target (width, height) dimension of the output image after resizing
given the input image and the desired size.

**Kind** : instance method of `ImageFeatureExtractor`  
**Returns** : `*` \- The target (width, height) dimension of the output image
after resizing.

Param| Type| Description  
---|---|---  
image| `RawImage`| The image to resize.  
size| `any`| The size to use for resizing the image.  
  
* * *

###  imageFeatureExtractor.resize(image) ‚áí <code> Promise. < RawImage >
</code>

Resizes the image.

**Kind** : instance method of `ImageFeatureExtractor`  
**Returns** : `Promise.<RawImage>` \- The resized image.

Param| Type| Description  
---|---|---  
image| `RawImage`| The image to resize.  
  
* * *

###  imageFeatureExtractor.preprocess(image, overrides) ‚áí <code> Promise. <
PreprocessedImage > </code>

Preprocesses the given image.

**Kind** : instance method of `ImageFeatureExtractor`  
**Returns** : `Promise.<PreprocessedImage>` \- The preprocessed image.

Param| Type| Description  
---|---|---  
image| `RawImage`| The image to preprocess.  
overrides| `Object`| The overrides for the preprocessing options.  
  
* * *

###  imageFeatureExtractor._call(images, ...args) ‚áí <code> Promise. <
ImageFeatureExtractorResult > </code>

Calls the feature extraction process on an array of images, preprocesses each
image, and concatenates the resulting features into a single Tensor.

**Kind** : instance method of `ImageFeatureExtractor`  
**Returns** : `Promise.<ImageFeatureExtractorResult>` \- An object containing
the concatenated pixel values (and other metadata) of the preprocessed images.

Param| Type| Description  
---|---|---  
images| `Array.<RawImage>`| The image(s) to extract features from.  
...args| `any`| Additional arguments.  
  
* * *

##  processors.DetrFeatureExtractor ‚áê <code> ImageFeatureExtractor </code>

Detr Feature Extractor.

**Kind** : static class of `processors`  
**Extends** : `ImageFeatureExtractor`

  * .DetrFeatureExtractor ‚áê `ImageFeatureExtractor`
    * `._call(images)` ‚áí `Promise.<DetrFeatureExtractorResult>`
    * `.post_process_object_detection()` : `*`
    * `.post_process_panoptic_segmentation()` : `*`

* * *

###  detrFeatureExtractor._call(images) ‚áí <code> Promise. <
DetrFeatureExtractorResult > </code>

Calls the feature extraction process on an array of images, preprocesses each
image, and concatenates the resulting features into a single Tensor.

**Kind** : instance method of `DetrFeatureExtractor`  
**Returns** : `Promise.<DetrFeatureExtractorResult>` \- An object containing
the concatenated pixel values of the preprocessed images.

Param| Type| Description  
---|---|---  
images| `Array.<RawImage>`| The image(s) to extract features from.  
  
* * *

###  detrFeatureExtractor.post_process_object_detection() : <code> * </code>

**Kind** : instance method of `DetrFeatureExtractor`

* * *

###  detrFeatureExtractor.post_process_panoptic_segmentation() : <code> *
</code>

**Kind** : instance method of `DetrFeatureExtractor`

* * *

##  processors.Processor ‚áê <code> Callable </code>

Represents a Processor that extracts features from an input.

**Kind** : static class of `processors`  
**Extends** : `Callable`

  * .Processor ‚áê `Callable`
    * `new Processor(feature_extractor)`
    * `._call(input, ...args)` ‚áí `Promise.<any>`

* * *

###  new Processor(feature_extractor)

Creates a new Processor with the given feature extractor.

Param| Type| Description  
---|---|---  
feature_extractor| `FeatureExtractor`| The function used to extract features
from the input.  
  
* * *

###  processor._call(input, ...args) ‚áí <code> Promise. < any > </code>

Calls the feature_extractor function with the given input.

**Kind** : instance method of `Processor`  
**Overrides** : `_call`  
**Returns** : `Promise.<any>` \- A Promise that resolves with the extracted
features.

Param| Type| Description  
---|---|---  
input| `any`| The input to extract features from.  
...args| `any`| Additional arguments.  
  
* * *

##  processors.WhisperProcessor ‚áê <code> Processor </code>

Represents a WhisperProcessor that extracts features from an audio input.

**Kind** : static class of `processors`  
**Extends** : `Processor`

* * *

###  whisperProcessor._call(audio) ‚áí <code> Promise. < any > </code>

Calls the feature_extractor function with the given audio input.

**Kind** : instance method of `WhisperProcessor`  
**Returns** : `Promise.<any>` \- A Promise that resolves with the extracted
features.

Param| Type| Description  
---|---|---  
audio| `any`| The audio input to extract features from.  
  
* * *

##  processors.AutoProcessor

Helper class which is used to instantiate pretrained processors with the
`from_pretrained` function. The chosen processor class is determined by the
type specified in the processor config.

**Example:** Load a processor using `from_pretrained`.

Copied

    
    
    let processor = await AutoProcessor.from_pretrained('openai/whisper-tiny.en');

**Example:** Run an image through a processor.

Copied

    
    
    let processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');
    let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/football-match.jpg');
    let image_inputs = await processor(image);
    // {
    //   "pixel_values": {
    //     "dims": [ 1, 3, 224, 224 ],
    //     "type": "float32",
    //     "data": Float32Array [ -1.558687686920166, -1.558687686920166, -1.5440893173217773, ... ],
    //     "size": 150528
    //   },
    //   "original_sizes": [
    //     [ 533, 800 ]
    //   ],
    //   "reshaped_input_sizes": [
    //     [ 224, 224 ]
    //   ]
    // }

**Kind** : static class of `processors`

* * *

###  AutoProcessor.from_pretrained(pretrained_model_name_or_path, options) ‚áí
<code> Promise. < Processor > </code>

Instantiate one of the processor classes of the library from a pretrained
model.

The processor class to instantiate is selected based on the
`feature_extractor_type` property of the config object (either passed as an
argument or loaded from `pretrained_model_name_or_path` if possible)

**Kind** : static method of `AutoProcessor`  
**Returns** : `Promise.<Processor>` \- A new instance of the Processor class.

Param| Type| Description  
---|---|---  
pretrained_model_name_or_path| `string`| The name or path of the pretrained
model. Can be either:

  * A string, the _model id_ of a pretrained processor hosted inside a model repo on huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  * A path to a _directory_ containing processor files, e.g., `./my_model_directory/`.

  
options| `*`| Additional options for loading the processor.  
  
* * *

##  processors.data : <code> Float32Array </code>

**Kind** : static property of `processors`

* * *

##  processors~center_to_corners_format(arr) ‚áí <code> Array. < number >
</code>

Converts bounding boxes from center format to corners format.

**Kind** : inner method of `processors`  
**Returns** : `Array.<number>` \- The coodinates for the top-left and bottom-
right corners of the box (top_left_x, top_left_y, bottom_right_x,
bottom_right_y)

Param| Type| Description  
---|---|---  
arr| `Array.<number>`| The coordinate for the center of the box and its width,
height dimensions (center_x, center_y, width, height)  
  
* * *

##  processors~post_process_semantic_segmentation(outputs, [target_sizes]) ‚áí
<code> * </code>

Post-processes the outputs of the model (for semantic segmentation).

**Kind** : inner method of `processors`  
**Returns** : `*` \- The semantic segmentation maps.

Param| Type| Default| Description  
---|---|---|---  
outputs| `*`| | Raw outputs of the model.  
[target_sizes]| `*`| ``| List of tuples corresponding to the requested final
size (height, width) of each prediction. If unset, predictions will not be
resized.  
  
* * *

###  post_process_semantic_segmentation~labels : <code> Array. < number >
</code>

The unique list of labels that were detected

**Kind** : inner constant of `post_process_semantic_segmentation`

* * *

##  processors~post_process_panoptic_segmentation(outputs, [threshold],
[mask_threshold], [overlap_mask_area_threshold], [label_ids_to_fuse],
[target_sizes]) ‚áí <code> Array. < {segmentation: Tensor, segments_info: Array
< {id: number, label_id: number, score: number} > } > </code>

Post-process the model output to generate the final panoptic segmentation.

**Kind** : inner method of `processors`

Param| Type| Default| Description  
---|---|---|---  
outputs| `*`| | The model output to post process  
[threshold]| `number`| `0.5`| The probability score threshold to keep
predicted instance masks.  
[mask_threshold]| `number`| `0.5`| Threshold to use when turning the predicted
masks into binary values.  
[overlap_mask_area_threshold]| `number`| `0.8`| The overlap mask area
threshold to merge or discard small disconnected parts within each binary
instance mask.  
[label_ids_to_fuse]| `Set.<number>`| ``| The labels in this state will have
all their instances be fused together.  
[target_sizes]| `*`| ``| The target sizes to resize the masks to.  
  
* * *

##  processors~post_process_instance_segmentation(outputs, [threshold],
[target_sizes]) ‚áí <code> Array. < {segmentation: Tensor, segments_info: Array
< {id: number, label_id: number, score: number} > } > </code>

Post-processes the outputs of the model (for instance segmentation).

**Kind** : inner method of `processors`

Param| Type| Default| Description  
---|---|---|---  
outputs| `*`| | Raw outputs of the model.  
[threshold]| `number`| `0.5`| The probability score threshold to keep
predicted instance masks.  
[target_sizes]| `*`| ``| List of tuples corresponding to the requested final
size (height, width) of each prediction. If unset, predictions will not be
resized.  
  
* * *

##  processors~enforce_size_divisibility(size, divisor) ‚áí <code> * </code>

Rounds the height and width down to the closest multiple of size_divisibility

**Kind** : inner method of `processors`  
**Returns** : `*` \- The rounded size.

Param| Type| Description  
---|---|---  
size| `*`| The size of the image  
divisor| `number`| The divisor to use.  
  
* * *

##  processors~HeightWidth : <code> * </code>

Named tuple to indicate the order we are using is (height x width), even
though the Graphics‚Äô industry standard is (width x height).

**Kind** : inner typedef of `processors`

* * *

##  processors~ImageFeatureExtractorResult : <code> object </code>

**Kind** : inner typedef of `processors`  
**Properties**

Name| Type| Description  
---|---|---  
pixel_values| `Tensor`| The pixel values of the batched preprocessed images.  
original_sizes| `Array.<HeightWidth>`| Array of two-dimensional tuples like
[[480, 640]].  
reshaped_input_sizes| `Array.<HeightWidth>`| Array of two-dimensional tuples
like [[1000, 1330]].  
  
* * *

##  processors~PreprocessedImage : <code> object </code>

**Kind** : inner typedef of `processors`  
**Properties**

Name| Type| Description  
---|---|---  
original_size| `HeightWidth`| The original size of the image.  
reshaped_input_size| `HeightWidth`| The reshaped input size of the image.  
pixel_values| `Tensor`| The pixel values of the preprocessed image.  
  
* * *

##  processors~DetrFeatureExtractorResult : <code> object </code>

**Kind** : inner typedef of `processors`  
**Properties**

Name| Type  
---|---  
pixel_mask| `Tensor`  
  
* * *

##  processors~SamImageProcessorResult : <code> object </code>

**Kind** : inner typedef of `processors`  
**Properties**

Name| Type  
---|---  
pixel_values| `Tensor`  
original_sizes| `Array.<HeightWidth>`  
reshaped_input_sizes| `Array.<HeightWidth>`  
[input_points]| `Tensor`  
[input_labels]| `Tensor`  
[input_boxes]| `Tensor`  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/processors.md)

[‚ÜêTokenizers](/docs/transformers.js/v3.0.0/en/api/tokenizers)
[Configs‚Üí](/docs/transformers.js/v3.0.0/en/api/configs)

processors processors.FeatureExtractor ‚áê ` Callable ` new
FeatureExtractor(config) featureExtractor._call(...args)
processors.ImageFeatureExtractor ‚áê ` FeatureExtractor ` new
ImageFeatureExtractor(config) imageFeatureExtractor.thumbnail(image, size,
[resample]) ‚áí ` Promise. < RawImage > `
imageFeatureExtractor.crop_margin(image, gray_threshold) ‚áí ` Promise. <
RawImage > ` imageFeatureExtractor.pad_image(pixelData, imgDims, padSize,
options) ‚áí ` * ` imageFeatureExtractor.rescale(pixelData) ‚áí ` void `
imageFeatureExtractor.get_resize_output_image_size(image, size) ‚áí ` * `
imageFeatureExtractor.resize(image) ‚áí ` Promise. < RawImage > `
imageFeatureExtractor.preprocess(image, overrides) ‚áí ` Promise. <
PreprocessedImage > ` imageFeatureExtractor._call(images, ...args) ‚áí `
Promise. < ImageFeatureExtractorResult > ` processors.DetrFeatureExtractor ‚áê `
ImageFeatureExtractor ` detrFeatureExtractor._call(images) ‚áí ` Promise. <
DetrFeatureExtractorResult > `
detrFeatureExtractor.post_process_object_detection() : ` * `
detrFeatureExtractor.post_process_panoptic_segmentation() : ` * `
processors.Processor ‚áê ` Callable ` new Processor(feature_extractor)
processor._call(input, ...args) ‚áí ` Promise. < any > `
processors.WhisperProcessor ‚áê ` Processor ` whisperProcessor._call(audio) ‚áí `
Promise. < any > ` processors.AutoProcessor
AutoProcessor.from_pretrained(pretrained_model_name_or_path, options) ‚áí `
Promise. < Processor > ` processors.data : ` Float32Array `
processors~center_to_corners_format(arr) ‚áí ` Array. < number > `
processors~post_process_semantic_segmentation(outputs, [target_sizes]) ‚áí ` * `
post_process_semantic_segmentation~labels : ` Array. < number > `
processors~post_process_panoptic_segmentation(outputs, [threshold],
[mask_threshold], [overlap_mask_area_threshold], [label_ids_to_fuse],
[target_sizes]) ‚áí ` Array. < {segmentation: Tensor, segments_info: Array <
{id: number, label_id: number, score: number} > } > `
processors~post_process_instance_segmentation(outputs, [threshold],
[target_sizes]) ‚áí ` Array. < {segmentation: Tensor, segments_info: Array <
{id: number, label_id: number, score: number} > } > `
processors~enforce_size_divisibility(size, divisor) ‚áí ` * `
processors~HeightWidth : ` * ` processors~ImageFeatureExtractorResult : `
object ` processors~PreprocessedImage : ` object `
processors~DetrFeatureExtractorResult : ` object `
processors~SamImageProcessorResult : ` object `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

tokenizers

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  tokenizers

Tokenizers are used to prepare textual inputs for a model.

**Example:** Create an `AutoTokenizer` and use it to tokenize a sentence. This
will automatically detect the tokenizer type based on the tokenizer class
defined in `tokenizer.json`.

Copied

    
    
    import { AutoTokenizer } from '@huggingface/transformers';
    
    const tokenizer = await AutoTokenizer.from_pretrained('Xenova/bert-base-uncased');
    const { input_ids } = await tokenizer('I love transformers!');
    // Tensor {
    //   data: BigInt64Array(6) [101n, 1045n, 2293n, 19081n, 999n, 102n],
    //   dims: [1, 6],
    //   type: 'int64',
    //   size: 6,
    // }

  * tokenizers
    *  _static_
      * .TokenizerModel ‚áê `Callable`
        * `new TokenizerModel(config)`
        * _instance_
          * `.vocab` : `Array.<string>`
          * `.tokens_to_ids` : `Map.<string, number>`
          * `.fuse_unk` : `boolean`
          * `._call(tokens)` ‚áí `Array.<string>`
          * `.encode(tokens)` ‚áí `Array.<string>`
          * `.convert_tokens_to_ids(tokens)` ‚áí `Array.<number>`
          * `.convert_ids_to_tokens(ids)` ‚áí `Array.<string>`
        * _static_
          * `.fromConfig(config, ...args)` ‚áí `TokenizerModel`
      * .PreTrainedTokenizer
        * `new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)`
        * _instance_
          * `.added_tokens` : `Array.<AddedToken>`
          * `.remove_space` : `boolean`
          * `._call(text, options)` ‚áí `BatchEncoding`
          * `._encode_text(text)` ‚áí `Array<string>` | `null`
          * `._tokenize_helper(text, options)` ‚áí `*`
          * `.tokenize(text, options)` ‚áí `Array.<string>`
          * `.encode(text, options)` ‚áí `Array.<number>`
          * `.batch_decode(batch, decode_args)` ‚áí `Array.<string>`
          * `.decode(token_ids, [decode_args])` ‚áí `string`
          * `.decode_single(token_ids, decode_args)` ‚áí `string`
          * `.get_chat_template(options)` ‚áí `string`
          * `.apply_chat_template(conversation, options)` ‚áí `string` | `Tensor` | `Array<number>` | `Array<Array<number>>` | `BatchEncoding`
        * _static_
          * `.from_pretrained(pretrained_model_name_or_path, options)` ‚áí `Promise.<PreTrainedTokenizer>`
      * .BertTokenizer ‚áê `PreTrainedTokenizer`
      * .AlbertTokenizer ‚áê `PreTrainedTokenizer`
      * .NllbTokenizer
        * `._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)` ‚áí `Object`
      * .M2M100Tokenizer
        * `._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs)` ‚áí `Object`
      * .WhisperTokenizer ‚áê `PreTrainedTokenizer`
        * `._decode_asr(sequences, options)` ‚áí `*`
        * `.decode()` : `*`
      * .MarianTokenizer
        * `new MarianTokenizer(tokenizerJSON, tokenizerConfig)`
        * `._encode_text(text)` ‚áí `Array`
      * .AutoTokenizer
        * `.from_pretrained(pretrained_model_name_or_path, options)` ‚áí `Promise.<PreTrainedTokenizer>`
      * `.is_chinese_char(cp)` ‚áí `boolean`
    * _inner_
      * ~AddedToken
        * `new AddedToken(config)`
      * ~WordPieceTokenizer ‚áê `TokenizerModel`
        * `new WordPieceTokenizer(config)`
        * `.tokens_to_ids` : `Map.<string, number>`
        * `.unk_token_id` : `number`
        * `.unk_token` : `string`
        * `.max_input_chars_per_word` : `number`
        * `.vocab` : `Array.<string>`
        * `.encode(tokens)` ‚áí `Array.<string>`
      * ~Unigram ‚áê `TokenizerModel`
        * `new Unigram(config, moreConfig)`
        * `.populateNodes(lattice)`
        * `.tokenize(normalized)` ‚áí `Array.<string>`
        * `.encode(tokens)` ‚áí `Array.<string>`
      * ~BPE ‚áê `TokenizerModel`
        * `new BPE(config)`
        * `.tokens_to_ids` : `Map.<string, number>`
        * `.merges` : `*`
          * `.config.merges` : `*`
        * `.cache` : `Map.<string, Array<string>>`
        * `.bpe(token)` ‚áí `Array.<string>`
        * `.encode(tokens)` ‚áí `Array.<string>`
      * ~LegacyTokenizerModel
        * `new LegacyTokenizerModel(config, moreConfig)`
        * `.tokens_to_ids` : `Map.<string, number>`
      * _~Normalizer_
        *  _`new Normalizer(config)`_
        * _instance_
          * **`.normalize(text)` ‚áí `string`**
          * _`._call(text)` ‚áí `string`_
        * _static_
          *  _`.fromConfig(config)` ‚áí `Normalizer`_
      * ~Replace ‚áê `Normalizer`
        * `.normalize(text)` ‚áí `string`
      * ~NFC ‚áê `Normalizer`
        * `.normalize(text)` ‚áí `string`
      * ~NFKC ‚áê `Normalizer`
        * `.normalize(text)` ‚áí `string`
      * ~NFKD ‚áê `Normalizer`
        * `.normalize(text)` ‚áí `string`
      * ~StripNormalizer
        * `.normalize(text)` ‚áí `string`
      * ~StripAccents ‚áê `Normalizer`
        * `.normalize(text)` ‚áí `string`
      * ~Lowercase ‚áê `Normalizer`
        * `.normalize(text)` ‚áí `string`
      * ~Prepend ‚áê `Normalizer`
        * `.normalize(text)` ‚áí `string`
      * ~NormalizerSequence ‚áê `Normalizer`
        * `new NormalizerSequence(config)`
        * `.normalize(text)` ‚áí `string`
      * ~BertNormalizer ‚áê `Normalizer`
        * `._tokenize_chinese_chars(text)` ‚áí `string`
        * `.stripAccents(text)` ‚áí `string`
        * `.normalize(text)` ‚áí `string`
      * ~PreTokenizer ‚áê `Callable`
        *  _instance_
          *  _`.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`_
          * `.pre_tokenize(text, [options])` ‚áí `Array.<string>`
          * `._call(text, [options])` ‚áí `Array.<string>`
        * _static_
          * `.fromConfig(config)` ‚áí `PreTokenizer`
      * ~BertPreTokenizer ‚áê `PreTokenizer`
        * `new BertPreTokenizer(config)`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * ~ByteLevelPreTokenizer ‚áê `PreTokenizer`
        * `new ByteLevelPreTokenizer(config)`
        * `.add_prefix_space` : `boolean`
        * `.trim_offsets` : `boolean`
        * `.use_regex` : `boolean`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * ~SplitPreTokenizer ‚áê `PreTokenizer`
        * `new SplitPreTokenizer(config)`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * ~PunctuationPreTokenizer ‚áê `PreTokenizer`
        * `new PunctuationPreTokenizer(config)`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * ~DigitsPreTokenizer ‚áê `PreTokenizer`
        * `new DigitsPreTokenizer(config)`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * ~PostProcessor ‚áê `Callable`
        * `new PostProcessor(config)`
        * _instance_
          * `.post_process(tokens, ...args)` ‚áí `PostProcessedOutput`
          * `._call(tokens, ...args)` ‚áí `PostProcessedOutput`
        * _static_
          * `.fromConfig(config)` ‚áí `PostProcessor`
      * ~BertProcessing
        * `new BertProcessing(config)`
        * `.post_process(tokens, [tokens_pair])` ‚áí `PostProcessedOutput`
      * ~TemplateProcessing ‚áê `PostProcessor`
        * `new TemplateProcessing(config)`
        * `.post_process(tokens, [tokens_pair])` ‚áí `PostProcessedOutput`
      * ~ByteLevelPostProcessor ‚áê `PostProcessor`
        * `.post_process(tokens, [tokens_pair])` ‚áí `PostProcessedOutput`
      * ~PostProcessorSequence
        * `new PostProcessorSequence(config)`
        * `.post_process(tokens, [tokens_pair])` ‚áí `PostProcessedOutput`
      * ~Decoder ‚áê `Callable`
        * `new Decoder(config)`
        * _instance_
          * `.added_tokens` : `Array.<AddedToken>`
          * `._call(tokens)` ‚áí `string`
          * `.decode(tokens)` ‚áí `string`
          * `.decode_chain(tokens)` ‚áí `Array.<string>`
        * _static_
          * `.fromConfig(config)` ‚áí `Decoder`
      * ~FuseDecoder
        * `.decode_chain()` : `*`
      * ~WordPieceDecoder ‚áê `Decoder`
        * `new WordPieceDecoder(config)`
        * `.decode_chain()` : `*`
      * ~ByteLevelDecoder ‚áê `Decoder`
        * `new ByteLevelDecoder(config)`
        * `.convert_tokens_to_string(tokens)` ‚áí `string`
        * `.decode_chain()` : `*`
      * ~CTCDecoder
        * `.convert_tokens_to_string(tokens)` ‚áí `string`
        * `.decode_chain()` : `*`
      * ~DecoderSequence ‚áê `Decoder`
        * `new DecoderSequence(config)`
        * `.decode_chain()` : `*`
      * ~MetaspacePreTokenizer ‚áê `PreTokenizer`
        * `new MetaspacePreTokenizer(config)`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * ~MetaspaceDecoder ‚áê `Decoder`
        * `new MetaspaceDecoder(config)`
        * `.decode_chain()` : `*`
      * ~Precompiled ‚áê `Normalizer`
        * `new Precompiled(config)`
        * `.normalize(text)` ‚áí `string`
      * ~PreTokenizerSequence ‚áê `PreTokenizer`
        * `new PreTokenizerSequence(config)`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * ~WhitespacePreTokenizer
        * `new WhitespacePreTokenizer(config)`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * ~WhitespaceSplit ‚áê `PreTokenizer`
        * `new WhitespaceSplit(config)`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * ~ReplacePreTokenizer
        * `new ReplacePreTokenizer(config)`
        * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`
      * `~BYTES_TO_UNICODE` ‚áí `Object`
      * `~loadTokenizer(pretrained_model_name_or_path, options)` ‚áí `Promise.<Array<any>>`
      * `~regexSplit(text, regex)` ‚áí `Array.<string>`
      * `~createPattern(pattern, invert)` ‚áí `RegExp` | `null`
      * `~objectToMap(obj)` ‚áí `Map.<string, any>`
      * `~prepareTensorForDecode(tensor)` ‚áí `Array.<number>`
      * `~clean_up_tokenization(text)` ‚áí `string`
      * `~remove_accents(text)` ‚áí `string`
      * `~lowercase_and_remove_accent(text)` ‚áí `string`
      * `~whitespace_split(text)` ‚áí `Array.<string>`
      * `~PretrainedTokenizerOptions` : `Object`
      * `~BPENode` : `Object`
      * `~SplitDelimiterBehavior` : `‚Äôremoved‚Äô` | `‚Äôisolated‚Äô` | `‚ÄômergedWithPrevious‚Äô` | `‚ÄômergedWithNext‚Äô` | `‚Äôcontiguous‚Äô`
      * `~PostProcessedOutput` : `Object`
      * `~EncodingSingle` : `Object`
      * `~Message` : `Object`
      * `~BatchEncoding` : `Array<number>` | `Array<Array<number>>` | `Tensor`

* * *

##  tokenizers.TokenizerModel ‚áê <code> Callable </code>

Abstract base class for tokenizer models.

**Kind** : static class of `tokenizers`  
**Extends** : `Callable`

  * .TokenizerModel ‚áê `Callable`
    * `new TokenizerModel(config)`
    * _instance_
      * `.vocab` : `Array.<string>`
      * `.tokens_to_ids` : `Map.<string, number>`
      * `.fuse_unk` : `boolean`
      * `._call(tokens)` ‚áí `Array.<string>`
      * `.encode(tokens)` ‚áí `Array.<string>`
      * `.convert_tokens_to_ids(tokens)` ‚áí `Array.<number>`
      * `.convert_ids_to_tokens(ids)` ‚áí `Array.<string>`
    * _static_
      * `.fromConfig(config, ...args)` ‚áí `TokenizerModel`

* * *

###  new TokenizerModel(config)

Creates a new instance of TokenizerModel.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the TokenizerModel.  
  
* * *

###  tokenizerModel.vocab : <code> Array. < string > </code>

**Kind** : instance property of `TokenizerModel`

* * *

###  tokenizerModel.tokens_to_ids : <code> Map. < string, number > </code>

A mapping of tokens to ids.

**Kind** : instance property of `TokenizerModel`

* * *

###  tokenizerModel.fuse_unk : <code> boolean </code>

Whether to fuse unknown tokens when encoding. Defaults to false.

**Kind** : instance property of `TokenizerModel`

* * *

###  tokenizerModel._call(tokens) ‚áí <code> Array. < string > </code>

Internal function to call the TokenizerModel instance.

**Kind** : instance method of `TokenizerModel`  
**Overrides** : `_call`  
**Returns** : `Array.<string>` \- The encoded tokens.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| The tokens to encode.  
  
* * *

###  tokenizerModel.encode(tokens) ‚áí <code> Array. < string > </code>

Encodes a list of tokens into a list of token IDs.

**Kind** : instance method of `TokenizerModel`  
**Returns** : `Array.<string>` \- The encoded tokens.  
**Throws** :

  * Will throw an error if not implemented in a subclass.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| The tokens to encode.  
  
* * *

###  tokenizerModel.convert_tokens_to_ids(tokens) ‚áí <code> Array. < number >
</code>

Converts a list of tokens into a list of token IDs.

**Kind** : instance method of `TokenizerModel`  
**Returns** : `Array.<number>` \- The converted token IDs.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| The tokens to convert.  
  
* * *

###  tokenizerModel.convert_ids_to_tokens(ids) ‚áí <code> Array. < string >
</code>

Converts a list of token IDs into a list of tokens.

**Kind** : instance method of `TokenizerModel`  
**Returns** : `Array.<string>` \- The converted tokens.

Param| Type| Description  
---|---|---  
ids| `Array<number>` | `Array<bigint>`| The token IDs to convert.  
  
* * *

###  TokenizerModel.fromConfig(config, ...args) ‚áí <code> TokenizerModel
</code>

Instantiates a new TokenizerModel instance based on the configuration object
provided.

**Kind** : static method of `TokenizerModel`  
**Returns** : `TokenizerModel` \- A new instance of a TokenizerModel.  
**Throws** :

  * Will throw an error if the TokenizerModel type in the config is not recognized.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the TokenizerModel.  
...args| `*`| Optional arguments to pass to the specific TokenizerModel
constructor.  
  
* * *

##  tokenizers.PreTrainedTokenizer

**Kind** : static class of `tokenizers`

  * .PreTrainedTokenizer
    * `new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)`
    * _instance_
      * `.added_tokens` : `Array.<AddedToken>`
      * `.remove_space` : `boolean`
      * `._call(text, options)` ‚áí `BatchEncoding`
      * `._encode_text(text)` ‚áí `Array<string>` | `null`
      * `._tokenize_helper(text, options)` ‚áí `*`
      * `.tokenize(text, options)` ‚áí `Array.<string>`
      * `.encode(text, options)` ‚áí `Array.<number>`
      * `.batch_decode(batch, decode_args)` ‚áí `Array.<string>`
      * `.decode(token_ids, [decode_args])` ‚áí `string`
      * `.decode_single(token_ids, decode_args)` ‚áí `string`
      * `.get_chat_template(options)` ‚áí `string`
      * `.apply_chat_template(conversation, options)` ‚áí `string` | `Tensor` | `Array<number>` | `Array<Array<number>>` | `BatchEncoding`
    * _static_
      * `.from_pretrained(pretrained_model_name_or_path, options)` ‚áí `Promise.<PreTrainedTokenizer>`

* * *

###  new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig)

Create a new PreTrainedTokenizer instance.

Param| Type| Description  
---|---|---  
tokenizerJSON| `Object`| The JSON of the tokenizer.  
tokenizerConfig| `Object`| The config of the tokenizer.  
  
* * *

###  preTrainedTokenizer.added_tokens : <code> Array. < AddedToken > </code>

**Kind** : instance property of `PreTrainedTokenizer`

* * *

###  preTrainedTokenizer.remove_space : <code> boolean </code>

Whether or not to strip the text when tokenizing (removing excess spaces
before and after the string).

**Kind** : instance property of `PreTrainedTokenizer`

* * *

###  preTrainedTokenizer._call(text, options) ‚áí <code> BatchEncoding </code>

Encode/tokenize the given text(s).

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `BatchEncoding` \- Object to be passed to the model.

Param| Type| Default| Description  
---|---|---|---  
text| `string` | `Array<string>`| | The text to tokenize.  
options| `Object`| | An optional object containing the following properties:  
[options.text_pair]| `string` | `Array<string>`| `null`| Optional second sequence to be encoded. If set, must be the same type as text.  
[options.padding]| `boolean` | `'max_length'`| `false`| Whether to pad the input sequences.  
[options.add_special_tokens]| `boolean`| `true`| Whether or not to add the
special tokens associated with the corresponding model.  
[options.truncation]| `boolean`| ``| Whether to truncate the input sequences.  
[options.max_length]| `number`| ``| Maximum length of the returned list and
optionally padding length.  
[options.return_tensor]| `boolean`| `true`| Whether to return the results as
Tensors or arrays.  
[options.return_token_type_ids]| `boolean`| ``| Whether to return the token
type ids.  
  
* * *

###  preTrainedTokenizer._encode_text(text) ‚áí <code> Array < string > </code> | <code> null </code>

Encodes a single text using the preprocessor pipeline of the tokenizer.

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `Array<string>` | `null` \- The encoded tokens.

Param| Type| Description  
---|---|---  
text| `string` | `null`| The text to encode.  
  
* * *

###  preTrainedTokenizer._tokenize_helper(text, options) ‚áí <code> * </code>

Internal helper function to tokenize a text, and optionally a pair of texts.

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `*` \- An object containing the tokens and optionally the token
type IDs.

Param| Type| Default| Description  
---|---|---|---  
text| `string`| | The text to tokenize.  
options| `Object`| | An optional object containing the following properties:  
[options.pair]| `string`| `null`| The optional second text to tokenize.  
[options.add_special_tokens]| `boolean`| `false`| Whether or not to add the
special tokens associated with the corresponding model.  
  
* * *

###  preTrainedTokenizer.tokenize(text, options) ‚áí <code> Array. < string >
</code>

Converts a string into a sequence of tokens.

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `Array.<string>` \- The list of tokens.

Param| Type| Default| Description  
---|---|---|---  
text| `string`| | The sequence to be encoded.  
options| `Object`| | An optional object containing the following properties:  
[options.pair]| `string`| | A second sequence to be encoded with the first.  
[options.add_special_tokens]| `boolean`| `false`| Whether or not to add the
special tokens associated with the corresponding model.  
  
* * *

###  preTrainedTokenizer.encode(text, options) ‚áí <code> Array. < number >
</code>

Encodes a single text or a pair of texts using the model‚Äôs tokenizer.

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `Array.<number>` \- An array of token IDs representing the
encoded text(s).

Param| Type| Default| Description  
---|---|---|---  
text| `string`| | The text to encode.  
options| `Object`| | An optional object containing the following properties:  
[options.text_pair]| `string`| `null`| The optional second text to encode.  
[options.add_special_tokens]| `boolean`| `true`| Whether or not to add the
special tokens associated with the corresponding model.  
[options.return_token_type_ids]| `boolean`| ``| Whether to return
token_type_ids.  
  
* * *

###  preTrainedTokenizer.batch_decode(batch, decode_args) ‚áí <code> Array. <
string > </code>

Decode a batch of tokenized sequences.

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `Array.<string>` \- List of decoded sequences.

Param| Type| Description  
---|---|---  
batch| `Array<Array<number>>` | `Tensor`| List/Tensor of tokenized input sequences.  
decode_args| `Object`| (Optional) Object with decoding arguments.  
  
* * *

###  preTrainedTokenizer.decode(token_ids, [decode_args]) ‚áí <code> string
</code>

Decodes a sequence of token IDs back to a string.

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `string` \- The decoded string.  
**Throws** :

  * `Error` If `token_ids` is not a non-empty array of integers.

Param| Type| Default| Description  
---|---|---|---  
token_ids| `Array<number>` | `Array<bigint>` | `Tensor`| | List/Tensor of token IDs to decode.  
[decode_args]| `Object`| `{}`|  
[decode_args.skip_special_tokens]| `boolean`| `false`| If true, special tokens
are removed from the output string.  
[decode_args.clean_up_tokenization_spaces]| `boolean`| `true`| If true, spaces
before punctuations and abbreviated forms are removed.  
  
* * *

###  preTrainedTokenizer.decode_single(token_ids, decode_args) ‚áí <code> string
</code>

Decode a single list of token ids to a string.

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `string` \- The decoded string

Param| Type| Default| Description  
---|---|---|---  
token_ids| `Array<number>` | `Array<bigint>`| | List of token ids to decode  
decode_args| `Object`| | Optional arguments for decoding  
[decode_args.skip_special_tokens]| `boolean`| `false`| Whether to skip special
tokens during decoding  
[decode_args.clean_up_tokenization_spaces]| `boolean`| ``| Whether to clean up
tokenization spaces during decoding. If null, the value is set to
`this.decoder.cleanup` if it exists, falling back to
`this.clean_up_tokenization_spaces` if it exists, falling back to `true`.  
  
* * *

###  preTrainedTokenizer.get_chat_template(options) ‚áí <code> string </code>

Retrieve the chat template string used for tokenizing chat messages. This
template is used internally by the `apply_chat_template` method and can also
be used externally to retrieve the model‚Äôs chat template for better generation
tracking.

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `string` \- The chat template string.

Param| Type| Default| Description  
---|---|---|---  
options| `Object`| | An optional object containing the following properties:  
[options.chat_template]| `string`| `null`| A Jinja template or the name of a
template to use for this conversion. It is usually not necessary to pass
anything to this argument, as the model's template will be used by default.  
[options.tools]| `Array.<Object>`| ``| A list of tools (callable functions)
that will be accessible to the model. If the template does not support
function calling, this argument will have no effect. Each tool should be
passed as a JSON Schema, giving the name, description and argument types for
the tool. See our [chat templating
guide](https://huggingface.co/docs/transformers/v3.0.0/en/chat_templating#automated-
function-conversion-for-tool-use) for more information.  
  
* * *

###  preTrainedTokenizer.apply_chat_template(conversation, options) ‚áí <code> string </code> | <code> Tensor </code> | <code> Array < number > </code> | <code> Array < Array < number > > </code> | <code> BatchEncoding </code>

Converts a list of message objects with `"role"` and `"content"` keys to a
list of token ids. This method is intended for use with chat models, and will
read the tokenizer‚Äôs chat_template attribute to determine the format and
control tokens to use when converting.

See [here](https://huggingface.co/docs/transformers/chat_templating) for more
information.

**Example:** Applying a chat template to a conversation.

Copied

    
    
    import { AutoTokenizer } from "@huggingface/transformers";
    
    const tokenizer = await AutoTokenizer.from_pretrained("Xenova/mistral-tokenizer-v1");
    
    const chat = [
      { "role": "user", "content": "Hello, how are you?" },
      { "role": "assistant", "content": "I'm doing great. How can I help you today?" },
      { "role": "user", "content": "I'd like to show off how chat templating works!" },
    ]
    
    const text = tokenizer.apply_chat_template(chat, { tokenize: false });
    // "<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]"
    
    const input_ids = tokenizer.apply_chat_template(chat, { tokenize: true, return_tensor: false });
    // [1, 733, 16289, 28793, 22557, 28725, 910, 460, 368, 28804, 733, 28748, 16289, 28793, 28737, 28742, 28719, 2548, 1598, 28723, 1602, 541, 315, 1316, 368, 3154, 28804, 2, 28705, 733, 16289, 28793, 315, 28742, 28715, 737, 298, 1347, 805, 910, 10706, 5752, 1077, 3791, 28808, 733, 28748, 16289, 28793]

**Kind** : instance method of `PreTrainedTokenizer`  
**Returns** : `string` | `Tensor` | `Array<number>` | `Array<Array<number>>` | `BatchEncoding` \- The tokenized output.

Param| Type| Default| Description  
---|---|---|---  
conversation| `Array.<Message>`| | A list of message objects with `"role"` and `"content"` keys, representing the chat history so far.  
options| `Object`| | An optional object containing the following properties:  
[options.chat_template]| `string`| `null`| A Jinja template to use for this
conversion. If this is not passed, the model's chat template will be used
instead.  
[options.tools]| `Array.<Object>`| ``| A list of tools (callable functions)
that will be accessible to the model. If the template does not support
function calling, this argument will have no effect. Each tool should be
passed as a JSON Schema, giving the name, description and argument types for
the tool. See our [chat templating
guide](https://huggingface.co/docs/transformers/v3.0.0/en/chat_templating#automated-
function-conversion-for-tool-use) for more information.  
[options.documents]| `*`| ``| A list of dicts representing documents that will
be accessible to the model if it is performing RAG (retrieval-augmented
generation). If the template does not support RAG, this argument will have no
effect. We recommend that each document should be a dict containing "title"
and "text" keys. Please see the RAG section of the [chat templating
guide](https://huggingface.co/docs/transformers/v3.0.0/en/chat_templating#arguments-
for-RAG) for examples of passing documents with chat templates.  
[options.add_generation_prompt]| `boolean`| `false`| Whether to end the prompt
with the token(s) that indicate the start of an assistant message. This is
useful when you want to generate a response from the model. Note that this
argument will be passed to the chat template, and so it must be supported in
the template for this argument to have any effect.  
[options.tokenize]| `boolean`| `true`| Whether to tokenize the output. If
false, the output will be a string.  
[options.padding]| `boolean`| `false`| Whether to pad sequences to the maximum
length. Has no effect if tokenize is false.  
[options.truncation]| `boolean`| `false`| Whether to truncate sequences to the
maximum length. Has no effect if tokenize is false.  
[options.max_length]| `number`| ``| Maximum length (in tokens) to use for
padding or truncation. Has no effect if tokenize is false. If not specified,
the tokenizer's `max_length` attribute will be used as a default.  
[options.return_tensor]| `boolean`| `true`| Whether to return the output as a
Tensor or an Array. Has no effect if tokenize is false.  
[options.return_dict]| `boolean`| `true`| Whether to return a dictionary with
named outputs. Has no effect if tokenize is false.  
[options.tokenizer_kwargs]| `Object`| `{}`| Additional options to pass to the
tokenizer.  
  
* * *

###  PreTrainedTokenizer.from_pretrained(pretrained_model_name_or_path,
options) ‚áí <code> Promise. < PreTrainedTokenizer > </code>

Loads a pre-trained tokenizer from the given `pretrained_model_name_or_path`.

**Kind** : static method of `PreTrainedTokenizer`  
**Returns** : `Promise.<PreTrainedTokenizer>` \- A new instance of the
`PreTrainedTokenizer` class.  
**Throws** :

  * `Error` Throws an error if the tokenizer.json or tokenizer_config.json files are not found in the `pretrained_model_name_or_path`.

Param| Type| Description  
---|---|---  
pretrained_model_name_or_path| `string`| The path to the pre-trained
tokenizer.  
options| `PretrainedTokenizerOptions`| Additional options for loading the
tokenizer.  
  
* * *

##  tokenizers.BertTokenizer ‚áê <code> PreTrainedTokenizer </code>

BertTokenizer is a class used to tokenize text for BERT models.

**Kind** : static class of `tokenizers`  
**Extends** : `PreTrainedTokenizer`

* * *

##  tokenizers.AlbertTokenizer ‚áê <code> PreTrainedTokenizer </code>

Albert tokenizer

**Kind** : static class of `tokenizers`  
**Extends** : `PreTrainedTokenizer`

* * *

##  tokenizers.NllbTokenizer

The NllbTokenizer class is used to tokenize text for NLLB (‚ÄúNo Language Left
Behind‚Äù) models.

No Language Left Behind (NLLB) is a first-of-its-kind, AI breakthrough project
that open-sources models capable of delivering high-quality translations
directly between any pair of 200+ languages ‚Äî including low-resource languages
like Asturian, Luganda, Urdu and more. It aims to help people communicate with
anyone, anywhere, regardless of their language preferences. For more
information, check out their [paper](https://arxiv.org/abs/2207.04672).

For a list of supported languages (along with their language codes),

**Kind** : static class of `tokenizers`  
**See** :
<https://github.com/facebookresearch/flores/blob/v3.0.0/flores200/README.md#languages-
in-flores-200>

* * *

###  nllbTokenizer._build_translation_inputs(raw_inputs, tokenizer_options,
generate_kwargs) ‚áí <code> Object </code>

Helper function to build translation inputs for an `NllbTokenizer`.

**Kind** : instance method of `NllbTokenizer`  
**Returns** : `Object` \- Object to be passed to the model.

Param| Type| Description  
---|---|---  
raw_inputs| `string` | `Array<string>`| The text to tokenize.  
tokenizer_options| `Object`| Options to be sent to the tokenizer  
generate_kwargs| `Object`| Generation options.  
  
* * *

##  tokenizers.M2M100Tokenizer

The M2M100Tokenizer class is used to tokenize text for M2M100 (‚ÄúMany-to-Many‚Äù)
models.

M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-
to-Many multilingual translation. It was introduced in this
[paper](https://arxiv.org/abs/2010.11125) and first released in
[this](https://github.com/pytorch/fairseq/tree/master/examples/m2m_100)
repository.

For a list of supported languages (along with their language codes),

**Kind** : static class of `tokenizers`  
**See** : <https://huggingface.co/facebook/m2m100_418M#languages-covered>

* * *

###  m2M100Tokenizer._build_translation_inputs(raw_inputs, tokenizer_options,
generate_kwargs) ‚áí <code> Object </code>

Helper function to build translation inputs for an `M2M100Tokenizer`.

**Kind** : instance method of `M2M100Tokenizer`  
**Returns** : `Object` \- Object to be passed to the model.

Param| Type| Description  
---|---|---  
raw_inputs| `string` | `Array<string>`| The text to tokenize.  
tokenizer_options| `Object`| Options to be sent to the tokenizer  
generate_kwargs| `Object`| Generation options.  
  
* * *

##  tokenizers.WhisperTokenizer ‚áê <code> PreTrainedTokenizer </code>

WhisperTokenizer tokenizer

**Kind** : static class of `tokenizers`  
**Extends** : `PreTrainedTokenizer`

  * .WhisperTokenizer ‚áê `PreTrainedTokenizer`
    * `._decode_asr(sequences, options)` ‚áí `*`
    * `.decode()` : `*`

* * *

###  whisperTokenizer._decode_asr(sequences, options) ‚áí <code> * </code>

Decodes automatic speech recognition (ASR) sequences.

**Kind** : instance method of `WhisperTokenizer`  
**Returns** : `*` \- The decoded sequences.

Param| Type| Description  
---|---|---  
sequences| `*`| The sequences to decode.  
options| `Object`| The options to use for decoding.  
  
* * *

###  whisperTokenizer.decode() : <code> * </code>

**Kind** : instance method of `WhisperTokenizer`

* * *

##  tokenizers.MarianTokenizer

**Kind** : static class of `tokenizers`  
**Todo**

  * This model is not yet supported by Hugging Face‚Äôs ‚Äúfast‚Äù tokenizers library (<https://github.com/huggingface/tokenizers>). Therefore, this implementation (which is based on fast tokenizers) may produce slightly inaccurate results.

  * .MarianTokenizer
    * `new MarianTokenizer(tokenizerJSON, tokenizerConfig)`
    * `._encode_text(text)` ‚áí `Array`

* * *

###  new MarianTokenizer(tokenizerJSON, tokenizerConfig)

Create a new MarianTokenizer instance.

Param| Type| Description  
---|---|---  
tokenizerJSON| `Object`| The JSON of the tokenizer.  
tokenizerConfig| `Object`| The config of the tokenizer.  
  
* * *

###  marianTokenizer._encode_text(text) ‚áí <code> Array </code>

Encodes a single text. Overriding this method is necessary since the language
codes must be removed before encoding with sentencepiece model.

**Kind** : instance method of `MarianTokenizer`  
**Returns** : `Array` \- The encoded tokens.  
**See** :
<https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213>

Param| Type| Description  
---|---|---  
text| `string` | `null`| The text to encode.  
  
* * *

##  tokenizers.AutoTokenizer

Helper class which is used to instantiate pretrained tokenizers with the
`from_pretrained` function. The chosen tokenizer class is determined by the
type specified in the tokenizer config.

**Kind** : static class of `tokenizers`

* * *

###  AutoTokenizer.from_pretrained(pretrained_model_name_or_path, options) ‚áí
<code> Promise. < PreTrainedTokenizer > </code>

Instantiate one of the tokenizer classes of the library from a pretrained
model.

The tokenizer class to instantiate is selected based on the `tokenizer_class`
property of the config object (either passed as an argument or loaded from
`pretrained_model_name_or_path` if possible)

**Kind** : static method of `AutoTokenizer`  
**Returns** : `Promise.<PreTrainedTokenizer>` \- A new instance of the
PreTrainedTokenizer class.

Param| Type| Description  
---|---|---  
pretrained_model_name_or_path| `string`| The name or path of the pretrained
model. Can be either:

  * A string, the _model id_ of a pretrained tokenizer hosted inside a model repo on huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
  * A path to a _directory_ containing tokenizer files, e.g., `./my_model_directory/`.

  
options| `PretrainedTokenizerOptions`| Additional options for loading the
tokenizer.  
  
* * *

##  tokenizers.is_chinese_char(cp) ‚áí <code> boolean </code>

Checks whether the given Unicode codepoint represents a CJK (Chinese,
Japanese, or Korean) character.

A ‚Äúchinese character‚Äù is defined as anything in the CJK Unicode block:
<https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)>

Note that the CJK Unicode block is NOT all Japanese and Korean characters,
despite its name. The modern Korean Hangul alphabet is a different block, as
is Japanese Hiragana and Katakana. Those alphabets are used to write space-
separated words, so they are not treated specially and are handled like all
other languages.

**Kind** : static method of `tokenizers`  
**Returns** : `boolean` \- True if the codepoint represents a CJK character,
false otherwise.

Param| Type| Description  
---|---|---  
cp| `number` | `bigint`| The Unicode codepoint to check.  
  
* * *

##  tokenizers~AddedToken

Represent a token added by the user on top of the existing Model vocabulary.
AddedToken can be configured to specify the behavior they should have in
various situations like:

  * Whether they should only match single words
  * Whether to include any whitespace on its left or right

**Kind** : inner class of `tokenizers`

* * *

###  new AddedToken(config)

Creates a new instance of AddedToken.

Param| Type| Default| Description  
---|---|---|---  
config| `Object`| | Added token configuration object.  
config.content| `string`| | The content of the added token.  
config.id| `number`| | The id of the added token.  
[config.single_word]| `boolean`| `false`| Whether this token must be a single
word or can break words.  
[config.lstrip]| `boolean`| `false`| Whether this token should strip
whitespaces on its left.  
[config.rstrip]| `boolean`| `false`| Whether this token should strip
whitespaces on its right.  
[config.normalized]| `boolean`| `false`| Whether this token should be
normalized.  
[config.special]| `boolean`| `false`| Whether this token is special.  
  
* * *

##  tokenizers~WordPieceTokenizer ‚áê <code> TokenizerModel </code>

A subclass of TokenizerModel that uses WordPiece encoding to encode tokens.

**Kind** : inner class of `tokenizers`  
**Extends** : `TokenizerModel`

  * ~WordPieceTokenizer ‚áê `TokenizerModel`
    * `new WordPieceTokenizer(config)`
    * `.tokens_to_ids` : `Map.<string, number>`
    * `.unk_token_id` : `number`
    * `.unk_token` : `string`
    * `.max_input_chars_per_word` : `number`
    * `.vocab` : `Array.<string>`
    * `.encode(tokens)` ‚áí `Array.<string>`

* * *

###  new WordPieceTokenizer(config)

Param| Type| Default| Description  
---|---|---|---  
config| `Object`| | The configuration object.  
config.vocab| `Object`| | A mapping of tokens to ids.  
config.unk_token| `string`| | The unknown token string.  
config.continuing_subword_prefix| `string`| | The prefix to use for continuing subwords.  
[config.max_input_chars_per_word]| `number`| `100`| The maximum number of
characters per word.  
  
* * *

###  wordPieceTokenizer.tokens_to_ids : <code> Map. < string, number > </code>

A mapping of tokens to ids.

**Kind** : instance property of `WordPieceTokenizer`

* * *

###  wordPieceTokenizer.unk_token_id : <code> number </code>

The id of the unknown token.

**Kind** : instance property of `WordPieceTokenizer`

* * *

###  wordPieceTokenizer.unk_token : <code> string </code>

The unknown token string.

**Kind** : instance property of `WordPieceTokenizer`

* * *

###  wordPieceTokenizer.max_input_chars_per_word : <code> number </code>

The maximum number of characters allowed per word.

**Kind** : instance property of `WordPieceTokenizer`

* * *

###  wordPieceTokenizer.vocab : <code> Array. < string > </code>

An array of tokens.

**Kind** : instance property of `WordPieceTokenizer`

* * *

###  wordPieceTokenizer.encode(tokens) ‚áí <code> Array. < string > </code>

Encodes an array of tokens using WordPiece encoding.

**Kind** : instance method of `WordPieceTokenizer`  
**Returns** : `Array.<string>` \- An array of encoded tokens.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| The tokens to encode.  
  
* * *

##  tokenizers~Unigram ‚áê <code> TokenizerModel </code>

Class representing a Unigram tokenizer model.

**Kind** : inner class of `tokenizers`  
**Extends** : `TokenizerModel`

  * ~Unigram ‚áê `TokenizerModel`
    * `new Unigram(config, moreConfig)`
    * `.populateNodes(lattice)`
    * `.tokenize(normalized)` ‚áí `Array.<string>`
    * `.encode(tokens)` ‚áí `Array.<string>`

* * *

###  new Unigram(config, moreConfig)

Create a new Unigram tokenizer model.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the Unigram model.  
config.unk_id| `number`| The ID of the unknown token  
config.vocab| `Array.<Array<any>>`| A 2D array representing a mapping of
tokens to scores.  
moreConfig| `Object`| Additional configuration object for the Unigram model.  
  
* * *

###  unigram.populateNodes(lattice)

Populates lattice nodes.

**Kind** : instance method of `Unigram`

Param| Type| Description  
---|---|---  
lattice| `TokenLattice`| The token lattice to populate with nodes.  
  
* * *

###  unigram.tokenize(normalized) ‚áí <code> Array. < string > </code>

Encodes an array of tokens into an array of subtokens using the unigram model.

**Kind** : instance method of `Unigram`  
**Returns** : `Array.<string>` \- An array of subtokens obtained by encoding
the input tokens using the unigram model.

Param| Type| Description  
---|---|---  
normalized| `string`| The normalized string.  
  
* * *

###  unigram.encode(tokens) ‚áí <code> Array. < string > </code>

Encodes an array of tokens using Unigram encoding.

**Kind** : instance method of `Unigram`  
**Returns** : `Array.<string>` \- An array of encoded tokens.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| The tokens to encode.  
  
* * *

##  tokenizers~BPE ‚áê <code> TokenizerModel </code>

BPE class for encoding text into Byte-Pair-Encoding (BPE) tokens.

**Kind** : inner class of `tokenizers`  
**Extends** : `TokenizerModel`

  * ~BPE ‚áê `TokenizerModel`
    * `new BPE(config)`
    * `.tokens_to_ids` : `Map.<string, number>`
    * `.merges` : `*`
      * `.config.merges` : `*`
    * `.cache` : `Map.<string, Array<string>>`
    * `.bpe(token)` ‚áí `Array.<string>`
    * `.encode(tokens)` ‚áí `Array.<string>`

* * *

###  new BPE(config)

Create a BPE instance.

Param| Type| Default| Description  
---|---|---|---  
config| `Object`| | The configuration object for BPE.  
config.vocab| `Object`| | A mapping of tokens to ids.  
config.merges| `*`| | An array of BPE merges as strings.  
config.unk_token| `string`| | The unknown token used for out of vocabulary words.  
config.end_of_word_suffix| `string`| | The suffix to place at the end of each word.  
[config.continuing_subword_suffix]| `string`| | The suffix to insert between words.  
[config.byte_fallback]| `boolean`| `false`| Whether to use spm byte-fallback
trick (defaults to False)  
[config.ignore_merges]| `boolean`| `false`| Whether or not to match tokens
with the vocab before using merges.  
  
* * *

###  bpE.tokens_to_ids : <code> Map. < string, number > </code>

**Kind** : instance property of `BPE`

* * *

###  bpE.merges : <code> * </code>

**Kind** : instance property of `BPE`

* * *

####  merges.config.merges : <code> * </code>

**Kind** : static property of `merges`

* * *

###  bpE.cache : <code> Map. < string, Array < string > > </code>

**Kind** : instance property of `BPE`

* * *

###  bpE.bpe(token) ‚áí <code> Array. < string > </code>

Apply Byte-Pair-Encoding (BPE) to a given token. Efficient heap-based priority
queue implementation adapted from <https://github.com/belladoreai/llama-
tokenizer-js>.

**Kind** : instance method of `BPE`  
**Returns** : `Array.<string>` \- The BPE encoded tokens.

Param| Type| Description  
---|---|---  
token| `string`| The token to encode.  
  
* * *

###  bpE.encode(tokens) ‚áí <code> Array. < string > </code>

Encodes the input sequence of tokens using the BPE algorithm and returns the
resulting subword tokens.

**Kind** : instance method of `BPE`  
**Returns** : `Array.<string>` \- The resulting subword tokens after applying
the BPE algorithm to the input sequence of tokens.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| The input sequence of tokens to encode.  
  
* * *

##  tokenizers~LegacyTokenizerModel

Legacy tokenizer class for tokenizers with only a vocabulary.

**Kind** : inner class of `tokenizers`

  * ~LegacyTokenizerModel
    * `new LegacyTokenizerModel(config, moreConfig)`
    * `.tokens_to_ids` : `Map.<string, number>`

* * *

###  new LegacyTokenizerModel(config, moreConfig)

Create a LegacyTokenizerModel instance.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for LegacyTokenizerModel.  
config.vocab| `Object`| A (possibly nested) mapping of tokens to ids.  
moreConfig| `Object`| Additional configuration object for the
LegacyTokenizerModel model.  
  
* * *

###  legacyTokenizerModel.tokens_to_ids : <code> Map. < string, number >
</code>

**Kind** : instance property of `LegacyTokenizerModel`

* * *

##  tokenizers~Normalizer

A base class for text normalization.

**Kind** : inner abstract class of `tokenizers`

  * _~Normalizer_
    *  _`new Normalizer(config)`_
    * _instance_
      * **`.normalize(text)` ‚áí `string`**
      * _`._call(text)` ‚áí `string`_
    * _static_
      *  _`.fromConfig(config)` ‚áí `Normalizer`_

* * *

###  new Normalizer(config)

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the normalizer.  
  
* * *

###  normalizer.normalize(text) ‚áí <code> string </code>

Normalize the input text.

**Kind** : instance abstract method of `Normalizer`  
**Returns** : `string` \- The normalized text.  
**Throws** :

  * `Error` If this method is not implemented in a subclass.

Param| Type| Description  
---|---|---  
text| `string`| The text to normalize.  
  
* * *

###  normalizer._call(text) ‚áí <code> string </code>

Alias for [Normalizer#normalize](Normalizer#normalize).

**Kind** : instance method of `Normalizer`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The text to normalize.  
  
* * *

###  Normalizer.fromConfig(config) ‚áí <code> Normalizer </code>

Factory method for creating normalizers from config objects.

**Kind** : static method of `Normalizer`  
**Returns** : `Normalizer` \- A Normalizer object.  
**Throws** :

  * `Error` If an unknown Normalizer type is specified in the config.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the normalizer.  
  
* * *

##  tokenizers~Replace ‚áê <code> Normalizer </code>

Replace normalizer that replaces occurrences of a pattern with a given string
or regular expression.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

* * *

###  replace.normalize(text) ‚áí <code> string </code>

Normalize the input text by replacing the pattern with the content.

**Kind** : instance method of `Replace`  
**Returns** : `string` \- The normalized text after replacing the pattern with
the content.

Param| Type| Description  
---|---|---  
text| `string`| The input text to be normalized.  
  
* * *

##  tokenizers~NFC ‚áê <code> Normalizer </code>

A normalizer that applies Unicode normalization form C (NFC) to the input
text.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

* * *

###  nfC.normalize(text) ‚áí <code> string </code>

Normalize the input text by applying Unicode normalization form C (NFC).

**Kind** : instance method of `NFC`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The input text to be normalized.  
  
* * *

##  tokenizers~NFKC ‚áê <code> Normalizer </code>

NFKC Normalizer.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

* * *

###  nfkC.normalize(text) ‚áí <code> string </code>

Normalize text using NFKC normalization.

**Kind** : instance method of `NFKC`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The text to be normalized.  
  
* * *

##  tokenizers~NFKD ‚áê <code> Normalizer </code>

NFKD Normalizer.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

* * *

###  nfkD.normalize(text) ‚áí <code> string </code>

Normalize text using NFKD normalization.

**Kind** : instance method of `NFKD`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The text to be normalized.  
  
* * *

##  tokenizers~StripNormalizer

A normalizer that strips leading and/or trailing whitespace from the input
text.

**Kind** : inner class of `tokenizers`

* * *

###  stripNormalizer.normalize(text) ‚áí <code> string </code>

Strip leading and/or trailing whitespace from the input text.

**Kind** : instance method of `StripNormalizer`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The input text.  
  
* * *

##  tokenizers~StripAccents ‚áê <code> Normalizer </code>

StripAccents normalizer removes all accents from the text.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

* * *

###  stripAccents.normalize(text) ‚áí <code> string </code>

Remove all accents from the text.

**Kind** : instance method of `StripAccents`  
**Returns** : `string` \- The normalized text without accents.

Param| Type| Description  
---|---|---  
text| `string`| The input text.  
  
* * *

##  tokenizers~Lowercase ‚áê <code> Normalizer </code>

A Normalizer that lowercases the input string.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

* * *

###  lowercase.normalize(text) ‚áí <code> string </code>

Lowercases the input string.

**Kind** : instance method of `Lowercase`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The text to normalize.  
  
* * *

##  tokenizers~Prepend ‚áê <code> Normalizer </code>

A Normalizer that prepends a string to the input string.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

* * *

###  prepend.normalize(text) ‚áí <code> string </code>

Prepends the input string.

**Kind** : instance method of `Prepend`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The text to normalize.  
  
* * *

##  tokenizers~NormalizerSequence ‚áê <code> Normalizer </code>

A Normalizer that applies a sequence of Normalizers.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

  * ~NormalizerSequence ‚áê `Normalizer`
    * `new NormalizerSequence(config)`
    * `.normalize(text)` ‚áí `string`

* * *

###  new NormalizerSequence(config)

Create a new instance of NormalizerSequence.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object.  
config.normalizers| `Array.<Object>`| An array of Normalizer configuration
objects.  
  
* * *

###  normalizerSequence.normalize(text) ‚áí <code> string </code>

Apply a sequence of Normalizers to the input text.

**Kind** : instance method of `NormalizerSequence`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The text to normalize.  
  
* * *

##  tokenizers~BertNormalizer ‚áê <code> Normalizer </code>

A class representing a normalizer used in BERT tokenization.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

  * ~BertNormalizer ‚áê `Normalizer`
    * `._tokenize_chinese_chars(text)` ‚áí `string`
    * `.stripAccents(text)` ‚áí `string`
    * `.normalize(text)` ‚áí `string`

* * *

###  bertNormalizer._tokenize_chinese_chars(text) ‚áí <code> string </code>

Adds whitespace around any CJK (Chinese, Japanese, or Korean) character in the
input text.

**Kind** : instance method of `BertNormalizer`  
**Returns** : `string` \- The tokenized text with whitespace added around CJK
characters.

Param| Type| Description  
---|---|---  
text| `string`| The input text to tokenize.  
  
* * *

###  bertNormalizer.stripAccents(text) ‚áí <code> string </code>

Strips accents from the given text.

**Kind** : instance method of `BertNormalizer`  
**Returns** : `string` \- The text with accents removed.

Param| Type| Description  
---|---|---  
text| `string`| The text to strip accents from.  
  
* * *

###  bertNormalizer.normalize(text) ‚áí <code> string </code>

Normalizes the given text based on the configuration.

**Kind** : instance method of `BertNormalizer`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The text to normalize.  
  
* * *

##  tokenizers~PreTokenizer ‚áê <code> Callable </code>

A callable class representing a pre-tokenizer used in tokenization. Subclasses
should implement the `pre_tokenize_text` method to define the specific pre-
tokenization logic.

**Kind** : inner class of `tokenizers`  
**Extends** : `Callable`

  * ~PreTokenizer ‚áê `Callable`
    *  _instance_
      *  _`.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`_
      * `.pre_tokenize(text, [options])` ‚áí `Array.<string>`
      * `._call(text, [options])` ‚áí `Array.<string>`
    * _static_
      * `.fromConfig(config)` ‚áí `PreTokenizer`

* * *

###  preTokenizer.pre_tokenize_text(text, [options]) ‚áí <code> Array. < string
> </code>

Method that should be implemented by subclasses to define the specific pre-
tokenization logic.

**Kind** : instance abstract method of `PreTokenizer`  
**Returns** : `Array.<string>` \- The pre-tokenized text.  
**Throws** :

  * `Error` If the method is not implemented in the subclass.

Param| Type| Description  
---|---|---  
text| `string`| The text to pre-tokenize.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

###  preTokenizer.pre_tokenize(text, [options]) ‚áí <code> Array. < string >
</code>

Tokenizes the given text into pre-tokens.

**Kind** : instance method of `PreTokenizer`  
**Returns** : `Array.<string>` \- An array of pre-tokens.

Param| Type| Description  
---|---|---  
text| `string` | `Array<string>`| The text or array of texts to pre-tokenize.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

###  preTokenizer._call(text, [options]) ‚áí <code> Array. < string > </code>

Alias for [PreTokenizer#pre_tokenize](PreTokenizer#pre_tokenize).

**Kind** : instance method of `PreTokenizer`  
**Overrides** : `_call`  
**Returns** : `Array.<string>` \- An array of pre-tokens.

Param| Type| Description  
---|---|---  
text| `string` | `Array<string>`| The text or array of texts to pre-tokenize.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

###  PreTokenizer.fromConfig(config) ‚áí <code> PreTokenizer </code>

Factory method that returns an instance of a subclass of `PreTokenizer` based
on the provided configuration.

**Kind** : static method of `PreTokenizer`  
**Returns** : `PreTokenizer` \- An instance of a subclass of `PreTokenizer`.  
**Throws** :

  * `Error` If the provided configuration object does not correspond to any known pre-tokenizer.

Param| Type| Description  
---|---|---  
config| `Object`| A configuration object for the pre-tokenizer.  
  
* * *

##  tokenizers~BertPreTokenizer ‚áê <code> PreTokenizer </code>

**Kind** : inner class of `tokenizers`  
**Extends** : `PreTokenizer`

  * ~BertPreTokenizer ‚áê `PreTokenizer`
    * `new BertPreTokenizer(config)`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new BertPreTokenizer(config)

A PreTokenizer that splits text into wordpieces using a basic tokenization
scheme similar to that used in the original implementation of BERT.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object.  
  
* * *

###  bertPreTokenizer.pre_tokenize_text(text, [options]) ‚áí <code> Array. <
string > </code>

Tokenizes a single text using the BERT pre-tokenization scheme.

**Kind** : instance method of `BertPreTokenizer`  
**Returns** : `Array.<string>` \- An array of tokens.

Param| Type| Description  
---|---|---  
text| `string`| The text to tokenize.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

##  tokenizers~ByteLevelPreTokenizer ‚áê <code> PreTokenizer </code>

A pre-tokenizer that splits text into Byte-Pair-Encoding (BPE) subwords.

**Kind** : inner class of `tokenizers`  
**Extends** : `PreTokenizer`

  * ~ByteLevelPreTokenizer ‚áê `PreTokenizer`
    * `new ByteLevelPreTokenizer(config)`
    * `.add_prefix_space` : `boolean`
    * `.trim_offsets` : `boolean`
    * `.use_regex` : `boolean`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new ByteLevelPreTokenizer(config)

Creates a new instance of the `ByteLevelPreTokenizer` class.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object.  
  
* * *

###  byteLevelPreTokenizer.add_prefix_space : <code> boolean </code>

Whether to add a leading space to the first word.This allows to treat the
leading word just as any other word.

**Kind** : instance property of `ByteLevelPreTokenizer`

* * *

###  byteLevelPreTokenizer.trim_offsets : <code> boolean </code>

Whether the post processing step should trim offsetsto avoid including
whitespaces.

**Kind** : instance property of `ByteLevelPreTokenizer`  
**Todo**

  * Use this in the pretokenization step.

* * *

###  byteLevelPreTokenizer.use_regex : <code> boolean </code>

Whether to use the standard GPT2 regex for whitespace splitting.Set it to
False if you want to use your own splitting. Defaults to true.

**Kind** : instance property of `ByteLevelPreTokenizer`

* * *

###  byteLevelPreTokenizer.pre_tokenize_text(text, [options]) ‚áí <code> Array.
< string > </code>

Tokenizes a single piece of text using byte-level tokenization.

**Kind** : instance method of `ByteLevelPreTokenizer`  
**Returns** : `Array.<string>` \- An array of tokens.

Param| Type| Description  
---|---|---  
text| `string`| The text to tokenize.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

##  tokenizers~SplitPreTokenizer ‚áê <code> PreTokenizer </code>

Splits text using a given pattern.

**Kind** : inner class of `tokenizers`  
**Extends** : `PreTokenizer`

  * ~SplitPreTokenizer ‚áê `PreTokenizer`
    * `new SplitPreTokenizer(config)`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new SplitPreTokenizer(config)

Param| Type| Description  
---|---|---  
config| `Object`| The configuration options for the pre-tokenizer.  
config.pattern| `Object`| The pattern used to split the text. Can be a string
or a regex object.  
config.pattern.String| `string` | `undefined`| The string to use for splitting. Only defined if the pattern is a string.  
config.pattern.Regex| `string` | `undefined`| The regex to use for splitting. Only defined if the pattern is a regex.  
config.behavior| `SplitDelimiterBehavior`| The behavior to use when splitting.  
config.invert| `boolean`| Whether to split (invert=false) or match
(invert=true) the pattern.  
  
* * *

###  splitPreTokenizer.pre_tokenize_text(text, [options]) ‚áí <code> Array. <
string > </code>

Tokenizes text by splitting it using the given pattern.

**Kind** : instance method of `SplitPreTokenizer`  
**Returns** : `Array.<string>` \- An array of tokens.

Param| Type| Description  
---|---|---  
text| `string`| The text to tokenize.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

##  tokenizers~PunctuationPreTokenizer ‚áê <code> PreTokenizer </code>

Splits text based on punctuation.

**Kind** : inner class of `tokenizers`  
**Extends** : `PreTokenizer`

  * ~PunctuationPreTokenizer ‚áê `PreTokenizer`
    * `new PunctuationPreTokenizer(config)`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new PunctuationPreTokenizer(config)

Param| Type| Description  
---|---|---  
config| `Object`| The configuration options for the pre-tokenizer.  
config.behavior| `SplitDelimiterBehavior`| The behavior to use when splitting.  
  
* * *

###  punctuationPreTokenizer.pre_tokenize_text(text, [options]) ‚áí <code>
Array. < string > </code>

Tokenizes text by splitting it using the given pattern.

**Kind** : instance method of `PunctuationPreTokenizer`  
**Returns** : `Array.<string>` \- An array of tokens.

Param| Type| Description  
---|---|---  
text| `string`| The text to tokenize.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

##  tokenizers~DigitsPreTokenizer ‚áê <code> PreTokenizer </code>

Splits text based on digits.

**Kind** : inner class of `tokenizers`  
**Extends** : `PreTokenizer`

  * ~DigitsPreTokenizer ‚áê `PreTokenizer`
    * `new DigitsPreTokenizer(config)`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new DigitsPreTokenizer(config)

Param| Type| Description  
---|---|---  
config| `Object`| The configuration options for the pre-tokenizer.  
config.individual_digits| `boolean`| Whether to split on individual digits.  
  
* * *

###  digitsPreTokenizer.pre_tokenize_text(text, [options]) ‚áí <code> Array. <
string > </code>

Tokenizes text by splitting it using the given pattern.

**Kind** : instance method of `DigitsPreTokenizer`  
**Returns** : `Array.<string>` \- An array of tokens.

Param| Type| Description  
---|---|---  
text| `string`| The text to tokenize.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

##  tokenizers~PostProcessor ‚áê <code> Callable </code>

**Kind** : inner class of `tokenizers`  
**Extends** : `Callable`

  * ~PostProcessor ‚áê `Callable`
    * `new PostProcessor(config)`
    * _instance_
      * `.post_process(tokens, ...args)` ‚áí `PostProcessedOutput`
      * `._call(tokens, ...args)` ‚áí `PostProcessedOutput`
    * _static_
      * `.fromConfig(config)` ‚áí `PostProcessor`

* * *

###  new PostProcessor(config)

Param| Type| Description  
---|---|---  
config| `Object`| The configuration for the post-processor.  
  
* * *

###  postProcessor.post_process(tokens, ...args) ‚áí <code> PostProcessedOutput
</code>

Method to be implemented in subclass to apply post-processing on the given
tokens.

**Kind** : instance method of `PostProcessor`  
**Returns** : `PostProcessedOutput` \- The post-processed tokens.  
**Throws** :

  * `Error` If the method is not implemented in subclass.

Param| Type| Description  
---|---|---  
tokens| `Array`| The input tokens to be post-processed.  
...args| `*`| Additional arguments required by the post-processing logic.  
  
* * *

###  postProcessor._call(tokens, ...args) ‚áí <code> PostProcessedOutput </code>

Alias for [PostProcessor#post_process](PostProcessor#post_process).

**Kind** : instance method of `PostProcessor`  
**Overrides** : `_call`  
**Returns** : `PostProcessedOutput` \- The post-processed tokens.

Param| Type| Description  
---|---|---  
tokens| `Array`| The text or array of texts to post-process.  
...args| `*`| Additional arguments required by the post-processing logic.  
  
* * *

###  PostProcessor.fromConfig(config) ‚áí <code> PostProcessor </code>

Factory method to create a PostProcessor object from a configuration object.

**Kind** : static method of `PostProcessor`  
**Returns** : `PostProcessor` \- A PostProcessor object created from the given
configuration.  
**Throws** :

  * `Error` If an unknown PostProcessor type is encountered.

Param| Type| Description  
---|---|---  
config| `Object`| Configuration object representing a PostProcessor.  
  
* * *

##  tokenizers~BertProcessing

A post-processor that adds special tokens to the beginning and end of the
input.

**Kind** : inner class of `tokenizers`

  * ~BertProcessing
    * `new BertProcessing(config)`
    * `.post_process(tokens, [tokens_pair])` ‚áí `PostProcessedOutput`

* * *

###  new BertProcessing(config)

Param| Type| Description  
---|---|---  
config| `Object`| The configuration for the post-processor.  
config.cls| `Array.<string>`| The special tokens to add to the beginning of
the input.  
config.sep| `Array.<string>`| The special tokens to add to the end of the
input.  
  
* * *

###  bertProcessing.post_process(tokens, [tokens_pair]) ‚áí <code>
PostProcessedOutput </code>

Adds the special tokens to the beginning and end of the input.

**Kind** : instance method of `BertProcessing`  
**Returns** : `PostProcessedOutput` \- The post-processed tokens with the
special tokens added to the beginning and end.

Param| Type| Default| Description  
---|---|---|---  
tokens| `Array.<string>`| | The input tokens.  
[tokens_pair]| `Array.<string>`| ``| An optional second set of input tokens.  
  
* * *

##  tokenizers~TemplateProcessing ‚áê <code> PostProcessor </code>

Post processor that replaces special tokens in a template with actual tokens.

**Kind** : inner class of `tokenizers`  
**Extends** : `PostProcessor`

  * ~TemplateProcessing ‚áê `PostProcessor`
    * `new TemplateProcessing(config)`
    * `.post_process(tokens, [tokens_pair])` ‚áí `PostProcessedOutput`

* * *

###  new TemplateProcessing(config)

Creates a new instance of `TemplateProcessing`.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration options for the post processor.  
config.single| `Array`| The template for a single sequence of tokens.  
config.pair| `Array`| The template for a pair of sequences of tokens.  
  
* * *

###  templateProcessing.post_process(tokens, [tokens_pair]) ‚áí <code>
PostProcessedOutput </code>

Replaces special tokens in the template with actual tokens.

**Kind** : instance method of `TemplateProcessing`  
**Returns** : `PostProcessedOutput` \- An object containing the list of tokens
with the special tokens replaced with actual tokens.

Param| Type| Default| Description  
---|---|---|---  
tokens| `Array.<string>`| | The list of tokens for the first sequence.  
[tokens_pair]| `Array.<string>`| ``| The list of tokens for the second
sequence (optional).  
  
* * *

##  tokenizers~ByteLevelPostProcessor ‚áê <code> PostProcessor </code>

A PostProcessor that returns the given tokens as is.

**Kind** : inner class of `tokenizers`  
**Extends** : `PostProcessor`

* * *

###  byteLevelPostProcessor.post_process(tokens, [tokens_pair]) ‚áí <code>
PostProcessedOutput </code>

Post process the given tokens.

**Kind** : instance method of `ByteLevelPostProcessor`  
**Returns** : `PostProcessedOutput` \- An object containing the post-processed
tokens.

Param| Type| Default| Description  
---|---|---|---  
tokens| `Array.<string>`| | The list of tokens for the first sequence.  
[tokens_pair]| `Array.<string>`| ``| The list of tokens for the second
sequence (optional).  
  
* * *

##  tokenizers~PostProcessorSequence

A post-processor that applies multiple post-processors in sequence.

**Kind** : inner class of `tokenizers`

  * ~PostProcessorSequence
    * `new PostProcessorSequence(config)`
    * `.post_process(tokens, [tokens_pair])` ‚áí `PostProcessedOutput`

* * *

###  new PostProcessorSequence(config)

Creates a new instance of PostProcessorSequence.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object.  
config.processors| `Array.<Object>`| The list of post-processors to apply.  
  
* * *

###  postProcessorSequence.post_process(tokens, [tokens_pair]) ‚áí <code>
PostProcessedOutput </code>

Post process the given tokens.

**Kind** : instance method of `PostProcessorSequence`  
**Returns** : `PostProcessedOutput` \- An object containing the post-processed
tokens.

Param| Type| Default| Description  
---|---|---|---  
tokens| `Array.<string>`| | The list of tokens for the first sequence.  
[tokens_pair]| `Array.<string>`| ``| The list of tokens for the second
sequence (optional).  
  
* * *

##  tokenizers~Decoder ‚áê <code> Callable </code>

The base class for token decoders.

**Kind** : inner class of `tokenizers`  
**Extends** : `Callable`

  * ~Decoder ‚áê `Callable`
    * `new Decoder(config)`
    * _instance_
      * `.added_tokens` : `Array.<AddedToken>`
      * `._call(tokens)` ‚áí `string`
      * `.decode(tokens)` ‚áí `string`
      * `.decode_chain(tokens)` ‚áí `Array.<string>`
    * _static_
      * `.fromConfig(config)` ‚áí `Decoder`

* * *

###  new Decoder(config)

Creates an instance of `Decoder`.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object.  
  
* * *

###  decoder.added_tokens : <code> Array. < AddedToken > </code>

**Kind** : instance property of `Decoder`

* * *

###  decoder._call(tokens) ‚áí <code> string </code>

Calls the `decode` method.

**Kind** : instance method of `Decoder`  
**Overrides** : `_call`  
**Returns** : `string` \- The decoded string.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| The list of tokens.  
  
* * *

###  decoder.decode(tokens) ‚áí <code> string </code>

Decodes a list of tokens.

**Kind** : instance method of `Decoder`  
**Returns** : `string` \- The decoded string.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| The list of tokens.  
  
* * *

###  decoder.decode_chain(tokens) ‚áí <code> Array. < string > </code>

Apply the decoder to a list of tokens.

**Kind** : instance method of `Decoder`  
**Returns** : `Array.<string>` \- The decoded list of tokens.  
**Throws** :

  * `Error` If the `decode_chain` method is not implemented in the subclass.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| The list of tokens.  
  
* * *

###  Decoder.fromConfig(config) ‚áí <code> Decoder </code>

Creates a decoder instance based on the provided configuration.

**Kind** : static method of `Decoder`  
**Returns** : `Decoder` \- A decoder instance.  
**Throws** :

  * `Error` If an unknown decoder type is provided.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object.  
  
* * *

##  tokenizers~FuseDecoder

Fuse simply fuses all tokens into one big string. It‚Äôs usually the last
decoding step anyway, but this decoder exists incase some decoders need to
happen after that step

**Kind** : inner class of `tokenizers`

* * *

###  fuseDecoder.decode_chain() : <code> * </code>

**Kind** : instance method of `FuseDecoder`

* * *

##  tokenizers~WordPieceDecoder ‚áê <code> Decoder </code>

A decoder that decodes a list of WordPiece tokens into a single string.

**Kind** : inner class of `tokenizers`  
**Extends** : `Decoder`

  * ~WordPieceDecoder ‚áê `Decoder`
    * `new WordPieceDecoder(config)`
    * `.decode_chain()` : `*`

* * *

###  new WordPieceDecoder(config)

Creates a new instance of WordPieceDecoder.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object.  
config.prefix| `string`| The prefix used for WordPiece encoding.  
config.cleanup| `boolean`| Whether to cleanup the decoded string.  
  
* * *

###  wordPieceDecoder.decode_chain() : <code> * </code>

**Kind** : instance method of `WordPieceDecoder`

* * *

##  tokenizers~ByteLevelDecoder ‚áê <code> Decoder </code>

Byte-level decoder for tokenization output. Inherits from the `Decoder` class.

**Kind** : inner class of `tokenizers`  
**Extends** : `Decoder`

  * ~ByteLevelDecoder ‚áê `Decoder`
    * `new ByteLevelDecoder(config)`
    * `.convert_tokens_to_string(tokens)` ‚áí `string`
    * `.decode_chain()` : `*`

* * *

###  new ByteLevelDecoder(config)

Create a `ByteLevelDecoder` object.

Param| Type| Description  
---|---|---  
config| `Object`| Configuration object.  
  
* * *

###  byteLevelDecoder.convert_tokens_to_string(tokens) ‚áí <code> string </code>

Convert an array of tokens to string by decoding each byte.

**Kind** : instance method of `ByteLevelDecoder`  
**Returns** : `string` \- The decoded string.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| Array of tokens to be decoded.  
  
* * *

###  byteLevelDecoder.decode_chain() : <code> * </code>

**Kind** : instance method of `ByteLevelDecoder`

* * *

##  tokenizers~CTCDecoder

The CTC (Connectionist Temporal Classification) decoder. See
<https://github.com/huggingface/tokenizers/blob/bb38f390a61883fc2f29d659af696f428d1cda6b/tokenizers/src/decoders/ctc.rs>

**Kind** : inner class of `tokenizers`

  * ~CTCDecoder
    * `.convert_tokens_to_string(tokens)` ‚áí `string`
    * `.decode_chain()` : `*`

* * *

###  ctcDecoder.convert_tokens_to_string(tokens) ‚áí <code> string </code>

Converts a connectionist-temporal-classification (CTC) output tokens into a
single string.

**Kind** : instance method of `CTCDecoder`  
**Returns** : `string` \- The decoded string.

Param| Type| Description  
---|---|---  
tokens| `Array.<string>`| Array of tokens to be decoded.  
  
* * *

###  ctcDecoder.decode_chain() : <code> * </code>

**Kind** : instance method of `CTCDecoder`

* * *

##  tokenizers~DecoderSequence ‚áê <code> Decoder </code>

Apply a sequence of decoders.

**Kind** : inner class of `tokenizers`  
**Extends** : `Decoder`

  * ~DecoderSequence ‚áê `Decoder`
    * `new DecoderSequence(config)`
    * `.decode_chain()` : `*`

* * *

###  new DecoderSequence(config)

Creates a new instance of DecoderSequence.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object.  
config.decoders| `Array.<Object>`| The list of decoders to apply.  
  
* * *

###  decoderSequence.decode_chain() : <code> * </code>

**Kind** : instance method of `DecoderSequence`

* * *

##  tokenizers~MetaspacePreTokenizer ‚áê <code> PreTokenizer </code>

This PreTokenizer replaces spaces with the given replacement character, adds a
prefix space if requested, and returns a list of tokens.

**Kind** : inner class of `tokenizers`  
**Extends** : `PreTokenizer`

  * ~MetaspacePreTokenizer ‚áê `PreTokenizer`
    * `new MetaspacePreTokenizer(config)`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new MetaspacePreTokenizer(config)

Param| Type| Default| Description  
---|---|---|---  
config| `Object`| | The configuration object for the MetaspacePreTokenizer.  
config.add_prefix_space| `boolean`| | Whether to add a prefix space to the first token.  
config.replacement| `string`| | The character to replace spaces with.  
[config.str_rep]| `string`| `"config.replacement"`| An optional string
representation of the replacement character.  
[config.prepend_scheme]| `'first'` | `'never'` | `'always'`| `'always'`| The metaspace prepending scheme.  
  
* * *

###  metaspacePreTokenizer.pre_tokenize_text(text, [options]) ‚áí <code> Array.
< string > </code>

This method takes a string, replaces spaces with the replacement character,
adds a prefix space if requested, and returns a new list of tokens.

**Kind** : instance method of `MetaspacePreTokenizer`  
**Returns** : `Array.<string>` \- A new list of pre-tokenized tokens.

Param| Type| Description  
---|---|---  
text| `string`| The text to pre-tokenize.  
[options]| `Object`| The options for the pre-tokenization.  
[options.section_index]| `number`| The index of the section to pre-tokenize.  
  
* * *

##  tokenizers~MetaspaceDecoder ‚áê <code> Decoder </code>

MetaspaceDecoder class extends the Decoder class and decodes Metaspace
tokenization.

**Kind** : inner class of `tokenizers`  
**Extends** : `Decoder`

  * ~MetaspaceDecoder ‚áê `Decoder`
    * `new MetaspaceDecoder(config)`
    * `.decode_chain()` : `*`

* * *

###  new MetaspaceDecoder(config)

Constructs a new MetaspaceDecoder object.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the MetaspaceDecoder.  
config.add_prefix_space| `boolean`| Whether to add a prefix space to the
decoded string.  
config.replacement| `string`| The string to replace spaces with.  
  
* * *

###  metaspaceDecoder.decode_chain() : <code> * </code>

**Kind** : instance method of `MetaspaceDecoder`

* * *

##  tokenizers~Precompiled ‚áê <code> Normalizer </code>

A normalizer that applies a precompiled charsmap. This is useful for applying
complex normalizations in C++ and exposing them to JavaScript.

**Kind** : inner class of `tokenizers`  
**Extends** : `Normalizer`

  * ~Precompiled ‚áê `Normalizer`
    * `new Precompiled(config)`
    * `.normalize(text)` ‚áí `string`

* * *

###  new Precompiled(config)

Create a new instance of Precompiled normalizer.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the Precompiled normalizer.  
config.precompiled_charsmap| `Object`| The precompiled charsmap object.  
  
* * *

###  precompiled.normalize(text) ‚áí <code> string </code>

Normalizes the given text by applying the precompiled charsmap.

**Kind** : instance method of `Precompiled`  
**Returns** : `string` \- The normalized text.

Param| Type| Description  
---|---|---  
text| `string`| The text to normalize.  
  
* * *

##  tokenizers~PreTokenizerSequence ‚áê <code> PreTokenizer </code>

A pre-tokenizer that applies a sequence of pre-tokenizers to the input text.

**Kind** : inner class of `tokenizers`  
**Extends** : `PreTokenizer`

  * ~PreTokenizerSequence ‚áê `PreTokenizer`
    * `new PreTokenizerSequence(config)`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new PreTokenizerSequence(config)

Creates an instance of PreTokenizerSequence.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the pre-tokenizer sequence.  
config.pretokenizers| `Array.<Object>`| An array of pre-tokenizer
configurations.  
  
* * *

###  preTokenizerSequence.pre_tokenize_text(text, [options]) ‚áí <code> Array. <
string > </code>

Applies each pre-tokenizer in the sequence to the input text in turn.

**Kind** : instance method of `PreTokenizerSequence`  
**Returns** : `Array.<string>` \- The pre-tokenized text.

Param| Type| Description  
---|---|---  
text| `string`| The text to pre-tokenize.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

##  tokenizers~WhitespacePreTokenizer

Splits on word boundaries (using the following regular expression:
`\w+|[^\w\s]+`).

**Kind** : inner class of `tokenizers`

  * ~WhitespacePreTokenizer
    * `new WhitespacePreTokenizer(config)`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new WhitespacePreTokenizer(config)

Creates an instance of WhitespacePreTokenizer.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the pre-tokenizer.  
  
* * *

###  whitespacePreTokenizer.pre_tokenize_text(text, [options]) ‚áí <code> Array.
< string > </code>

Pre-tokenizes the input text by splitting it on word boundaries.

**Kind** : instance method of `WhitespacePreTokenizer`  
**Returns** : `Array.<string>` \- An array of tokens produced by splitting the
input text on whitespace.

Param| Type| Description  
---|---|---  
text| `string`| The text to be pre-tokenized.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

##  tokenizers~WhitespaceSplit ‚áê <code> PreTokenizer </code>

Splits a string of text by whitespace characters into individual tokens.

**Kind** : inner class of `tokenizers`  
**Extends** : `PreTokenizer`

  * ~WhitespaceSplit ‚áê `PreTokenizer`
    * `new WhitespaceSplit(config)`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new WhitespaceSplit(config)

Creates an instance of WhitespaceSplit.

Param| Type| Description  
---|---|---  
config| `Object`| The configuration object for the pre-tokenizer.  
  
* * *

###  whitespaceSplit.pre_tokenize_text(text, [options]) ‚áí <code> Array. <
string > </code>

Pre-tokenizes the input text by splitting it on whitespace characters.

**Kind** : instance method of `WhitespaceSplit`  
**Returns** : `Array.<string>` \- An array of tokens produced by splitting the
input text on whitespace.

Param| Type| Description  
---|---|---  
text| `string`| The text to be pre-tokenized.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

##  tokenizers~ReplacePreTokenizer

**Kind** : inner class of `tokenizers`

  * ~ReplacePreTokenizer
    * `new ReplacePreTokenizer(config)`
    * `.pre_tokenize_text(text, [options])` ‚áí `Array.<string>`

* * *

###  new ReplacePreTokenizer(config)

Param| Type| Description  
---|---|---  
config| `Object`| The configuration options for the pre-tokenizer.  
config.pattern| `Object`| The pattern used to split the text. Can be a string
or a regex object.  
config.content| `string`| What to replace the pattern with.  
  
* * *

###  replacePreTokenizer.pre_tokenize_text(text, [options]) ‚áí <code> Array. <
string > </code>

Pre-tokenizes the input text by replacing certain characters.

**Kind** : instance method of `ReplacePreTokenizer`  
**Returns** : `Array.<string>` \- An array of tokens produced by replacing
certain characters.

Param| Type| Description  
---|---|---  
text| `string`| The text to be pre-tokenized.  
[options]| `Object`| Additional options for the pre-tokenization logic.  
  
* * *

##  tokenizers~BYTES_TO_UNICODE ‚áí <code> Object </code>

Returns list of utf-8 byte and a mapping to unicode strings. Specifically
avoids mapping to whitespace/control characters the BPE code barfs on.

**Kind** : inner constant of `tokenizers`  
**Returns** : `Object` \- Object with utf-8 byte keys and unicode string
values.

* * *

##  tokenizers~loadTokenizer(pretrained_model_name_or_path, options) ‚áí <code>
Promise. < Array < any > > </code>

Loads a tokenizer from the specified path.

**Kind** : inner method of `tokenizers`  
**Returns** : `Promise.<Array<any>>` \- A promise that resolves with
information about the loaded tokenizer.

Param| Type| Description  
---|---|---  
pretrained_model_name_or_path| `string`| The path to the tokenizer directory.  
options| `PretrainedTokenizerOptions`| Additional options for loading the
tokenizer.  
  
* * *

##  tokenizers~regexSplit(text, regex) ‚áí <code> Array. < string > </code>

Helper function to split a string on a regex, but keep the delimiters. This is
required, because the JavaScript `.split()` method does not keep the
delimiters, and wrapping in a capturing group causes issues with existing
capturing groups (due to nesting).

**Kind** : inner method of `tokenizers`  
**Returns** : `Array.<string>` \- The split string.

Param| Type| Description  
---|---|---  
text| `string`| The text to split.  
regex| `RegExp`| The regex to split on.  
  
* * *

##  tokenizers~createPattern(pattern, invert) ‚áí <code> RegExp </code> | <code> null </code>

Helper method to construct a pattern from a config object.

**Kind** : inner method of `tokenizers`  
**Returns** : `RegExp` | `null` \- The compiled pattern.

Param| Type| Default| Description  
---|---|---|---  
pattern| `Object`| | The pattern object.  
invert| `boolean`| `true`| Whether to invert the pattern.  
  
* * *

##  tokenizers~objectToMap(obj) ‚áí <code> Map. < string, any > </code>

Helper function to convert an Object to a Map

**Kind** : inner method of `tokenizers`  
**Returns** : `Map.<string, any>` \- The map.

Param| Type| Description  
---|---|---  
obj| `Object`| The object to convert.  
  
* * *

##  tokenizers~prepareTensorForDecode(tensor) ‚áí <code> Array. < number >
</code>

Helper function to convert a tensor to a list before decoding.

**Kind** : inner method of `tokenizers`  
**Returns** : `Array.<number>` \- The tensor as a list.

Param| Type| Description  
---|---|---  
tensor| `Tensor`| The tensor to convert.  
  
* * *

##  tokenizers~clean_up_tokenization(text) ‚áí <code> string </code>

Clean up a list of simple English tokenization artifacts like spaces before
punctuations and abbreviated forms

**Kind** : inner method of `tokenizers`  
**Returns** : `string` \- The cleaned up text.

Param| Type| Description  
---|---|---  
text| `string`| The text to clean up.  
  
* * *

##  tokenizers~remove_accents(text) ‚áí <code> string </code>

Helper function to remove accents from a string.

**Kind** : inner method of `tokenizers`  
**Returns** : `string` \- The text with accents removed.

Param| Type| Description  
---|---|---  
text| `string`| The text to remove accents from.  
  
* * *

##  tokenizers~lowercase_and_remove_accent(text) ‚áí <code> string </code>

Helper function to lowercase a string and remove accents.

**Kind** : inner method of `tokenizers`  
**Returns** : `string` \- The lowercased text with accents removed.

Param| Type| Description  
---|---|---  
text| `string`| The text to lowercase and remove accents from.  
  
* * *

##  tokenizers~whitespace_split(text) ‚áí <code> Array. < string > </code>

Split a string on whitespace.

**Kind** : inner method of `tokenizers`  
**Returns** : `Array.<string>` \- The split string.

Param| Type| Description  
---|---|---  
text| `string`| The text to split.  
  
* * *

##  tokenizers~PretrainedTokenizerOptions : <code> Object </code>

Additional tokenizer-specific properties.

**Kind** : inner typedef of `tokenizers`  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
[legacy]| `boolean`| `false`| Whether or not the `legacy` behavior of the
tokenizer should be used.  
  
* * *

##  tokenizers~BPENode : <code> Object </code>

**Kind** : inner typedef of `tokenizers`  
**Properties**

Name| Type| Description  
---|---|---  
token| `string`| The token associated with the node  
bias| `number`| A positional bias for the node.  
[score]| `number`| The score of the node.  
[prev]| `BPENode`| The previous node in the linked list.  
[next]| `BPENode`| The next node in the linked list.  
  
* * *

##  tokenizers~SplitDelimiterBehavior : <code> ‚Äô removed ‚Äô </code> | <code> ‚Äô isolated ‚Äô </code> | <code> ‚Äô mergedWithPrevious ‚Äô </code> | <code> ‚Äô mergedWithNext ‚Äô </code> | <code> ‚Äô contiguous ‚Äô </code>

**Kind** : inner typedef of `tokenizers`

* * *

##  tokenizers~PostProcessedOutput : <code> Object </code>

**Kind** : inner typedef of `tokenizers`  
**Properties**

Name| Type| Description  
---|---|---  
tokens| `Array.<string>`| List of token produced by the post-processor.  
[token_type_ids]| `Array.<number>`| List of token type ids produced by the
post-processor.  
  
* * *

##  tokenizers~EncodingSingle : <code> Object </code>

**Kind** : inner typedef of `tokenizers`  
**Properties**

Name| Type| Description  
---|---|---  
input_ids| `Array.<number>`| List of token ids to be fed to a model.  
attention_mask| `Array.<number>`| List of token type ids to be fed to a model  
[token_type_ids]| `Array.<number>`| List of indices specifying which tokens
should be attended to by the model  
  
* * *

##  tokenizers~Message : <code> Object </code>

**Kind** : inner typedef of `tokenizers`  
**Properties**

Name| Type| Description  
---|---|---  
role| `string`| The role of the message (e.g., "user" or "assistant" or
"system").  
content| `string`| The content of the message.  
  
* * *

##  tokenizers~BatchEncoding : <code> Array < number > </code> | <code> Array < Array < number > > </code> | <code> Tensor </code>

Holds the output of the tokenizer‚Äôs call function.

**Kind** : inner typedef of `tokenizers`  
**Properties**

Name| Type| Description  
---|---|---  
input_ids| `BatchEncodingItem`| List of token ids to be fed to a model.  
attention_mask| `BatchEncodingItem`| List of indices specifying which tokens
should be attended to by the model.  
[token_type_ids]| `BatchEncodingItem`| List of token type ids to be fed to a
model.  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/tokenizers.md)

[‚ÜêModels](/docs/transformers.js/v3.0.0/en/api/models)
[Processors‚Üí](/docs/transformers.js/v3.0.0/en/api/processors)

tokenizers tokenizers.TokenizerModel ‚áê ` Callable ` new TokenizerModel(config) tokenizerModel.vocab : ` Array. < string > ` tokenizerModel.tokens_to_ids : ` Map. < string, number > ` tokenizerModel.fuse_unk : ` boolean ` tokenizerModel._call(tokens) ‚áí ` Array. < string > ` tokenizerModel.encode(tokens) ‚áí ` Array. < string > ` tokenizerModel.convert_tokens_to_ids(tokens) ‚áí ` Array. < number > ` tokenizerModel.convert_ids_to_tokens(ids) ‚áí ` Array. < string > ` TokenizerModel.fromConfig(config, ...args) ‚áí ` TokenizerModel ` tokenizers.PreTrainedTokenizer new PreTrainedTokenizer(tokenizerJSON, tokenizerConfig) preTrainedTokenizer.added_tokens : ` Array. < AddedToken > ` preTrainedTokenizer.remove_space : ` boolean ` preTrainedTokenizer._call(text, options) ‚áí ` BatchEncoding ` preTrainedTokenizer._encode_text(text) ‚áí ` Array < string > ` | ` null ` preTrainedTokenizer._tokenize_helper(text, options) ‚áí ` * ` preTrainedTokenizer.tokenize(text, options) ‚áí ` Array. < string > ` preTrainedTokenizer.encode(text, options) ‚áí ` Array. < number > ` preTrainedTokenizer.batch_decode(batch, decode_args) ‚áí ` Array. < string > ` preTrainedTokenizer.decode(token_ids, [decode_args]) ‚áí ` string ` preTrainedTokenizer.decode_single(token_ids, decode_args) ‚áí ` string ` preTrainedTokenizer.get_chat_template(options) ‚áí ` string ` preTrainedTokenizer.apply_chat_template(conversation, options) ‚áí ` string ` | ` Tensor ` | ` Array < number > ` | ` Array < Array < number > > ` | ` BatchEncoding ` PreTrainedTokenizer.from_pretrained(pretrained_model_name_or_path, options) ‚áí ` Promise. < PreTrainedTokenizer > ` tokenizers.BertTokenizer ‚áê ` PreTrainedTokenizer ` tokenizers.AlbertTokenizer ‚áê ` PreTrainedTokenizer ` tokenizers.NllbTokenizer nllbTokenizer._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs) ‚áí ` Object ` tokenizers.M2M100Tokenizer m2M100Tokenizer._build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs) ‚áí ` Object ` tokenizers.WhisperTokenizer ‚áê ` PreTrainedTokenizer ` whisperTokenizer._decode_asr(sequences, options) ‚áí ` * ` whisperTokenizer.decode() : ` * ` tokenizers.MarianTokenizer new MarianTokenizer(tokenizerJSON, tokenizerConfig) marianTokenizer._encode_text(text) ‚áí ` Array ` tokenizers.AutoTokenizer AutoTokenizer.from_pretrained(pretrained_model_name_or_path, options) ‚áí ` Promise. < PreTrainedTokenizer > ` tokenizers.is_chinese_char(cp) ‚áí ` boolean ` tokenizers~AddedToken new AddedToken(config) tokenizers~WordPieceTokenizer ‚áê ` TokenizerModel ` new WordPieceTokenizer(config) wordPieceTokenizer.tokens_to_ids : ` Map. < string, number > ` wordPieceTokenizer.unk_token_id : ` number ` wordPieceTokenizer.unk_token : ` string ` wordPieceTokenizer.max_input_chars_per_word : ` number ` wordPieceTokenizer.vocab : ` Array. < string > ` wordPieceTokenizer.encode(tokens) ‚áí ` Array. < string > ` tokenizers~Unigram ‚áê ` TokenizerModel ` new Unigram(config, moreConfig) unigram.populateNodes(lattice) unigram.tokenize(normalized) ‚áí ` Array. < string > ` unigram.encode(tokens) ‚áí ` Array. < string > ` tokenizers~BPE ‚áê ` TokenizerModel ` new BPE(config) bpE.tokens_to_ids : ` Map. < string, number > ` bpE.merges : ` * ` merges.config.merges : ` * ` bpE.cache : ` Map. < string, Array < string > > ` bpE.bpe(token) ‚áí ` Array. < string > ` bpE.encode(tokens) ‚áí ` Array. < string > ` tokenizers~LegacyTokenizerModel new LegacyTokenizerModel(config, moreConfig) legacyTokenizerModel.tokens_to_ids : ` Map. < string, number > ` tokenizers~Normalizer new Normalizer(config) normalizer.normalize(text) ‚áí ` string ` normalizer._call(text) ‚áí ` string ` Normalizer.fromConfig(config) ‚áí ` Normalizer ` tokenizers~Replace ‚áê ` Normalizer ` replace.normalize(text) ‚áí ` string ` tokenizers~NFC ‚áê ` Normalizer ` nfC.normalize(text) ‚áí ` string ` tokenizers~NFKC ‚áê ` Normalizer ` nfkC.normalize(text) ‚áí ` string ` tokenizers~NFKD ‚áê ` Normalizer ` nfkD.normalize(text) ‚áí ` string ` tokenizers~StripNormalizer stripNormalizer.normalize(text) ‚áí ` string ` tokenizers~StripAccents ‚áê ` Normalizer ` stripAccents.normalize(text) ‚áí ` string ` tokenizers~Lowercase ‚áê ` Normalizer ` lowercase.normalize(text) ‚áí ` string ` tokenizers~Prepend ‚áê ` Normalizer ` prepend.normalize(text) ‚áí ` string ` tokenizers~NormalizerSequence ‚áê ` Normalizer ` new NormalizerSequence(config) normalizerSequence.normalize(text) ‚áí ` string ` tokenizers~BertNormalizer ‚áê ` Normalizer ` bertNormalizer._tokenize_chinese_chars(text) ‚áí ` string ` bertNormalizer.stripAccents(text) ‚áí ` string ` bertNormalizer.normalize(text) ‚áí ` string ` tokenizers~PreTokenizer ‚áê ` Callable ` preTokenizer.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` preTokenizer.pre_tokenize(text, [options]) ‚áí ` Array. < string > ` preTokenizer._call(text, [options]) ‚áí ` Array. < string > ` PreTokenizer.fromConfig(config) ‚áí ` PreTokenizer ` tokenizers~BertPreTokenizer ‚áê ` PreTokenizer ` new BertPreTokenizer(config) bertPreTokenizer.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~ByteLevelPreTokenizer ‚áê ` PreTokenizer ` new ByteLevelPreTokenizer(config) byteLevelPreTokenizer.add_prefix_space : ` boolean ` byteLevelPreTokenizer.trim_offsets : ` boolean ` byteLevelPreTokenizer.use_regex : ` boolean ` byteLevelPreTokenizer.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~SplitPreTokenizer ‚áê ` PreTokenizer ` new SplitPreTokenizer(config) splitPreTokenizer.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~PunctuationPreTokenizer ‚áê ` PreTokenizer ` new PunctuationPreTokenizer(config) punctuationPreTokenizer.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~DigitsPreTokenizer ‚áê ` PreTokenizer ` new DigitsPreTokenizer(config) digitsPreTokenizer.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~PostProcessor ‚áê ` Callable ` new PostProcessor(config) postProcessor.post_process(tokens, ...args) ‚áí ` PostProcessedOutput ` postProcessor._call(tokens, ...args) ‚áí ` PostProcessedOutput ` PostProcessor.fromConfig(config) ‚áí ` PostProcessor ` tokenizers~BertProcessing new BertProcessing(config) bertProcessing.post_process(tokens, [tokens_pair]) ‚áí ` PostProcessedOutput ` tokenizers~TemplateProcessing ‚áê ` PostProcessor ` new TemplateProcessing(config) templateProcessing.post_process(tokens, [tokens_pair]) ‚áí ` PostProcessedOutput ` tokenizers~ByteLevelPostProcessor ‚áê ` PostProcessor ` byteLevelPostProcessor.post_process(tokens, [tokens_pair]) ‚áí ` PostProcessedOutput ` tokenizers~PostProcessorSequence new PostProcessorSequence(config) postProcessorSequence.post_process(tokens, [tokens_pair]) ‚áí ` PostProcessedOutput ` tokenizers~Decoder ‚áê ` Callable ` new Decoder(config) decoder.added_tokens : ` Array. < AddedToken > ` decoder._call(tokens) ‚áí ` string ` decoder.decode(tokens) ‚áí ` string ` decoder.decode_chain(tokens) ‚áí ` Array. < string > ` Decoder.fromConfig(config) ‚áí ` Decoder ` tokenizers~FuseDecoder fuseDecoder.decode_chain() : ` * ` tokenizers~WordPieceDecoder ‚áê ` Decoder ` new WordPieceDecoder(config) wordPieceDecoder.decode_chain() : ` * ` tokenizers~ByteLevelDecoder ‚áê ` Decoder ` new ByteLevelDecoder(config) byteLevelDecoder.convert_tokens_to_string(tokens) ‚áí ` string ` byteLevelDecoder.decode_chain() : ` * ` tokenizers~CTCDecoder ctcDecoder.convert_tokens_to_string(tokens) ‚áí ` string ` ctcDecoder.decode_chain() : ` * ` tokenizers~DecoderSequence ‚áê ` Decoder ` new DecoderSequence(config) decoderSequence.decode_chain() : ` * ` tokenizers~MetaspacePreTokenizer ‚áê ` PreTokenizer ` new MetaspacePreTokenizer(config) metaspacePreTokenizer.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~MetaspaceDecoder ‚áê ` Decoder ` new MetaspaceDecoder(config) metaspaceDecoder.decode_chain() : ` * ` tokenizers~Precompiled ‚áê ` Normalizer ` new Precompiled(config) precompiled.normalize(text) ‚áí ` string ` tokenizers~PreTokenizerSequence ‚áê ` PreTokenizer ` new PreTokenizerSequence(config) preTokenizerSequence.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~WhitespacePreTokenizer new WhitespacePreTokenizer(config) whitespacePreTokenizer.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~WhitespaceSplit ‚áê ` PreTokenizer ` new WhitespaceSplit(config) whitespaceSplit.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~ReplacePreTokenizer new ReplacePreTokenizer(config) replacePreTokenizer.pre_tokenize_text(text, [options]) ‚áí ` Array. < string > ` tokenizers~BYTES_TO_UNICODE ‚áí ` Object ` tokenizers~loadTokenizer(pretrained_model_name_or_path, options) ‚áí ` Promise. < Array < any > > ` tokenizers~regexSplit(text, regex) ‚áí ` Array. < string > ` tokenizers~createPattern(pattern, invert) ‚áí ` RegExp ` | ` null ` tokenizers~objectToMap(obj) ‚áí ` Map. < string, any > ` tokenizers~prepareTensorForDecode(tensor) ‚áí ` Array. < number > ` tokenizers~clean_up_tokenization(text) ‚áí ` string ` tokenizers~remove_accents(text) ‚áí ` string ` tokenizers~lowercase_and_remove_accent(text) ‚áí ` string ` tokenizers~whitespace_split(text) ‚áí ` Array. < string > ` tokenizers~PretrainedTokenizerOptions : ` Object ` tokenizers~BPENode : ` Object ` tokenizers~SplitDelimiterBehavior : ` ‚Äô removed ‚Äô ` | ` ‚Äô isolated ‚Äô ` | ` ‚Äô mergedWithPrevious ‚Äô ` | ` ‚Äô mergedWithNext ‚Äô ` | ` ‚Äô contiguous ‚Äô ` tokenizers~PostProcessedOutput : ` Object ` tokenizers~EncodingSingle : ` Object ` tokenizers~Message : ` Object ` tokenizers~BatchEncoding : ` Array < number > ` | ` Array < Array < number > > ` | ` Tensor `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

transformers

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  transformers

Entry point for the Transformers.js library. Only the exports from this file
are available to the end user, and are grouped as follows:

  1. [Pipelines](./pipelines)
  2. [Environment variables](./env)
  3. [Models](./models)
  4. [Tokenizers](./tokenizers)
  5. [Processors](./processors)

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/transformers.md)

[‚ÜêServer-side Audio Processing in
Node.js](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)
[Pipelines‚Üí](/docs/transformers.js/v3.0.0/en/api/pipelines)

transformers

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

utils/audio

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

[Core ](/docs/transformers.js/v3.0.0/en/api/utils/core)[Hub
](/docs/transformers.js/v3.0.0/en/api/utils/hub)[Image
](/docs/transformers.js/v3.0.0/en/api/utils/image)[Audio
](/docs/transformers.js/v3.0.0/en/api/utils/audio)[Tensor
](/docs/transformers.js/v3.0.0/en/api/utils/tensor)[Maths
](/docs/transformers.js/v3.0.0/en/api/utils/maths)[Data Structures
](/docs/transformers.js/v3.0.0/en/api/utils/data-structures)

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  utils/audio

Helper module for audio processing.

These functions and classes are only used internally, meaning an end-user
shouldn‚Äôt need to access anything here.

  * utils/audio
    *  _static_
      * `.read_audio(url, sampling_rate)` ‚áí `Promise.<Float32Array>`
        * `~audio` : `Float32Array`
      * `.hanning(M)` ‚áí `Float64Array`
      * `.hamming(M)` ‚áí `Float64Array`
      * `.mel_filter_bank(num_frequency_bins, num_mel_filters, min_frequency, max_frequency, sampling_rate, [norm], [mel_scale], [triangularize_in_mel_space])` ‚áí `Array.<Array<number>>`
      * `.spectrogram(waveform, window, frame_length, hop_length, options)` ‚áí `Promise.<Tensor>`
      * `.window_function(window_length, name, options)` ‚áí `Float64Array`
    * _inner_
      * `~generalized_cosine_window(M, a_0)` ‚áí `Float64Array`
      * `~hertz_to_mel(freq, [mel_scale])` ‚áí `T`
      * `~mel_to_hertz(mels, [mel_scale])` ‚áí `T`
      * `~_create_triangular_filter_bank(fft_freqs, filter_freqs)` ‚áí `Array.<Array<number>>`
      * `~linspace(start, end, num)` ‚áí
      * `~padReflect(array, left, right)` ‚áí `T`
      * `~_db_conversion_helper(spectrogram, factor, reference, min_value, db_range)` ‚áí `T`
      * `~amplitude_to_db(spectrogram, [reference], [min_value], [db_range])` ‚áí `T`
      * `~power_to_db(spectrogram, [reference], [min_value], [db_range])` ‚áí `T`

* * *

##  utils/audio.read_audio(url, sampling_rate) ‚áí <code> Promise. <
Float32Array > </code>

Helper function to read audio from a path/URL.

**Kind** : static method of `utils/audio`  
**Returns** : `Promise.<Float32Array>` \- The decoded audio as a
`Float32Array`.

Param| Type| Description  
---|---|---  
url| `string` | `URL`| The path/URL to load the audio from.  
sampling_rate| `number`| The sampling rate to use when decoding the audio.  
  
* * *

###  read_audio~audio : <code> Float32Array </code>

**Kind** : inner property of `read_audio`

* * *

##  utils/audio.hanning(M) ‚áí <code> Float64Array </code>

Generates a Hanning window of length M. See
<https://numpy.org/doc/stable/reference/generated/numpy.hanning.html> for more
information.

**Kind** : static method of `utils/audio`  
**Returns** : `Float64Array` \- The generated Hanning window.

Param| Type| Description  
---|---|---  
M| `number`| The length of the Hanning window to generate.  
  
* * *

##  utils/audio.hamming(M) ‚áí <code> Float64Array </code>

Generates a Hamming window of length M. See
<https://numpy.org/doc/stable/reference/generated/numpy.hamming.html> for more
information.

**Kind** : static method of `utils/audio`  
**Returns** : `Float64Array` \- The generated Hamming window.

Param| Type| Description  
---|---|---  
M| `number`| The length of the Hamming window to generate.  
  
* * *

##  utils/audio.mel_filter_bank(num_frequency_bins, num_mel_filters,
min_frequency, max_frequency, sampling_rate, [norm], [mel_scale],
[triangularize_in_mel_space]) ‚áí <code> Array. < Array < number > > </code>

Creates a frequency bin conversion matrix used to obtain a mel spectrogram.
This is called a _mel filter bank_ , and various implementation exist, which
differ in the number of filters, the shape of the filters, the way the filters
are spaced, the bandwidth of the filters, and the manner in which the spectrum
is warped. The goal of these features is to approximate the non-linear human
perception of the variation in pitch with respect to the frequency.

**Kind** : static method of `utils/audio`  
**Returns** : `Array.<Array<number>>` \- Triangular filter bank matrix, which
is a 2D array of shape (`num_frequency_bins`, `num_mel_filters`). This is a
projection matrix to go from a spectrogram to a mel spectrogram.

Param| Type| Description  
---|---|---  
num_frequency_bins| `number`| Number of frequencies used to compute the
spectrogram (should be the same as in `stft`).  
num_mel_filters| `number`| Number of mel filters to generate.  
min_frequency| `number`| Lowest frequency of interest in Hz.  
max_frequency| `number`| Highest frequency of interest in Hz. This should not
exceed `sampling_rate / 2`.  
sampling_rate| `number`| Sample rate of the audio waveform.  
[norm]| `string`| If `"slaney"`, divide the triangular mel weights by the
width of the mel band (area normalization).  
[mel_scale]| `string`| The mel frequency scale to use, `"htk"` or `"slaney"`.  
[triangularize_in_mel_space]| `boolean`| If this option is enabled, the
triangular filter is applied in mel space rather than frequency space. This
should be set to `true` in order to get the same results as `torchaudio` when
computing mel filters.  
  
* * *

##  utils/audio.spectrogram(waveform, window, frame_length, hop_length,
options) ‚áí <code> Promise. < Tensor > </code>

Calculates a spectrogram over one waveform using the Short-Time Fourier
Transform.

This function can create the following kinds of spectrograms:

  * amplitude spectrogram (`power = 1.0`)
  * power spectrogram (`power = 2.0`)
  * complex-valued spectrogram (`power = None`)
  * log spectrogram (use `log_mel` argument)
  * mel spectrogram (provide `mel_filters`)
  * log-mel spectrogram (provide `mel_filters` and `log_mel`)

In this implementation, the window is assumed to be zero-padded to have the
same size as the analysis frame. A padded window can be obtained from
`window_function()`. The FFT input buffer may be larger than the analysis
frame, typically the next power of two.

**Kind** : static method of `utils/audio`  
**Returns** : `Promise.<Tensor>` \- Spectrogram of shape `(num_frequency_bins,
length)` (regular spectrogram) or shape `(num_mel_filters, length)` (mel
spectrogram).

Param| Type| Default| Description  
---|---|---|---  
waveform| `Float32Array` | `Float64Array`| | The input waveform of shape `(length,)`. This must be a single real-valued, mono waveform.  
window| `Float32Array` | `Float64Array`| | The windowing function to apply of shape `(frame_length,)`, including zero-padding if necessary. The actual window length may be shorter than `frame_length`, but we're assuming the array has already been zero-padded.  
frame_length| `number`| | The length of the analysis frames in samples (a.k.a., `fft_length`).  
hop_length| `number`| | The stride between successive analysis frames in samples.  
options| `Object`| |   
[options.fft_length]| `number`| ``| The size of the FFT buffer in samples.
This determines how many frequency bins the spectrogram will have. For optimal
speed, this should be a power of two. If `null`, uses `frame_length`.  
[options.power]| `number`| `1.0`| If 1.0, returns the amplitude spectrogram.
If 2.0, returns the power spectrogram. If `null`, returns complex numbers.  
[options.center]| `boolean`| `true`| Whether to pad the waveform so that frame
`t` is centered around time `t * hop_length`. If `false`, frame `t` will start
at time `t * hop_length`.  
[options.pad_mode]| `string`| `""reflect""`| Padding mode used when `center`
is `true`. Possible values are: `"constant"` (pad with zeros), `"edge"` (pad
with edge values), `"reflect"` (pads with mirrored values).  
[options.onesided]| `boolean`| `true`| If `true`, only computes the positive
frequencies and returns a spectrogram containing `fft_length // 2 + 1`
frequency bins. If `false`, also computes the negative frequencies and returns
`fft_length` frequency bins.  
[options.preemphasis]| `number`| ``| Coefficient for a low-pass filter that
applies pre-emphasis before the DFT.  
[options.mel_filters]| `Array.<Array<number>>`| ``| The mel filter bank of
shape `(num_freq_bins, num_mel_filters)`. If supplied, applies this filter
bank to create a mel spectrogram.  
[options.mel_floor]| `number`| `1e-10`| Minimum value of mel frequency banks.  
[options.log_mel]| `string`| `null`| How to convert the spectrogram to log
scale. Possible options are: `null` (don't convert), `"log"` (take the natural
logarithm) `"log10"` (take the base-10 logarithm), `"dB"` (convert to
decibels). Can only be used when `power` is not `null`.  
[options.reference]| `number`| `1.0`| Sets the input spectrogram value that
corresponds to 0 dB. For example, use `max(spectrogram)[0]` to set the loudest
part to 0 dB. Must be greater than zero.  
[options.min_value]| `number`| `1e-10`| The spectrogram will be clipped to
this minimum value before conversion to decibels, to avoid taking `log(0)`.
For a power spectrogram, the default of `1e-10` corresponds to a minimum of
-100 dB. For an amplitude spectrogram, the value `1e-5` corresponds to -100
dB. Must be greater than zero.  
[options.db_range]| `number`| ``| Sets the maximum dynamic range in decibels.
For example, if `db_range = 80`, the difference between the peak value and the
smallest value will never be more than 80 dB. Must be greater than zero.  
[options.remove_dc_offset]| `boolean`| ``| Subtract mean from waveform on each
frame, applied before pre-emphasis. This should be set to `true` in order to
get the same results as `torchaudio.compliance.kaldi.fbank` when computing mel
filters.  
[options.max_num_frames]| `number`| ``| If provided, limits the number of
frames to compute to this value.  
[options.min_num_frames]| `number`| ``| If provided, ensures the number of
frames to compute is at least this value.  
[options.do_pad]| `boolean`| `true`| If `true`, pads the output spectrogram to
have `max_num_frames` frames.  
[options.transpose]| `boolean`| `false`| If `true`, the returned spectrogram
will have shape `(num_frames, num_frequency_bins/num_mel_filters)`. If
`false`, the returned spectrogram will have shape
`(num_frequency_bins/num_mel_filters, num_frames)`.  
  
* * *

##  utils/audio.window_function(window_length, name, options) ‚áí <code>
Float64Array </code>

Returns an array containing the specified window.

**Kind** : static method of `utils/audio`  
**Returns** : `Float64Array` \- The window of shape `(window_length,)` or
`(frame_length,)`.

Param| Type| Default| Description  
---|---|---|---  
window_length| `number`| | The length of the window in samples.  
name| `string`| | The name of the window function.  
options| `Object`| | Additional options.  
[options.periodic]| `boolean`| `true`| Whether the window is periodic or
symmetric.  
[options.frame_length]| `number`| ``| The length of the analysis frames in
samples. Provide a value for `frame_length` if the window is smaller than the
frame length, so that it will be zero-padded.  
[options.center]| `boolean`| `true`| Whether to center the window inside the
FFT buffer. Only used when `frame_length` is provided.  
  
* * *

##  utils/audio~generalized_cosine_window(M, a_0) ‚áí <code> Float64Array
</code>

Helper function to generate windows that are special cases of the generalized
cosine window. See <https://www.mathworks.com/help/signal/ug/generalized-
cosine-windows.html> for more information.

**Kind** : inner method of `utils/audio`  
**Returns** : `Float64Array` \- The generated window.

Param| Type| Description  
---|---|---  
M| `number`| Number of points in the output window. If zero or less, an empty
array is returned.  
a_0| `number`| Offset for the generalized cosine window.  
  
* * *

##  utils/audio~hertz_to_mel(freq, [mel_scale]) ‚áí <code> T </code>

**Kind** : inner method of `utils/audio`

Param| Type| Default  
---|---|---  
freq| `T`|  
[mel_scale]| `string`| `"htk"`  
  
* * *

##  utils/audio~mel_to_hertz(mels, [mel_scale]) ‚áí <code> T </code>

**Kind** : inner method of `utils/audio`

Param| Type| Default  
---|---|---  
mels| `T`|  
[mel_scale]| `string`| `"htk"`  
  
* * *

##  utils/audio~_create_triangular_filter_bank(fft_freqs, filter_freqs) ‚áí
<code> Array. < Array < number > > </code>

Creates a triangular filter bank.

Adapted from torchaudio and librosa.

**Kind** : inner method of `utils/audio`  
**Returns** : `Array.<Array<number>>` \- of shape `(num_frequency_bins,
num_mel_filters)`.

Param| Type| Description  
---|---|---  
fft_freqs| `Float64Array`| Discrete frequencies of the FFT bins in Hz, of
shape `(num_frequency_bins,)`.  
filter_freqs| `Float64Array`| Center frequencies of the triangular filters to
create, in Hz, of shape `(num_mel_filters,)`.  
  
* * *

##  utils/audio~linspace(start, end, num) ‚áí

Return evenly spaced numbers over a specified interval.

**Kind** : inner method of `utils/audio`  
**Returns** : `num` evenly spaced samples, calculated over the interval
`[start, stop]`.

Param| Type| Description  
---|---|---  
start| `number`| The starting value of the sequence.  
end| `number`| The end value of the sequence.  
num| `number`| Number of samples to generate.  
  
* * *

##  utils/audio~padReflect(array, left, right) ‚áí <code> T </code>

**Kind** : inner method of `utils/audio`  
**Returns** : `T` \- The padded array.

Param| Type| Description  
---|---|---  
array| `T`| The array to pad.  
left| `number`| The amount of padding to add to the left.  
right| `number`| The amount of padding to add to the right.  
  
* * *

##  utils/audio~_db_conversion_helper(spectrogram, factor, reference,
min_value, db_range) ‚áí <code> T </code>

Helper function to compute `amplitude_to_db` and `power_to_db`.

**Kind** : inner method of `utils/audio`

Param| Type  
---|---  
spectrogram| `T`  
factor| `number`  
reference| `number`  
min_value| `number`  
db_range| `number`  
  
* * *

##  utils/audio~amplitude_to_db(spectrogram, [reference], [min_value],
[db_range]) ‚áí <code> T </code>

Converts an amplitude spectrogram to the decibel scale. This computes `20 *
log10(spectrogram / reference)`, using basic logarithm properties for
numerical stability. NOTE: Operates in-place.

The motivation behind applying the log function on the (mel) spectrogram is
that humans do not hear loudness on a linear scale. Generally to double the
perceived volume of a sound we need to put 8 times as much energy into it.
This means that large variations in energy may not sound all that different if
the sound is loud to begin with. This compression operation makes the (mel)
spectrogram features match more closely what humans actually hear.

**Kind** : inner method of `utils/audio`  
**Returns** : `T` \- The modified spectrogram in decibels.

Param| Type| Default| Description  
---|---|---|---  
spectrogram| `T`| | The input amplitude (mel) spectrogram.  
[reference]| `number`| `1.0`| Sets the input spectrogram value that
corresponds to 0 dB. For example, use `np.max(spectrogram)` to set the loudest
part to 0 dB. Must be greater than zero.  
[min_value]| `number`| `1e-5`| The spectrogram will be clipped to this minimum
value before conversion to decibels, to avoid taking `log(0)`. The default of
`1e-5` corresponds to a minimum of -100 dB. Must be greater than zero.  
[db_range]| `number`| ``| Sets the maximum dynamic range in decibels. For
example, if `db_range = 80`, the difference between the peak value and the
smallest value will never be more than 80 dB. Must be greater than zero.  
  
* * *

##  utils/audio~power_to_db(spectrogram, [reference], [min_value], [db_range])
‚áí <code> T </code>

Converts a power spectrogram to the decibel scale. This computes `10 *
log10(spectrogram / reference)`, using basic logarithm properties for
numerical stability. NOTE: Operates in-place.

The motivation behind applying the log function on the (mel) spectrogram is
that humans do not hear loudness on a linear scale. Generally to double the
perceived volume of a sound we need to put 8 times as much energy into it.
This means that large variations in energy may not sound all that different if
the sound is loud to begin with. This compression operation makes the (mel)
spectrogram features match more closely what humans actually hear.

Based on the implementation of `librosa.power_to_db`.

**Kind** : inner method of `utils/audio`  
**Returns** : `T` \- The modified spectrogram in decibels.

Param| Type| Default| Description  
---|---|---|---  
spectrogram| `T`| | The input power (mel) spectrogram. Note that a power spectrogram has the amplitudes squared!  
[reference]| `number`| `1.0`| Sets the input spectrogram value that
corresponds to 0 dB. For example, use `np.max(spectrogram)` to set the loudest
part to 0 dB. Must be greater than zero.  
[min_value]| `number`| `1e-10`| The spectrogram will be clipped to this
minimum value before conversion to decibels, to avoid taking `log(0)`. The
default of `1e-10` corresponds to a minimum of -100 dB. Must be greater than
zero.  
[db_range]| `number`| ``| Sets the maximum dynamic range in decibels. For
example, if `db_range = 80`, the difference between the peak value and the
smallest value will never be more than 80 dB. Must be greater than zero.  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/utils/audio.md)

[‚ÜêImage](/docs/transformers.js/v3.0.0/en/api/utils/image)
[Tensor‚Üí](/docs/transformers.js/v3.0.0/en/api/utils/tensor)

utils/audio utils/audio.read_audio(url, sampling_rate) ‚áí ` Promise. <
Float32Array > ` read_audio~audio : ` Float32Array ` utils/audio.hanning(M) ‚áí
` Float64Array ` utils/audio.hamming(M) ‚áí ` Float64Array `
utils/audio.mel_filter_bank(num_frequency_bins, num_mel_filters,
min_frequency, max_frequency, sampling_rate, [norm], [mel_scale],
[triangularize_in_mel_space]) ‚áí ` Array. < Array < number > > `
utils/audio.spectrogram(waveform, window, frame_length, hop_length, options) ‚áí
` Promise. < Tensor > ` utils/audio.window_function(window_length, name,
options) ‚áí ` Float64Array ` utils/audio~generalized_cosine_window(M, a_0) ‚áí `
Float64Array ` utils/audio~hertz_to_mel(freq, [mel_scale]) ‚áí ` T `
utils/audio~mel_to_hertz(mels, [mel_scale]) ‚áí ` T `
utils/audio~_create_triangular_filter_bank(fft_freqs, filter_freqs) ‚áí ` Array.
< Array < number > > ` utils/audio~linspace(start, end, num) ‚áí
utils/audio~padReflect(array, left, right) ‚áí ` T `
utils/audio~_db_conversion_helper(spectrogram, factor, reference, min_value,
db_range) ‚áí ` T ` utils/audio~amplitude_to_db(spectrogram, [reference],
[min_value], [db_range]) ‚áí ` T ` utils/audio~power_to_db(spectrogram,
[reference], [min_value], [db_range]) ‚áí ` T `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

utils/core

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

[Core ](/docs/transformers.js/v3.0.0/en/api/utils/core)[Hub
](/docs/transformers.js/v3.0.0/en/api/utils/hub)[Image
](/docs/transformers.js/v3.0.0/en/api/utils/image)[Audio
](/docs/transformers.js/v3.0.0/en/api/utils/audio)[Tensor
](/docs/transformers.js/v3.0.0/en/api/utils/tensor)[Maths
](/docs/transformers.js/v3.0.0/en/api/utils/maths)[Data Structures
](/docs/transformers.js/v3.0.0/en/api/utils/data-structures)

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  utils/core

Core utility functions/classes for Transformers.js.

These are only used internally, meaning an end-user shouldn‚Äôt need to access
anything here.

  * utils/core
    * `.reverseDictionary(data)` ‚áí `Object`
    * `.escapeRegExp(string)` ‚áí `string`
    * `.isTypedArray(val)` ‚áí `boolean`
    * `.isIntegralNumber(x)` ‚áí `boolean`
    * `.calculateDimensions(arr)` ‚áí `Array.<number>`
    * `.pop(obj, key, defaultValue)` ‚áí `*`
    * `.mergeArrays(arrs)` ‚áí `Array`
    * `.calculateReflectOffset(i, w)` ‚áí `number`
    * `.pick(o, props)` ‚áí `Object`
    * `.len(s)` ‚áí `number`

* * *

##  utils/core.reverseDictionary(data) ‚áí <code> Object </code>

Reverses the keys and values of an object.

**Kind** : static method of `utils/core`  
**Returns** : `Object` \- The reversed object.  
**See** : <https://ultimatecourses.com/blog/reverse-object-keys-and-values-in-
javascript>

Param| Type| Description  
---|---|---  
data| `Object`| The object to reverse.  
  
* * *

##  utils/core.escapeRegExp(string) ‚áí <code> string </code>

Escapes regular expression special characters from a string by replacing them
with their escaped counterparts.

**Kind** : static method of `utils/core`  
**Returns** : `string` \- The escaped string.

Param| Type| Description  
---|---|---  
string| `string`| The string to escape.  
  
* * *

##  utils/core.isTypedArray(val) ‚áí <code> boolean </code>

Check if a value is a typed array.

**Kind** : static method of `utils/core`  
**Returns** : `boolean` \- True if the value is a `TypedArray`, false
otherwise.

Adapted from <https://stackoverflow.com/a/71091338/13989043>

Param| Type| Description  
---|---|---  
val| `*`| The value to check.  
  
* * *

##  utils/core.isIntegralNumber(x) ‚áí <code> boolean </code>

Check if a value is an integer.

**Kind** : static method of `utils/core`  
**Returns** : `boolean` \- True if the value is a string, false otherwise.

Param| Type| Description  
---|---|---  
x| `*`| The value to check.  
  
* * *

##  utils/core.calculateDimensions(arr) ‚áí <code> Array. < number > </code>

Calculates the dimensions of a nested array.

**Kind** : static method of `utils/core`  
**Returns** : `Array.<number>` \- An array containing the dimensions of the
input array.

Param| Type| Description  
---|---|---  
arr| `Array.<any>`| The nested array to calculate dimensions for.  
  
* * *

##  utils/core.pop(obj, key, defaultValue) ‚áí <code> * </code>

Replicate python‚Äôs .pop() method for objects.

**Kind** : static method of `utils/core`  
**Returns** : `*` \- The value of the popped key.  
**Throws** :

  * `Error` If the key does not exist and no default value is provided.

Param| Type| Description  
---|---|---  
obj| `Object`| The object to pop from.  
key| `string`| The key to pop.  
defaultValue| `*`| The default value to return if the key does not exist.  
  
* * *

##  utils/core.mergeArrays(arrs) ‚áí <code> Array </code>

Efficiently merge arrays, creating a new copy. Adapted from
<https://stackoverflow.com/a/6768642/13989043>

**Kind** : static method of `utils/core`  
**Returns** : `Array` \- The merged array.

Param| Type| Description  
---|---|---  
arrs| `Array.<Array>`| Arrays to merge.  
  
* * *

##  utils/core.calculateReflectOffset(i, w) ‚áí <code> number </code>

Calculates the index offset for a given index and window size.

**Kind** : static method of `utils/core`  
**Returns** : `number` \- The index offset.

Param| Type| Description  
---|---|---  
i| `number`| The index.  
w| `number`| The window size.  
  
* * *

##  utils/core.pick(o, props) ‚áí <code> Object </code>

**Kind** : static method of `utils/core`

Param| Type  
---|---  
o| `Object`  
props| `Array.<string>`  
  
* * *

##  utils/core.len(s) ‚áí <code> number </code>

Calculate the length of a string, taking multi-byte characters into account.
This mimics the behavior of Python‚Äôs `len` function.

**Kind** : static method of `utils/core`  
**Returns** : `number` \- The length of the string.

Param| Type| Description  
---|---|---  
s| `string`| The string to calculate the length of.  
  
* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/utils/core.md)

[‚ÜêStreamers](/docs/transformers.js/v3.0.0/en/api/generation/streamers)
[Hub‚Üí](/docs/transformers.js/v3.0.0/en/api/utils/hub)

utils/core utils/core.reverseDictionary(data) ‚áí ` Object `
utils/core.escapeRegExp(string) ‚áí ` string ` utils/core.isTypedArray(val) ‚áí `
boolean ` utils/core.isIntegralNumber(x) ‚áí ` boolean `
utils/core.calculateDimensions(arr) ‚áí ` Array. < number > `
utils/core.pop(obj, key, defaultValue) ‚áí ` * ` utils/core.mergeArrays(arrs) ‚áí
` Array ` utils/core.calculateReflectOffset(i, w) ‚áí ` number `
utils/core.pick(o, props) ‚áí ` Object ` utils/core.len(s) ‚áí ` number `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

utils/data-structures

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

[Core ](/docs/transformers.js/v3.0.0/en/api/utils/core)[Hub
](/docs/transformers.js/v3.0.0/en/api/utils/hub)[Image
](/docs/transformers.js/v3.0.0/en/api/utils/image)[Audio
](/docs/transformers.js/v3.0.0/en/api/utils/audio)[Tensor
](/docs/transformers.js/v3.0.0/en/api/utils/tensor)[Maths
](/docs/transformers.js/v3.0.0/en/api/utils/maths)[Data Structures
](/docs/transformers.js/v3.0.0/en/api/utils/data-structures)

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  utils/data-structures

Custom data structures.

These are only used internally, meaning an end-user shouldn‚Äôt need to access
anything here.

  * utils/data-structures
    *  _static_
      * .PriorityQueue
        * `new PriorityQueue(comparator)`
        * `.size`
        * `.isEmpty()` ‚áí `boolean`
        * `.peek()` ‚áí `any`
        * `.push(...values)` ‚áí `number`
        * `.extend(values)` ‚áí `number`
        * `.pop()` ‚áí `any`
        * `.replace(value)` ‚áí `*`
        * `._siftUpFrom(node)`
      * .CharTrie
        * `.extend(texts)`
        * `.push(text)`
        * `.commonPrefixSearch(text)`
      * .TokenLattice
        * `new TokenLattice(sentence, bosTokenId, eosTokenId)`
        * `.insert(pos, length, score, tokenId)`
        * `.viterbi()` ‚áí `Array.<TokenLatticeNode>`
        * `.piece(node)` ‚áí `string`
        * `.tokens()` ‚áí `Array.<string>`
        * `.tokenIds()` ‚áí `Array.<number>`
    * _inner_
      * ~CharTrieNode
        * `new CharTrieNode(isLeaf, children)`
        * `.default()` ‚áí `CharTrieNode`
      * ~TokenLatticeNode
        * `new TokenLatticeNode(tokenId, nodeId, pos, length, score)`
        * `.clone()` ‚áí `TokenLatticeNode`

* * *

##  utils/data-structures.PriorityQueue

Efficient Heap-based Implementation of a Priority Queue. It uses an array-
based binary heap, where the root is at index `0`, and the children of node
`i` are located at indices `2i + 1` and `2i + 2`, respectively.

Adapted from the following sources:

  * <https://stackoverflow.com/a/42919752/13989043> (original)
  * <https://github.com/belladoreai/llama-tokenizer-js> (minor improvements)

**Kind** : static class of `utils/data-structures`

  * .PriorityQueue
    * `new PriorityQueue(comparator)`
    * `.size`
    * `.isEmpty()` ‚áí `boolean`
    * `.peek()` ‚áí `any`
    * `.push(...values)` ‚áí `number`
    * `.extend(values)` ‚áí `number`
    * `.pop()` ‚áí `any`
    * `.replace(value)` ‚áí `*`
    * `._siftUpFrom(node)`

* * *

###  new PriorityQueue(comparator)

Create a new PriorityQueue.

Param| Type| Description  
---|---|---  
comparator| `function`| Comparator function to determine priority. Defaults to
a MaxHeap.  
  
* * *

###  priorityQueue.size

The size of the queue

**Kind** : instance property of `PriorityQueue`

* * *

###  priorityQueue.isEmpty() ‚áí <code> boolean </code>

Check if the queue is empty.

**Kind** : instance method of `PriorityQueue`  
**Returns** : `boolean` \- `true` if the queue is empty, `false` otherwise.

* * *

###  priorityQueue.peek() ‚áí <code> any </code>

Return the element with the highest priority in the queue.

**Kind** : instance method of `PriorityQueue`  
**Returns** : `any` \- The highest priority element in the queue.

* * *

###  priorityQueue.push(...values) ‚áí <code> number </code>

Add one or more elements to the queue.

**Kind** : instance method of `PriorityQueue`  
**Returns** : `number` \- The new size of the queue.

Param| Type| Description  
---|---|---  
...values| `any`| The values to push into the queue.  
  
* * *

###  priorityQueue.extend(values) ‚áí <code> number </code>

Add multiple elements to the queue.

**Kind** : instance method of `PriorityQueue`  
**Returns** : `number` \- The new size of the queue.

Param| Type| Description  
---|---|---  
values| `Array.<any>`| The values to push into the queue.  
  
* * *

###  priorityQueue.pop() ‚áí <code> any </code>

Remove and return the element with the highest priority in the queue.

**Kind** : instance method of `PriorityQueue`  
**Returns** : `any` \- The element with the highest priority in the queue.

* * *

###  priorityQueue.replace(value) ‚áí <code> * </code>

Replace the element with the highest priority in the queue with a new value.

**Kind** : instance method of `PriorityQueue`  
**Returns** : `*` \- The replaced value.

Param| Type| Description  
---|---|---  
value| `*`| The new value.  
  
* * *

###  priorityQueue._siftUpFrom(node)

Helper function to sift up from a given node.

**Kind** : instance method of `PriorityQueue`

Param| Type| Description  
---|---|---  
node| `number`| The index of the node to start sifting up from.  
  
* * *

##  utils/data-structures.CharTrie

A trie structure to efficiently store and search for strings.

**Kind** : static class of `utils/data-structures`

  * .CharTrie
    * `.extend(texts)`
    * `.push(text)`
    * `.commonPrefixSearch(text)`

* * *

###  charTrie.extend(texts)

Adds one or more `texts` to the trie.

**Kind** : instance method of `CharTrie`

Param| Type| Description  
---|---|---  
texts| `Array.<string>`| The strings to add to the trie.  
  
* * *

###  charTrie.push(text)

Adds text to the trie.

**Kind** : instance method of `CharTrie`

Param| Type| Description  
---|---|---  
text| `string`| The string to add to the trie.  
  
* * *

###  charTrie.commonPrefixSearch(text)

Searches the trie for all strings with a common prefix of `text`.

**Kind** : instance method of `CharTrie`

Param| Type| Description  
---|---|---  
text| `string`| The common prefix to search for.  
  
* * *

##  utils/data-structures.TokenLattice

A lattice data structure to be used for tokenization.

**Kind** : static class of `utils/data-structures`

  * .TokenLattice
    * `new TokenLattice(sentence, bosTokenId, eosTokenId)`
    * `.insert(pos, length, score, tokenId)`
    * `.viterbi()` ‚áí `Array.<TokenLatticeNode>`
    * `.piece(node)` ‚áí `string`
    * `.tokens()` ‚áí `Array.<string>`
    * `.tokenIds()` ‚áí `Array.<number>`

* * *

###  new TokenLattice(sentence, bosTokenId, eosTokenId)

Creates a new TokenLattice instance.

Param| Type| Description  
---|---|---  
sentence| `string`| The input sentence to be tokenized.  
bosTokenId| `number`| The beginning-of-sequence token ID.  
eosTokenId| `number`| The end-of-sequence token ID.  
  
* * *

###  tokenLattice.insert(pos, length, score, tokenId)

Inserts a new token node into the token lattice.

**Kind** : instance method of `TokenLattice`

Param| Type| Description  
---|---|---  
pos| `number`| The starting position of the token.  
length| `number`| The length of the token.  
score| `number`| The score of the token.  
tokenId| `number`| The token ID of the token.  
  
* * *

###  tokenLattice.viterbi() ‚áí <code> Array. < TokenLatticeNode > </code>

Implements the Viterbi algorithm to compute the most likely sequence of
tokens.

**Kind** : instance method of `TokenLattice`  
**Returns** : `Array.<TokenLatticeNode>` \- The most likely sequence of
tokens.

* * *

###  tokenLattice.piece(node) ‚áí <code> string </code>

**Kind** : instance method of `TokenLattice`  
**Returns** : `string` \- The array of nodes representing the most likely
sequence of tokens.

Param| Type  
---|---  
node| `TokenLatticeNode`  
  
* * *

###  tokenLattice.tokens() ‚áí <code> Array. < string > </code>

**Kind** : instance method of `TokenLattice`  
**Returns** : `Array.<string>` \- The most likely sequence of tokens.

* * *

###  tokenLattice.tokenIds() ‚áí <code> Array. < number > </code>

**Kind** : instance method of `TokenLattice`  
**Returns** : `Array.<number>` \- The most likely sequence of token ids.

* * *

##  utils/data-structures~CharTrieNode

Represents a node in a character trie.

**Kind** : inner class of `utils/data-structures`

  * ~CharTrieNode
    * `new CharTrieNode(isLeaf, children)`
    * `.default()` ‚áí `CharTrieNode`

* * *

###  new CharTrieNode(isLeaf, children)

Create a new CharTrieNode.

Param| Type| Description  
---|---|---  
isLeaf| `boolean`| Whether the node is a leaf node or not.  
children| `Map.<string, CharTrieNode>`| A map containing the node's children,
where the key is a character and the value is a `CharTrieNode`.  
  
* * *

###  CharTrieNode.default() ‚áí <code> CharTrieNode </code>

Returns a new `CharTrieNode` instance with default values.

**Kind** : static method of `CharTrieNode`  
**Returns** : `CharTrieNode` \- A new `CharTrieNode` instance with `isLeaf`
set to `false` and an empty `children` map.

* * *

##  utils/data-structures~TokenLatticeNode

**Kind** : inner class of `utils/data-structures`

  * ~TokenLatticeNode
    * `new TokenLatticeNode(tokenId, nodeId, pos, length, score)`
    * `.clone()` ‚áí `TokenLatticeNode`

* * *

###  new TokenLatticeNode(tokenId, nodeId, pos, length, score)

Represents a node in a token lattice for a given sentence.

Param| Type| Description  
---|---|---  
tokenId| `number`| The ID of the token associated with this node.  
nodeId| `number`| The ID of this node.  
pos| `number`| The starting position of the token in the sentence.  
length| `number`| The length of the token.  
score| `number`| The score associated with the token.  
  
* * *

###  tokenLatticeNode.clone() ‚áí <code> TokenLatticeNode </code>

Returns a clone of this node.

**Kind** : instance method of `TokenLatticeNode`  
**Returns** : `TokenLatticeNode` \- A clone of this node.

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/utils/data-
structures.md)

[‚ÜêMaths](/docs/transformers.js/v3.0.0/en/api/utils/maths)

utils/data-structures utils/data-structures.PriorityQueue new
PriorityQueue(comparator) priorityQueue.size priorityQueue.isEmpty() ‚áí `
boolean ` priorityQueue.peek() ‚áí ` any ` priorityQueue.push(...values) ‚áí `
number ` priorityQueue.extend(values) ‚áí ` number ` priorityQueue.pop() ‚áí ` any
` priorityQueue.replace(value) ‚áí ` * ` priorityQueue._siftUpFrom(node)
utils/data-structures.CharTrie charTrie.extend(texts) charTrie.push(text)
charTrie.commonPrefixSearch(text) utils/data-structures.TokenLattice new
TokenLattice(sentence, bosTokenId, eosTokenId) tokenLattice.insert(pos,
length, score, tokenId) tokenLattice.viterbi() ‚áí ` Array. < TokenLatticeNode >
` tokenLattice.piece(node) ‚áí ` string ` tokenLattice.tokens() ‚áí ` Array. <
string > ` tokenLattice.tokenIds() ‚áí ` Array. < number > ` utils/data-
structures~CharTrieNode new CharTrieNode(isLeaf, children)
CharTrieNode.default() ‚áí ` CharTrieNode ` utils/data-
structures~TokenLatticeNode new TokenLatticeNode(tokenId, nodeId, pos, length,
score) tokenLatticeNode.clone() ‚áí ` TokenLatticeNode `

The documentation page API/UTILS/GENERATION doesn‚Äôt exist in v3.0.0, but
exists on the main version. Click
[here](/docs/transformers.js/main/en/api/utils/generation) to redirect to the
main version of the documentation.

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

utils/hub

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

[Core ](/docs/transformers.js/v3.0.0/en/api/utils/core)[Hub
](/docs/transformers.js/v3.0.0/en/api/utils/hub)[Image
](/docs/transformers.js/v3.0.0/en/api/utils/image)[Audio
](/docs/transformers.js/v3.0.0/en/api/utils/audio)[Tensor
](/docs/transformers.js/v3.0.0/en/api/utils/tensor)[Maths
](/docs/transformers.js/v3.0.0/en/api/utils/maths)[Data Structures
](/docs/transformers.js/v3.0.0/en/api/utils/data-structures)

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  utils/hub

Utility functions to interact with the Hugging Face Hub
(<https://huggingface.co/models>)

  * utils/hub
    *  _static_
      * `.getFile(urlOrPath)` ‚áí `Promise.<(FileResponse|Response)>`
      * `.getModelFile(path_or_repo_id, filename, [fatal], [options])` ‚áí `Promise.<Uint8Array>`
        * `~cacheKey` : `string`
        * `~response` : `Response` | `FileResponse` | `undefined`
        * `~buffer` : `Uint8Array`
      * `.getModelJSON(modelPath, fileName, [fatal], [options])` ‚áí `Promise.<Object>`
    * _inner_
      * ~FileResponse
        * `new FileResponse(filePath)`
        * `.updateContentType()` ‚áí `void`
        * `.clone()` ‚áí `FileResponse`
        * `.arrayBuffer()` ‚áí `Promise.<ArrayBuffer>`
        * `.blob()` ‚áí `Promise.<Blob>`
        * `.text()` ‚áí `Promise.<string>`
        * `.json()` ‚áí `Promise.<Object>`
      * ~FileCache
        * `new FileCache(path)`
        * `.match(request)` ‚áí `Promise.<(FileResponse|undefined)>`
        * `.put(request, response)` ‚áí `Promise.<void>`
      * `~CONTENT_TYPE_MAP`
      * `~isValidUrl(string, [protocols], [validHosts])` ‚áí `boolean`
      * `~handleError(status, remoteURL, fatal)` ‚áí `null`
      * `~tryCache(cache, ...names)` ‚áí `Promise.<(FileResponse|Response|undefined)>`
      * `~readResponse(response, progress_callback)` ‚áí `Promise.<Uint8Array>`
      * `~pathJoin(...parts)` ‚áí `string`
      * `~PretrainedOptions` : `Object`
      * `~ModelSpecificPretrainedOptions` : `Object`
      * `~PretrainedModelOptions` : `*`

* * *

##  utils/hub.getFile(urlOrPath) ‚áí <code> Promise. < (FileResponse|Response) >
</code>

Helper function to get a file, using either the Fetch API or FileSystem API.

**Kind** : static method of `utils/hub`  
**Returns** : `Promise.<(FileResponse|Response)>` \- A promise that resolves
to a FileResponse object (if the file is retrieved using the FileSystem API),
or a Response object (if the file is retrieved using the Fetch API).

Param| Type| Description  
---|---|---  
urlOrPath| `URL` | `string`| The URL/path of the file to get.  
  
* * *

##  utils/hub.getModelFile(path_or_repo_id, filename, [fatal], [options]) ‚áí
<code> Promise. < Uint8Array > </code>

Retrieves a file from either a remote URL using the Fetch API or from the
local file system using the FileSystem API. If the filesystem is available and
`env.useCache = true`, the file will be downloaded and cached.

**Kind** : static method of `utils/hub`  
**Returns** : `Promise.<Uint8Array>` \- A Promise that resolves with the file
content as a buffer.  
**Throws** :

  * Will throw an error if the file is not found and `fatal` is true.

Param| Type| Default| Description  
---|---|---|---  
path_or_repo_id| `string`| | This can be either:

  * a string, the _model id_ of a model repo on huggingface.co.
  * a path to a _directory_ potentially containing the file.

  
filename| `string`| | The name of the file to locate in `path_or_repo`.  
[fatal]| `boolean`| `true`| Whether to throw an error if the file is not
found.  
[options]| `PretrainedOptions`| | An object containing optional parameters.  
  
  * `.getModelFile(path_or_repo_id, filename, [fatal], [options])` ‚áí `Promise.<Uint8Array>`
    * `~cacheKey` : `string`
    * `~response` : `Response` | `FileResponse` | `undefined`
    * `~buffer` : `Uint8Array`

* * *

###  getModelFile~cacheKey : <code> string </code>

**Kind** : inner property of `getModelFile`

* * *

###  getModelFile~response : <code> Response </code> | <code> FileResponse </code> | <code> undefined </code>

**Kind** : inner property of `getModelFile`

* * *

###  getModelFile~buffer : <code> Uint8Array </code>

**Kind** : inner property of `getModelFile`

* * *

##  utils/hub.getModelJSON(modelPath, fileName, [fatal], [options]) ‚áí <code>
Promise. < Object > </code>

Fetches a JSON file from a given path and file name.

**Kind** : static method of `utils/hub`  
**Returns** : `Promise.<Object>` \- The JSON data parsed into a JavaScript
object.  
**Throws** :

  * Will throw an error if the file is not found and `fatal` is true.

Param| Type| Default| Description  
---|---|---|---  
modelPath| `string`| | The path to the directory containing the file.  
fileName| `string`| | The name of the file to fetch.  
[fatal]| `boolean`| `true`| Whether to throw an error if the file is not
found.  
[options]| `PretrainedOptions`| | An object containing optional parameters.  
  
* * *

##  utils/hub~FileResponse

**Kind** : inner class of `utils/hub`

  * ~FileResponse
    * `new FileResponse(filePath)`
    * `.updateContentType()` ‚áí `void`
    * `.clone()` ‚áí `FileResponse`
    * `.arrayBuffer()` ‚áí `Promise.<ArrayBuffer>`
    * `.blob()` ‚áí `Promise.<Blob>`
    * `.text()` ‚áí `Promise.<string>`
    * `.json()` ‚áí `Promise.<Object>`

* * *

###  new FileResponse(filePath)

Creates a new `FileResponse` object.

Param| Type  
---|---  
filePath| `string` | `URL`  
  
* * *

###  fileResponse.updateContentType() ‚áí <code> void </code>

Updates the ‚Äòcontent-type‚Äô header property of the response based on the
extension of the file specified by the filePath property of the current
object.

**Kind** : instance method of `FileResponse`

* * *

###  fileResponse.clone() ‚áí <code> FileResponse </code>

Clone the current FileResponse object.

**Kind** : instance method of `FileResponse`  
**Returns** : `FileResponse` \- A new FileResponse object with the same
properties as the current object.

* * *

###  fileResponse.arrayBuffer() ‚áí <code> Promise. < ArrayBuffer > </code>

Reads the contents of the file specified by the filePath property and returns
a Promise that resolves with an ArrayBuffer containing the file‚Äôs contents.

**Kind** : instance method of `FileResponse`  
**Returns** : `Promise.<ArrayBuffer>` \- A Promise that resolves with an
ArrayBuffer containing the file‚Äôs contents.  
**Throws** :

  * `Error` If the file cannot be read.

* * *

###  fileResponse.blob() ‚áí <code> Promise. < Blob > </code>

Reads the contents of the file specified by the filePath property and returns
a Promise that resolves with a Blob containing the file‚Äôs contents.

**Kind** : instance method of `FileResponse`  
**Returns** : `Promise.<Blob>` \- A Promise that resolves with a Blob
containing the file‚Äôs contents.  
**Throws** :

  * `Error` If the file cannot be read.

* * *

###  fileResponse.text() ‚áí <code> Promise. < string > </code>

Reads the contents of the file specified by the filePath property and returns
a Promise that resolves with a string containing the file‚Äôs contents.

**Kind** : instance method of `FileResponse`  
**Returns** : `Promise.<string>` \- A Promise that resolves with a string
containing the file‚Äôs contents.  
**Throws** :

  * `Error` If the file cannot be read.

* * *

###  fileResponse.json() ‚áí <code> Promise. < Object > </code>

Reads the contents of the file specified by the filePath property and returns
a Promise that resolves with a parsed JavaScript object containing the file‚Äôs
contents.

**Kind** : instance method of `FileResponse`  
**Returns** : `Promise.<Object>` \- A Promise that resolves with a parsed
JavaScript object containing the file‚Äôs contents.  
**Throws** :

  * `Error` If the file cannot be read.

* * *

##  utils/hub~FileCache

**Kind** : inner class of `utils/hub`

  * ~FileCache
    * `new FileCache(path)`
    * `.match(request)` ‚áí `Promise.<(FileResponse|undefined)>`
    * `.put(request, response)` ‚áí `Promise.<void>`

* * *

###  new FileCache(path)

Instantiate a `FileCache` object.

Param| Type  
---|---  
path| `string`  
  
* * *

###  fileCache.match(request) ‚áí <code> Promise. < (FileResponse|undefined) >
</code>

Checks whether the given request is in the cache.

**Kind** : instance method of `FileCache`

Param| Type  
---|---  
request| `string`  
  
* * *

###  fileCache.put(request, response) ‚áí <code> Promise. < void > </code>

Adds the given response to the cache.

**Kind** : instance method of `FileCache`

Param| Type  
---|---  
request| `string`  
response| `Response` | `FileResponse`  
  
* * *

##  utils/hub~CONTENT_TYPE_MAP

Mapping from file extensions to MIME types.

**Kind** : inner constant of `utils/hub`

* * *

##  utils/hub~isValidUrl(string, [protocols], [validHosts]) ‚áí <code> boolean
</code>

Determines whether the given string is a valid URL.

**Kind** : inner method of `utils/hub`  
**Returns** : `boolean` \- True if the string is a valid URL, false otherwise.

Param| Type| Default| Description  
---|---|---|---  
string| `string` | `URL`| | The string to test for validity as an URL.  
[protocols]| `Array.<string>`| ``| A list of valid protocols. If specified,
the protocol must be in this list.  
[validHosts]| `Array.<string>`| ``| A list of valid hostnames. If specified,
the URL's hostname must be in this list.  
  
* * *

##  utils/hub~handleError(status, remoteURL, fatal) ‚áí <code> null </code>

Helper method to handle fatal errors that occur while trying to load a file
from the Hugging Face Hub.

**Kind** : inner method of `utils/hub`  
**Returns** : `null` \- Returns `null` if `fatal = true`.  
**Throws** :

  * `Error` If `fatal = false`.

Param| Type| Description  
---|---|---  
status| `number`| The HTTP status code of the error.  
remoteURL| `string`| The URL of the file that could not be loaded.  
fatal| `boolean`| Whether to raise an error if the file could not be loaded.  
  
* * *

##  utils/hub~tryCache(cache, ...names) ‚áí <code> Promise. <
(FileResponse|Response|undefined) > </code>

**Kind** : inner method of `utils/hub`  
**Returns** : `Promise.<(FileResponse|Response|undefined)>` \- The item from
the cache, or undefined if not found.

Param| Type| Description  
---|---|---  
cache| `FileCache` | `Cache`| The cache to search  
...names| `Array.<string>`| The names of the item to search for  
  
* * *

##  utils/hub~readResponse(response, progress_callback) ‚áí <code> Promise. <
Uint8Array > </code>

Read and track progress when reading a Response object

**Kind** : inner method of `utils/hub`  
**Returns** : `Promise.<Uint8Array>` \- A Promise that resolves with the
Uint8Array buffer

Param| Type| Description  
---|---|---  
response| `any`| The Response object to read  
progress_callback| `function`| The function to call with progress updates  
  
* * *

##  utils/hub~pathJoin(...parts) ‚áí <code> string </code>

Joins multiple parts of a path into a single path, while handling leading and
trailing slashes.

**Kind** : inner method of `utils/hub`  
**Returns** : `string` \- A string representing the joined path.

Param| Type| Description  
---|---|---  
...parts| `string`| Multiple parts of a path.  
  
* * *

##  utils/hub~PretrainedOptions : <code> Object </code>

Options for loading a pretrained model.

**Kind** : inner typedef of `utils/hub`  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
[progress_callback]| `function`| ``| If specified, this function will be
called during model construction, to provide the user with progress updates.  
[config]| `*`| ``| Configuration for the model to use instead of an
automatically loaded configuration. Configuration can be automatically loaded
when:

  * The model is a model provided by the library (loaded with the _model id_ string of a pretrained model).
  * The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a configuration JSON file named _config.json_ is found in the directory.

  
[cache_dir]| `string`| `null`| Path to a directory in which a downloaded
pretrained model configuration should be cached if the standard cache should
not be used.  
[local_files_only]| `boolean`| `false`| Whether or not to only look at local
files (e.g., not try downloading the model).  
[revision]| `string`| `"'main'"`| The specific model version to use. It can be
a branch name, a tag name, or a commit id, since we use a git-based system for
storing models and other artifacts on huggingface.co, so `revision` can be any
identifier allowed by git. NOTE: This setting is ignored for local requests.  
  
* * *

##  utils/hub~ModelSpecificPretrainedOptions : <code> Object </code>

Options for loading a pretrained model.

**Kind** : inner typedef of `utils/hub`  
**Properties**

Name| Type| Default| Description  
---|---|---|---  
[subfolder]| `string`| `"'onnx'"`| In case the relevant files are located
inside a subfolder of the model repo on huggingface.co, you can specify the
folder name here.  
[model_file_name]| `string`| `null`| If specified, load the model with this
name (excluding the .onnx suffix). Currently only valid for encoder- or
decoder-only models.  
[device]| `*`| ``| The device to run the model on. If not specified, the
device will be chosen from the environment settings.  
[dtype]| `*`| ``| The data type to use for the model. If not specified, the
data type will be chosen from the environment settings.  
[use_external_data_format]| `boolean` | `Record<string, boolean>`| `false`| Whether to load the model using the external data format (used for models >= 2GB in size).  
[session_options]| `*`| | (Optional) User-specified session options passed to the runtime. If not provided, suitable defaults will be chosen.  
  
* * *

##  utils/hub~PretrainedModelOptions : <code> * </code>

Options for loading a pretrained model.

**Kind** : inner typedef of `utils/hub`

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/utils/hub.md)

[‚ÜêCore](/docs/transformers.js/v3.0.0/en/api/utils/core)
[Image‚Üí](/docs/transformers.js/v3.0.0/en/api/utils/image)

utils/hub utils/hub.getFile(urlOrPath) ‚áí ` Promise. < (FileResponse|Response) > ` utils/hub.getModelFile(path_or_repo_id, filename, [fatal], [options]) ‚áí ` Promise. < Uint8Array > ` getModelFile~cacheKey : ` string ` getModelFile~response : ` Response ` | ` FileResponse ` | ` undefined ` getModelFile~buffer : ` Uint8Array ` utils/hub.getModelJSON(modelPath, fileName, [fatal], [options]) ‚áí ` Promise. < Object > ` utils/hub~FileResponse new FileResponse(filePath) fileResponse.updateContentType() ‚áí ` void ` fileResponse.clone() ‚áí ` FileResponse ` fileResponse.arrayBuffer() ‚áí ` Promise. < ArrayBuffer > ` fileResponse.blob() ‚áí ` Promise. < Blob > ` fileResponse.text() ‚áí ` Promise. < string > ` fileResponse.json() ‚áí ` Promise. < Object > ` utils/hub~FileCache new FileCache(path) fileCache.match(request) ‚áí ` Promise. < (FileResponse|undefined) > ` fileCache.put(request, response) ‚áí ` Promise. < void > ` utils/hub~CONTENT_TYPE_MAP utils/hub~isValidUrl(string, [protocols], [validHosts]) ‚áí ` boolean ` utils/hub~handleError(status, remoteURL, fatal) ‚áí ` null ` utils/hub~tryCache(cache, ...names) ‚áí ` Promise. < (FileResponse|Response|undefined) > ` utils/hub~readResponse(response, progress_callback) ‚áí ` Promise. < Uint8Array > ` utils/hub~pathJoin(...parts) ‚áí ` string ` utils/hub~PretrainedOptions : ` Object ` utils/hub~ModelSpecificPretrainedOptions : ` Object ` utils/hub~PretrainedModelOptions : ` * `

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

utils/image

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

[Core ](/docs/transformers.js/v3.0.0/en/api/utils/core)[Hub
](/docs/transformers.js/v3.0.0/en/api/utils/hub)[Image
](/docs/transformers.js/v3.0.0/en/api/utils/image)[Audio
](/docs/transformers.js/v3.0.0/en/api/utils/audio)[Tensor
](/docs/transformers.js/v3.0.0/en/api/utils/tensor)[Maths
](/docs/transformers.js/v3.0.0/en/api/utils/maths)[Data Structures
](/docs/transformers.js/v3.0.0/en/api/utils/data-structures)

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  utils/image

Helper module for image processing.

These functions and classes are only used internally, meaning an end-user
shouldn‚Äôt need to access anything here.

  * utils/image
    *  _static_
      * .RawImage
        * `new RawImage(data, width, height, channels)`
        * _instance_
          * `.size` ‚áí `*`
          * `.grayscale()` ‚áí `RawImage`
          * `.rgb()` ‚áí `RawImage`
          * `.rgba()` ‚áí `RawImage`
          * `.resize(width, height, options)` ‚áí `Promise.<RawImage>`
          * `.clone()` ‚áí `RawImage`
          * `.convert(numChannels)` ‚áí `RawImage`
          * `.save(path)`
        * _static_
          * `.read(input)` ‚áí `*`
          * `.fromCanvas(canvas)` ‚áí `RawImage`
          * `.fromURL(url)` ‚áí `Promise.<RawImage>`
          * `.fromBlob(blob)` ‚áí `Promise.<RawImage>`
          * `.fromTensor(tensor)`
    * _inner_
      * `~CONTENT_TYPE_MAP`

* * *

##  utils/image.RawImage

**Kind** : static class of `utils/image`

  * .RawImage
    * `new RawImage(data, width, height, channels)`
    * _instance_
      * `.size` ‚áí `*`
      * `.grayscale()` ‚áí `RawImage`
      * `.rgb()` ‚áí `RawImage`
      * `.rgba()` ‚áí `RawImage`
      * `.resize(width, height, options)` ‚áí `Promise.<RawImage>`
      * `.clone()` ‚áí `RawImage`
      * `.convert(numChannels)` ‚áí `RawImage`
      * `.save(path)`
    * _static_
      * `.read(input)` ‚áí `*`
      * `.fromCanvas(canvas)` ‚áí `RawImage`
      * `.fromURL(url)` ‚áí `Promise.<RawImage>`
      * `.fromBlob(blob)` ‚áí `Promise.<RawImage>`
      * `.fromTensor(tensor)`

* * *

###  new RawImage(data, width, height, channels)

Create a new `RawImage` object.

Param| Type| Description  
---|---|---  
data| `Uint8ClampedArray` | `Uint8Array`| The pixel data.  
width| `number`| The width of the image.  
height| `number`| The height of the image.  
channels| `1` | `2` | `3` | `4`| The number of channels.  
  
* * *

###  rawImage.size ‚áí <code> * </code>

Returns the size of the image (width, height).

**Kind** : instance property of `RawImage`  
**Returns** : `*` \- The size of the image (width, height).

* * *

###  rawImage.grayscale() ‚áí <code> RawImage </code>

Convert the image to grayscale format.

**Kind** : instance method of `RawImage`  
**Returns** : `RawImage` \- `this` to support chaining.

* * *

###  rawImage.rgb() ‚áí <code> RawImage </code>

Convert the image to RGB format.

**Kind** : instance method of `RawImage`  
**Returns** : `RawImage` \- `this` to support chaining.

* * *

###  rawImage.rgba() ‚áí <code> RawImage </code>

Convert the image to RGBA format.

**Kind** : instance method of `RawImage`  
**Returns** : `RawImage` \- `this` to support chaining.

* * *

###  rawImage.resize(width, height, options) ‚áí <code> Promise. < RawImage >
</code>

Resize the image to the given dimensions. This method uses the canvas API to
perform the resizing.

**Kind** : instance method of `RawImage`  
**Returns** : `Promise.<RawImage>` \- `this` to support chaining.

Param| Type| Description  
---|---|---  
width| `number`| The width of the new image.  
height| `number`| The height of the new image.  
options| `Object`| Additional options for resizing.  
[options.resample]| `0` | `1` | `2` | `3` | `4` | `5` | `string`| The resampling method to use.  
  
* * *

###  rawImage.clone() ‚áí <code> RawImage </code>

Clone the image

**Kind** : instance method of `RawImage`  
**Returns** : `RawImage` \- The cloned image

* * *

###  rawImage.convert(numChannels) ‚áí <code> RawImage </code>

Helper method for converting image to have a certain number of channels

**Kind** : instance method of `RawImage`  
**Returns** : `RawImage` \- `this` to support chaining.

Param| Type| Description  
---|---|---  
numChannels| `number`| The number of channels. Must be 1, 3, or 4.  
  
* * *

###  rawImage.save(path)

Save the image to the given path.

**Kind** : instance method of `RawImage`

Param| Type| Description  
---|---|---  
path| `string`| The path to save the image to.  
  
* * *

###  RawImage.read(input) ‚áí <code> * </code>

Helper method for reading an image from a variety of input types.

**Kind** : static method of `RawImage`  
**Returns** : `*` \- The image object.

**Example:** Read image from a URL.

Copied

    
    
    let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/football-match.jpg');
    // RawImage 

Param| Type  
---|---  
input| `RawImage` | `string` | `URL`  
  
* * *

###  RawImage.fromCanvas(canvas) ‚áí <code> RawImage </code>

Read an image from a canvas.

**Kind** : static method of `RawImage`  
**Returns** : `RawImage` \- The image object.

Param| Type| Description  
---|---|---  
canvas| `HTMLCanvasElement` | `OffscreenCanvas`| The canvas to read the image from.  
  
* * *

###  RawImage.fromURL(url) ‚áí <code> Promise. < RawImage > </code>

Read an image from a URL or file path.

**Kind** : static method of `RawImage`  
**Returns** : `Promise.<RawImage>` \- The image object.

Param| Type| Description  
---|---|---  
url| `string` | `URL`| The URL or file path to read the image from.  
  
* * *

###  RawImage.fromBlob(blob) ‚áí <code> Promise. < RawImage > </code>

Helper method to create a new Image from a blob.

**Kind** : static method of `RawImage`  
**Returns** : `Promise.<RawImage>` \- The image object.

Param| Type| Description  
---|---|---  
blob| `Blob`| The blob to read the image from.  
  
* * *

###  RawImage.fromTensor(tensor)

Helper method to create a new Image from a tensor

**Kind** : static method of `RawImage`

Param| Type  
---|---  
tensor| `Tensor`  
  
* * *

##  utils/image~CONTENT_TYPE_MAP

Mapping from file extensions to MIME types.

**Kind** : inner constant of `utils/image`

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/utils/image.md)

[‚ÜêHub](/docs/transformers.js/v3.0.0/en/api/utils/hub)
[Audio‚Üí](/docs/transformers.js/v3.0.0/en/api/utils/audio)

utils/image utils/image.RawImage new RawImage(data, width, height, channels)
rawImage.size ‚áí ` * ` rawImage.grayscale() ‚áí ` RawImage ` rawImage.rgb() ‚áí `
RawImage ` rawImage.rgba() ‚áí ` RawImage ` rawImage.resize(width, height,
options) ‚áí ` Promise. < RawImage > ` rawImage.clone() ‚áí ` RawImage `
rawImage.convert(numChannels) ‚áí ` RawImage ` rawImage.save(path)
RawImage.read(input) ‚áí ` * ` RawImage.fromCanvas(canvas) ‚áí ` RawImage `
RawImage.fromURL(url) ‚áí ` Promise. < RawImage > ` RawImage.fromBlob(blob) ‚áí `
Promise. < RawImage > ` RawImage.fromTensor(tensor)
utils/image~CONTENT_TYPE_MAP

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

utils/maths

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

[Core ](/docs/transformers.js/v3.0.0/en/api/utils/core)[Hub
](/docs/transformers.js/v3.0.0/en/api/utils/hub)[Image
](/docs/transformers.js/v3.0.0/en/api/utils/image)[Audio
](/docs/transformers.js/v3.0.0/en/api/utils/audio)[Tensor
](/docs/transformers.js/v3.0.0/en/api/utils/tensor)[Maths
](/docs/transformers.js/v3.0.0/en/api/utils/maths)[Data Structures
](/docs/transformers.js/v3.0.0/en/api/utils/data-structures)

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  utils/maths

Helper module for mathematical processing.

These functions and classes are only used internally, meaning an end-user
shouldn‚Äôt need to access anything here.

  * utils/maths
    *  _static_
      * `.interpolate_data(input)`
      * `.permute_data(array, dims, axes)` ‚áí `*`
      * `.softmax(arr)` ‚áí `T`
      * `.log_softmax(arr)` ‚áí `T`
      * `.dot(arr1, arr2)` ‚áí `number`
      * `.cos_sim(arr1, arr2)` ‚áí `number`
      * `.magnitude(arr)` ‚áí `number`
      * `.min(arr)` ‚áí `*`
      * `.max(arr)` ‚áí `*`
      * `.medianFilter(data, windowSize)`
      * `.round(num, decimals)` ‚áí `number`
      * `.bankers_round(x)` ‚áí `number`
      * `.dynamic_time_warping(matrix)` ‚áí `Array.<Array<number>>`
    * _inner_
      * ~P2FFT
        * `new P2FFT(size)`
        * `.createComplexArray()` ‚áí `Float64Array`
        * `.fromComplexArray(complex, [storage])` ‚áí `Array.<number>`
        * `.toComplexArray(input, [storage])` ‚áí `Float64Array`
        * `.transform(out, data)` ‚áí `void`
        * `.realTransform(out, data)`
        * `.inverseTransform(out, data)` ‚áí `void`
        * `._transform4(out, data, inv)` ‚áí `void`
        * `._singleTransform2(data, out, outOff, off, step)` ‚áí `void`
        * `._singleTransform4(data, out, outOff, off, step, inv)` ‚áí `void`
        * `._realTransform4(out, data, inv)`
        * `._singleRealTransform2(data, out, outOff, off, step)` ‚áí `void`
        * `._singleRealTransform4(data, out, outOff, off, step, inv)`
      * ~NP2FFT
        * `new NP2FFT(fft_length)`
      * `~AnyTypedArray` : `Int8Array` | `Uint8Array` | `Uint8ClampedArray` | `Int16Array` | `Uint16Array` | `Int32Array` | `Uint32Array` | `Float32Array` | `Float64Array`

* * *

##  utils/maths.interpolate_data(input)

**Kind** : static method of `utils/maths`

Param| Type  
---|---  
input| `TypedArray`  
  
* * *

##  utils/maths.permute_data(array, dims, axes) ‚áí <code> * </code>

Helper method to permute a `AnyTypedArray` directly

**Kind** : static method of `utils/maths`  
**Returns** : `*` \- The permuted array and the new shape.

Param| Type  
---|---  
array| `T`  
dims| `Array.<number>`  
axes| `Array.<number>`  
  
* * *

##  utils/maths.softmax(arr) ‚áí <code> T </code>

Compute the softmax of an array of numbers.

**Kind** : static method of `utils/maths`  
**Returns** : `T` \- The softmax array.

Param| Type| Description  
---|---|---  
arr| `T`| The array of numbers to compute the softmax of.  
  
* * *

##  utils/maths.log_softmax(arr) ‚áí <code> T </code>

Calculates the logarithm of the softmax function for the input array.

**Kind** : static method of `utils/maths`  
**Returns** : `T` \- The resulting log_softmax array.

Param| Type| Description  
---|---|---  
arr| `T`| The input array to calculate the log_softmax function for.  
  
* * *

##  utils/maths.dot(arr1, arr2) ‚áí <code> number </code>

Calculates the dot product of two arrays.

**Kind** : static method of `utils/maths`  
**Returns** : `number` \- The dot product of arr1 and arr2.

Param| Type| Description  
---|---|---  
arr1| `Array.<number>`| The first array.  
arr2| `Array.<number>`| The second array.  
  
* * *

##  utils/maths.cos_sim(arr1, arr2) ‚áí <code> number </code>

Computes the cosine similarity between two arrays.

**Kind** : static method of `utils/maths`  
**Returns** : `number` \- The cosine similarity between the two arrays.

Param| Type| Description  
---|---|---  
arr1| `Array.<number>`| The first array.  
arr2| `Array.<number>`| The second array.  
  
* * *

##  utils/maths.magnitude(arr) ‚áí <code> number </code>

Calculates the magnitude of a given array.

**Kind** : static method of `utils/maths`  
**Returns** : `number` \- The magnitude of the array.

Param| Type| Description  
---|---|---  
arr| `Array.<number>`| The array to calculate the magnitude of.  
  
* * *

##  utils/maths.min(arr) ‚áí <code> * </code>

Returns the value and index of the minimum element in an array.

**Kind** : static method of `utils/maths`  
**Returns** : `*` \- the value and index of the minimum element, of the form:
[valueOfMin, indexOfMin]  
**Throws** :

  * `Error` If array is empty.

Param| Type| Description  
---|---|---  
arr| `Array<number>` | `TypedArray`| array of numbers.  
  
* * *

##  utils/maths.max(arr) ‚áí <code> * </code>

Returns the value and index of the maximum element in an array.

**Kind** : static method of `utils/maths`  
**Returns** : `*` \- the value and index of the maximum element, of the form:
[valueOfMax, indexOfMax]  
**Throws** :

  * `Error` If array is empty.

Param| Type| Description  
---|---|---  
arr| `Array<number>` | `AnyTypedArray`| array of numbers.  
  
* * *

##  utils/maths.medianFilter(data, windowSize)

Performs median filter on the provided data. Padding is done by mirroring the
data.

**Kind** : static method of `utils/maths`

Param| Type| Description  
---|---|---  
data| `AnyTypedArray`| The input array  
windowSize| `number`| The window size  
  
* * *

##  utils/maths.round(num, decimals) ‚áí <code> number </code>

Helper function to round a number to a given number of decimals

**Kind** : static method of `utils/maths`  
**Returns** : `number` \- The rounded number

Param| Type| Description  
---|---|---  
num| `number`| The number to round  
decimals| `number`| The number of decimals  
  
* * *

##  utils/maths.bankers_round(x) ‚áí <code> number </code>

Helper function to round a number to the nearest integer, with ties rounded to
the nearest even number. Also known as ‚Äúbankers‚Äô rounding‚Äù. This is the
default rounding mode in python. For example: 1.5 rounds to 2 and 2.5 rounds
to 2.

**Kind** : static method of `utils/maths`  
**Returns** : `number` \- The rounded number

Param| Type| Description  
---|---|---  
x| `number`| The number to round  
  
* * *

##  utils/maths.dynamic_time_warping(matrix) ‚áí <code> Array. < Array < number
> > </code>

Measures similarity between two temporal sequences (e.g., input audio and
output tokens to generate token-level timestamps).

**Kind** : static method of `utils/maths`

Param| Type  
---|---  
matrix| `Array.<Array<number>>`  
  
* * *

##  utils/maths~P2FFT

Implementation of Radix-4 FFT.

P2FFT class provides functionality for performing Fast Fourier Transform on
arrays which are a power of two in length. Code adapted from
<https://www.npmjs.com/package/fft.js>

**Kind** : inner class of `utils/maths`

  * ~P2FFT
    * `new P2FFT(size)`
    * `.createComplexArray()` ‚áí `Float64Array`
    * `.fromComplexArray(complex, [storage])` ‚áí `Array.<number>`
    * `.toComplexArray(input, [storage])` ‚áí `Float64Array`
    * `.transform(out, data)` ‚áí `void`
    * `.realTransform(out, data)`
    * `.inverseTransform(out, data)` ‚áí `void`
    * `._transform4(out, data, inv)` ‚áí `void`
    * `._singleTransform2(data, out, outOff, off, step)` ‚áí `void`
    * `._singleTransform4(data, out, outOff, off, step, inv)` ‚áí `void`
    * `._realTransform4(out, data, inv)`
    * `._singleRealTransform2(data, out, outOff, off, step)` ‚áí `void`
    * `._singleRealTransform4(data, out, outOff, off, step, inv)`

* * *

###  new P2FFT(size)

**Throws** :

  * `Error` FFT size must be a power of two larger than 1.

Param| Type| Description  
---|---|---  
size| `number`| The size of the input array. Must be a power of two larger
than 1.  
  
* * *

###  p2FFT.createComplexArray() ‚áí <code> Float64Array </code>

Create a complex number array with size `2 * size`

**Kind** : instance method of `P2FFT`  
**Returns** : `Float64Array` \- A complex number array with size `2 * size`

* * *

###  p2FFT.fromComplexArray(complex, [storage]) ‚áí <code> Array. < number >
</code>

Converts a complex number representation stored in a Float64Array to an array
of real numbers.

**Kind** : instance method of `P2FFT`  
**Returns** : `Array.<number>` \- An array of real numbers representing the
input complex number representation.

Param| Type| Description  
---|---|---  
complex| `Float64Array`| The complex number representation to be converted.  
[storage]| `Array.<number>`| An optional array to store the result in.  
  
* * *

###  p2FFT.toComplexArray(input, [storage]) ‚áí <code> Float64Array </code>

Convert a real-valued input array to a complex-valued output array.

**Kind** : instance method of `P2FFT`  
**Returns** : `Float64Array` \- The complex-valued output array.

Param| Type| Description  
---|---|---  
input| `Float64Array`| The real-valued input array.  
[storage]| `Float64Array`| Optional buffer to store the output array.  
  
* * *

###  p2FFT.transform(out, data) ‚áí <code> void </code>

Performs a Fast Fourier Transform (FFT) on the given input data and stores the
result in the output buffer.

**Kind** : instance method of `P2FFT`  
**Throws** :

  * `Error` Input and output buffers must be different.

Param| Type| Description  
---|---|---  
out| `Float64Array`| The output buffer to store the result.  
data| `Float64Array`| The input data to transform.  
  
* * *

###  p2FFT.realTransform(out, data)

Performs a real-valued forward FFT on the given input buffer and stores the
result in the given output buffer. The input buffer must contain real values
only, while the output buffer will contain complex values. The input and
output buffers must be different.

**Kind** : instance method of `P2FFT`  
**Throws** :

  * `Error` If the input and output buffers are the same.

Param| Type| Description  
---|---|---  
out| `Float64Array`| The output buffer.  
data| `Float64Array`| The input buffer containing real values.  
  
* * *

###  p2FFT.inverseTransform(out, data) ‚áí <code> void </code>

Performs an inverse FFT transformation on the given `data` array, and stores
the result in `out`. The `out` array must be a different buffer than the
`data` array. The `out` array will contain the result of the transformation.
The `data` array will not be modified.

**Kind** : instance method of `P2FFT`  
**Throws** :

  * `Error` If `out` and `data` refer to the same buffer.

Param| Type| Description  
---|---|---  
out| `Float64Array`| The output buffer for the transformed data.  
data| `Float64Array`| The input data to transform.  
  
* * *

###  p2FFT._transform4(out, data, inv) ‚áí <code> void </code>

Performs a radix-4 implementation of a discrete Fourier transform on a given
set of data.

**Kind** : instance method of `P2FFT`

Param| Type| Description  
---|---|---  
out| `Float64Array`| The output buffer for the transformed data.  
data| `Float64Array`| The input buffer of data to be transformed.  
inv| `number`| A scaling factor to apply to the transform.  
  
* * *

###  p2FFT._singleTransform2(data, out, outOff, off, step) ‚áí <code> void
</code>

Performs a radix-2 implementation of a discrete Fourier transform on a given
set of data.

**Kind** : instance method of `P2FFT`

Param| Type| Description  
---|---|---  
data| `Float64Array`| The input buffer of data to be transformed.  
out| `Float64Array`| The output buffer for the transformed data.  
outOff| `number`| The offset at which to write the output data.  
off| `number`| The offset at which to begin reading the input data.  
step| `number`| The step size for indexing the input data.  
  
* * *

###  p2FFT._singleTransform4(data, out, outOff, off, step, inv) ‚áí <code> void
</code>

Performs radix-4 transformation on input data of length 8

**Kind** : instance method of `P2FFT`

Param| Type| Description  
---|---|---  
data| `Float64Array`| Input data array of length 8  
out| `Float64Array`| Output data array of length 8  
outOff| `number`| Index of output array to start writing from  
off| `number`| Index of input array to start reading from  
step| `number`| Step size between elements in input array  
inv| `number`| Scaling factor for inverse transform  
  
* * *

###  p2FFT._realTransform4(out, data, inv)

Real input radix-4 implementation

**Kind** : instance method of `P2FFT`

Param| Type| Description  
---|---|---  
out| `Float64Array`| Output array for the transformed data  
data| `Float64Array`| Input array of real data to be transformed  
inv| `number`| The scale factor used to normalize the inverse transform  
  
* * *

###  p2FFT._singleRealTransform2(data, out, outOff, off, step) ‚áí <code> void
</code>

Performs a single real input radix-2 transformation on the provided data

**Kind** : instance method of `P2FFT`

Param| Type| Description  
---|---|---  
data| `Float64Array`| The input data array  
out| `Float64Array`| The output data array  
outOff| `number`| The output offset  
off| `number`| The input offset  
step| `number`| The step  
  
* * *

###  p2FFT._singleRealTransform4(data, out, outOff, off, step, inv)

Computes a single real-valued transform using radix-4 algorithm. This method
is only called for len=8.

**Kind** : instance method of `P2FFT`

Param| Type| Description  
---|---|---  
data| `Float64Array`| The input data array.  
out| `Float64Array`| The output data array.  
outOff| `number`| The offset into the output array.  
off| `number`| The offset into the input array.  
step| `number`| The step size for the input array.  
inv| `number`| The value of inverse.  
  
* * *

##  utils/maths~NP2FFT

NP2FFT class provides functionality for performing Fast Fourier Transform on
arrays which are not a power of two in length. In such cases, the chirp-z
transform is used.

For more information, see:
<https://math.stackexchange.com/questions/77118/non-power-
of-2-ffts/77156#77156>

**Kind** : inner class of `utils/maths`

* * *

###  new NP2FFT(fft_length)

Constructs a new NP2FFT object.

Param| Type| Description  
---|---|---  
fft_length| `number`| The length of the FFT  
  
* * *

##  utils/maths~AnyTypedArray : <code> Int8Array </code> | <code> Uint8Array </code> | <code> Uint8ClampedArray </code> | <code> Int16Array </code> | <code> Uint16Array </code> | <code> Int32Array </code> | <code> Uint32Array </code> | <code> Float32Array </code> | <code> Float64Array </code>

**Kind** : inner typedef of `utils/maths`

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/utils/maths.md)

[‚ÜêTensor](/docs/transformers.js/v3.0.0/en/api/utils/tensor) [Data
Structures‚Üí](/docs/transformers.js/v3.0.0/en/api/utils/data-structures)

utils/maths utils/maths.interpolate_data(input) utils/maths.permute_data(array, dims, axes) ‚áí ` * ` utils/maths.softmax(arr) ‚áí ` T ` utils/maths.log_softmax(arr) ‚áí ` T ` utils/maths.dot(arr1, arr2) ‚áí ` number ` utils/maths.cos_sim(arr1, arr2) ‚áí ` number ` utils/maths.magnitude(arr) ‚áí ` number ` utils/maths.min(arr) ‚áí ` * ` utils/maths.max(arr) ‚áí ` * ` utils/maths.medianFilter(data, windowSize) utils/maths.round(num, decimals) ‚áí ` number ` utils/maths.bankers_round(x) ‚áí ` number ` utils/maths.dynamic_time_warping(matrix) ‚áí ` Array. < Array < number > > ` utils/maths~P2FFT new P2FFT(size) p2FFT.createComplexArray() ‚áí ` Float64Array ` p2FFT.fromComplexArray(complex, [storage]) ‚áí ` Array. < number > ` p2FFT.toComplexArray(input, [storage]) ‚áí ` Float64Array ` p2FFT.transform(out, data) ‚áí ` void ` p2FFT.realTransform(out, data) p2FFT.inverseTransform(out, data) ‚áí ` void ` p2FFT._transform4(out, data, inv) ‚áí ` void ` p2FFT._singleTransform2(data, out, outOff, off, step) ‚áí ` void ` p2FFT._singleTransform4(data, out, outOff, off, step, inv) ‚áí ` void ` p2FFT._realTransform4(out, data, inv) p2FFT._singleRealTransform2(data, out, outOff, off, step) ‚áí ` void ` p2FFT._singleRealTransform4(data, out, outOff, off, step, inv) utils/maths~NP2FFT new NP2FFT(fft_length) utils/maths~AnyTypedArray : ` Int8Array ` | ` Uint8Array ` | ` Uint8ClampedArray ` | ` Int16Array ` | ` Uint16Array ` | ` Int32Array ` | ` Uint32Array ` | ` Float32Array ` | ` Float64Array `

The documentation page API/UTILS/TENSOR.CLAMP doesn‚Äôt exist in v3.0.0, but
exists on the main version. Click
[here](/docs/transformers.js/main/en/api/utils/Tensor.clamp) to redirect to
the main version of the documentation.

The documentation page API/UTILS/TENSOR.FLATTEN doesn‚Äôt exist in v3.0.0, but
exists on the main version. Click
[here](/docs/transformers.js/main/en/api/utils/Tensor.flatten) to redirect to
the main version of the documentation.

The documentation page API/UTILS/TENSOR.ROUND doesn‚Äôt exist in v3.0.0, but
exists on the main version. Click
[here](/docs/transformers.js/main/en/api/utils/Tensor.round) to redirect to
the main version of the documentation.

The documentation page API/UTILS/TENSOR.SQUEEZE doesn‚Äôt exist in v3.0.0, but
exists on the main version. Click
[here](/docs/transformers.js/main/en/api/utils/Tensor.squeeze) to redirect to
the main version of the documentation.

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

utils/tensor

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

[Core ](/docs/transformers.js/v3.0.0/en/api/utils/core)[Hub
](/docs/transformers.js/v3.0.0/en/api/utils/hub)[Image
](/docs/transformers.js/v3.0.0/en/api/utils/image)[Audio
](/docs/transformers.js/v3.0.0/en/api/utils/audio)[Tensor
](/docs/transformers.js/v3.0.0/en/api/utils/tensor)[Maths
](/docs/transformers.js/v3.0.0/en/api/utils/maths)[Data Structures
](/docs/transformers.js/v3.0.0/en/api/utils/data-structures)

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  utils/tensor

Helper module for `Tensor` processing.

These functions and classes are only used internally, meaning an end-user
shouldn‚Äôt need to access anything here.

  * utils/tensor
    *  _static_
      * .Tensor
        * `new Tensor(...args)`
        * `.dims` : `Array.<number>`
        * `.type` : `DataType`
        * `.data` : `DataArray`
        * `.size` : `number`
        * `.location` : `string`
        * `.Symbol.iterator()` ‚áí `Iterator`
        * `._getitem(index)` ‚áí `Tensor`
        * `.indexOf(item)` ‚áí `number`
        * `._subarray(index, iterSize, iterDims)` ‚áí `Tensor`
        * `.item()` ‚áí `number` | `bigint`
        * `.tolist()` ‚áí `Array`
        * `.sigmoid()` ‚áí `Tensor`
        * `.sigmoid_()` ‚áí `Tensor`
        * `.map(callback)` ‚áí `Tensor`
        * `.map_(callback)` ‚áí `Tensor`
        * `.mul(val)` ‚áí `Tensor`
        * `.mul_(val)` ‚áí `Tensor`
        * `.div(val)` ‚áí `Tensor`
        * `.div_(val)` ‚áí `Tensor`
        * `.add(val)` ‚áí `Tensor`
        * `.add_(val)` ‚áí `Tensor`
        * `.sub(val)` ‚áí `Tensor`
        * `.sub_(val)` ‚áí `Tensor`
        * `.permute(...dims)` ‚áí `Tensor`
        * `.sum([dim], keepdim)` ‚áí
        * `.norm([p], [dim], [keepdim])` ‚áí `Tensor`
        * `.normalize_([p], [dim])` ‚áí `Tensor`
        * `.normalize([p], [dim])` ‚áí `Tensor`
        * `.stride()` ‚áí `Array.<number>`
        * `.squeeze([dim])` ‚áí `Tensor`
        * `.squeeze_()`
        * `.unsqueeze(dim)` ‚áí `Tensor`
        * `.unsqueeze_()`
        * `.flatten_()`
        * `.flatten(start_dim, end_dim)` ‚áí `Tensor`
        * `.view(...dims)` ‚áí `Tensor`
        * `.clamp_()`
        * `.clamp(min, max)` ‚áí `Tensor`
        * `.round_()`
        * `.round()` ‚áí `Tensor`
        * `.to(type)` ‚áí `Tensor`
      * `.permute(tensor, axes)` ‚áí `Tensor`
      * `.interpolate(input, size, mode, align_corners)` ‚áí `Tensor`
      * `.interpolate_4d(input, options)` ‚áí `Promise.<Tensor>`
      * `.matmul(a, b)` ‚áí `Promise.<Tensor>`
      * `.rfft(x, a)` ‚áí `Promise.<Tensor>`
      * `.topk(x, k)` ‚áí `*`
      * `.mean_pooling(last_hidden_state, attention_mask)` ‚áí `Tensor`
      * `.layer_norm(input, normalized_shape, options)` ‚áí `Tensor`
      * `.cat(tensors, dim)` ‚áí `Tensor`
      * `.stack(tensors, dim)` ‚áí `Tensor`
      * `.std_mean(input, dim, correction, keepdim)` ‚áí `Array.<Tensor>`
      * `.mean(input, dim, keepdim)` ‚áí `Tensor`
      * `.full(size, fill_value)` ‚áí `Tensor`
      * `.ones(size)` ‚áí `Tensor`
      * `.ones_like(tensor)` ‚áí `Tensor`
      * `.zeros(size)` ‚áí `Tensor`
      * `.zeros_like(tensor)` ‚áí `Tensor`
      * `.quantize_embeddings(tensor, precision)` ‚áí `Tensor`
    * _inner_
      * `~args[0]` : `ONNXTensor`
      * `~reshape(data, dimensions)` ‚áí `*`
        * `~reshapedArray` : `any`
      * `~DataArray` : `*`
      * `~NestArray` : `*`

* * *

##  utils/tensor.Tensor

**Kind** : static class of `utils/tensor`

  * .Tensor
    * `new Tensor(...args)`
    * `.dims` : `Array.<number>`
    * `.type` : `DataType`
    * `.data` : `DataArray`
    * `.size` : `number`
    * `.location` : `string`
    * `.Symbol.iterator()` ‚áí `Iterator`
    * `._getitem(index)` ‚áí `Tensor`
    * `.indexOf(item)` ‚áí `number`
    * `._subarray(index, iterSize, iterDims)` ‚áí `Tensor`
    * `.item()` ‚áí `number` | `bigint`
    * `.tolist()` ‚áí `Array`
    * `.sigmoid()` ‚áí `Tensor`
    * `.sigmoid_()` ‚áí `Tensor`
    * `.map(callback)` ‚áí `Tensor`
    * `.map_(callback)` ‚áí `Tensor`
    * `.mul(val)` ‚áí `Tensor`
    * `.mul_(val)` ‚áí `Tensor`
    * `.div(val)` ‚áí `Tensor`
    * `.div_(val)` ‚áí `Tensor`
    * `.add(val)` ‚áí `Tensor`
    * `.add_(val)` ‚áí `Tensor`
    * `.sub(val)` ‚áí `Tensor`
    * `.sub_(val)` ‚áí `Tensor`
    * `.permute(...dims)` ‚áí `Tensor`
    * `.sum([dim], keepdim)` ‚áí
    * `.norm([p], [dim], [keepdim])` ‚áí `Tensor`
    * `.normalize_([p], [dim])` ‚áí `Tensor`
    * `.normalize([p], [dim])` ‚áí `Tensor`
    * `.stride()` ‚áí `Array.<number>`
    * `.squeeze([dim])` ‚áí `Tensor`
    * `.squeeze_()`
    * `.unsqueeze(dim)` ‚áí `Tensor`
    * `.unsqueeze_()`
    * `.flatten_()`
    * `.flatten(start_dim, end_dim)` ‚áí `Tensor`
    * `.view(...dims)` ‚áí `Tensor`
    * `.clamp_()`
    * `.clamp(min, max)` ‚áí `Tensor`
    * `.round_()`
    * `.round()` ‚áí `Tensor`
    * `.to(type)` ‚áí `Tensor`

* * *

###  new Tensor(...args)

Create a new Tensor or copy an existing Tensor.

Param| Type  
---|---  
...args| `*`  
  
* * *

###  tensor.dims : <code> Array. < number > </code>

Dimensions of the tensor.

**Kind** : instance property of `Tensor`

* * *

###  tensor.type : <code> DataType </code>

Type of the tensor.

**Kind** : instance property of `Tensor`

* * *

###  tensor.data : <code> DataArray </code>

The data stored in the tensor.

**Kind** : instance property of `Tensor`

* * *

###  tensor.size : <code> number </code>

The number of elements in the tensor.

**Kind** : instance property of `Tensor`

* * *

###  tensor.location : <code> string </code>

The location of the tensor data.

**Kind** : instance property of `Tensor`

* * *

###  tensor.Symbol.iterator() ‚áí <code> Iterator </code>

Returns an iterator object for iterating over the tensor data in row-major
order. If the tensor has more than one dimension, the iterator will yield
subarrays.

**Kind** : instance method of `Tensor`  
**Returns** : `Iterator` \- An iterator object for iterating over the tensor
data in row-major order.

* * *

###  tensor._getitem(index) ‚áí <code> Tensor </code>

Index into a Tensor object.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The data at the specified index.

Param| Type| Description  
---|---|---  
index| `number`| The index to access.  
  
* * *

###  tensor.indexOf(item) ‚áí <code> number </code>

**Kind** : instance method of `Tensor`  
**Returns** : `number` \- The index of the first occurrence of item in the
tensor data.

Param| Type| Description  
---|---|---  
item| `number` | `bigint`| The item to search for in the tensor  
  
* * *

###  tensor._subarray(index, iterSize, iterDims) ‚áí <code> Tensor </code>

**Kind** : instance method of `Tensor`

Param| Type  
---|---  
index| `number`  
iterSize| `number`  
iterDims| `any`  
  
* * *

###  tensor.item() ‚áí <code> number </code> | <code> bigint </code>

Returns the value of this tensor as a standard JavaScript Number. This only
works for tensors with one element. For other cases, see `Tensor.tolist()`.

**Kind** : instance method of `Tensor`  
**Returns** : `number` | `bigint` \- The value of this tensor as a standard JavaScript Number.  
**Throws** :

  * `Error` If the tensor has more than one element.

* * *

###  tensor.tolist() ‚áí <code> Array </code>

Convert tensor data to a n-dimensional JS list

**Kind** : instance method of `Tensor`

* * *

###  tensor.sigmoid() ‚áí <code> Tensor </code>

Return a new Tensor with the sigmoid function applied to each element.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The tensor with the sigmoid function applied.

* * *

###  tensor.sigmoid_() ‚áí <code> Tensor </code>

Applies the sigmoid function to the tensor in place.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- Returns `this`.

* * *

###  tensor.map(callback) ‚áí <code> Tensor </code>

Return a new Tensor with a callback function applied to each element.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- A new Tensor with the callback function applied to
each element.

Param| Type| Description  
---|---|---  
callback| `function`| The function to apply to each element. It should take
three arguments: the current element, its index, and the tensor's data array.  
  
* * *

###  tensor.map_(callback) ‚áí <code> Tensor </code>

Apply a callback function to each element of the tensor in place.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- Returns `this`.

Param| Type| Description  
---|---|---  
callback| `function`| The function to apply to each element. It should take
three arguments: the current element, its index, and the tensor's data array.  
  
* * *

###  tensor.mul(val) ‚áí <code> Tensor </code>

Return a new Tensor with every element multiplied by a constant.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The new tensor.

Param| Type| Description  
---|---|---  
val| `number`| The value to multiply by.  
  
* * *

###  tensor.mul_(val) ‚áí <code> Tensor </code>

Multiply the tensor by a constant in place.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- Returns `this`.

Param| Type| Description  
---|---|---  
val| `number`| The value to multiply by.  
  
* * *

###  tensor.div(val) ‚áí <code> Tensor </code>

Return a new Tensor with every element divided by a constant.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The new tensor.

Param| Type| Description  
---|---|---  
val| `number`| The value to divide by.  
  
* * *

###  tensor.div_(val) ‚áí <code> Tensor </code>

Divide the tensor by a constant in place.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- Returns `this`.

Param| Type| Description  
---|---|---  
val| `number`| The value to divide by.  
  
* * *

###  tensor.add(val) ‚áí <code> Tensor </code>

Return a new Tensor with every element added by a constant.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The new tensor.

Param| Type| Description  
---|---|---  
val| `number`| The value to add by.  
  
* * *

###  tensor.add_(val) ‚áí <code> Tensor </code>

Add the tensor by a constant in place.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- Returns `this`.

Param| Type| Description  
---|---|---  
val| `number`| The value to add by.  
  
* * *

###  tensor.sub(val) ‚áí <code> Tensor </code>

Return a new Tensor with every element subtracted by a constant.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The new tensor.

Param| Type| Description  
---|---|---  
val| `number`| The value to subtract by.  
  
* * *

###  tensor.sub_(val) ‚áí <code> Tensor </code>

Subtract the tensor by a constant in place.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- Returns `this`.

Param| Type| Description  
---|---|---  
val| `number`| The value to subtract by.  
  
* * *

###  tensor.permute(...dims) ‚áí <code> Tensor </code>

Return a permuted version of this Tensor, according to the provided
dimensions.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The permuted tensor.

Param| Type| Description  
---|---|---  
...dims| `number`| Dimensions to permute.  
  
* * *

###  tensor.sum([dim], keepdim) ‚áí

Returns the sum of each row of the input tensor in the given dimension dim.

**Kind** : instance method of `Tensor`  
**Returns** : The summed tensor

Param| Type| Default| Description  
---|---|---|---  
[dim]| `number`| ``| The dimension or dimensions to reduce. If `null`, all
dimensions are reduced.  
keepdim| `boolean`| `false`| Whether the output tensor has `dim` retained or
not.  
  
* * *

###  tensor.norm([p], [dim], [keepdim]) ‚áí <code> Tensor </code>

Returns the matrix norm or vector norm of a given tensor.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The norm of the tensor.

Param| Type| Default| Description  
---|---|---|---  
[p]| `number` | `string`| `'fro'`| The order of norm  
[dim]| `number`| ``| Specifies which dimension of the tensor to calculate the
norm across. If dim is None, the norm will be calculated across all dimensions
of input.  
[keepdim]| `boolean`| `false`| Whether the output tensors have dim retained or
not.  
  
* * *

###  tensor.normalize_([p], [dim]) ‚áí <code> Tensor </code>

Performs `L_p` normalization of inputs over specified dimension. Operates in
place.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- `this` for operation chaining.

Param| Type| Default| Description  
---|---|---|---  
[p]| `number`| `2`| The exponent value in the norm formulation  
[dim]| `number`| `1`| The dimension to reduce  
  
* * *

###  tensor.normalize([p], [dim]) ‚áí <code> Tensor </code>

Performs `L_p` normalization of inputs over specified dimension.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The normalized tensor.

Param| Type| Default| Description  
---|---|---|---  
[p]| `number`| `2`| The exponent value in the norm formulation  
[dim]| `number`| `1`| The dimension to reduce  
  
* * *

###  tensor.stride() ‚áí <code> Array. < number > </code>

Compute and return the stride of this tensor. Stride is the jump necessary to
go from one element to the next one in the specified dimension dim.

**Kind** : instance method of `Tensor`  
**Returns** : `Array.<number>` \- The stride of this tensor.

* * *

###  tensor.squeeze([dim]) ‚áí <code> Tensor </code>

Returns a tensor with all specified dimensions of input of size 1 removed.

NOTE: The returned tensor shares the storage with the input tensor, so
changing the contents of one will change the contents of the other. If you
would like a copy, use `tensor.clone()` before squeezing.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The squeezed tensor

Param| Type| Default| Description  
---|---|---|---  
[dim]| `number`| ``| If given, the input will be squeezed only in the
specified dimensions.  
  
* * *

###  tensor.squeeze_()

In-place version of @see [Tensor.squeeze](Tensor.squeeze)

**Kind** : instance method of `Tensor`

* * *

###  tensor.unsqueeze(dim) ‚áí <code> Tensor </code>

Returns a new tensor with a dimension of size one inserted at the specified
position.

NOTE: The returned tensor shares the same underlying data with this tensor.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The unsqueezed tensor

Param| Type| Default| Description  
---|---|---|---  
dim| `number`| ``| The index at which to insert the singleton dimension  
  
* * *

###  tensor.unsqueeze_()

In-place version of @see [Tensor.unsqueeze](Tensor.unsqueeze)

**Kind** : instance method of `Tensor`

* * *

###  tensor.flatten_()

In-place version of @see [Tensor.flatten](Tensor.flatten)

**Kind** : instance method of `Tensor`

* * *

###  tensor.flatten(start_dim, end_dim) ‚áí <code> Tensor </code>

Flattens input by reshaping it into a one-dimensional tensor. If `start_dim`
or `end_dim` are passed, only dimensions starting with `start_dim` and ending
with `end_dim` are flattened. The order of elements in input is unchanged.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The flattened tensor.

Param| Type| Default| Description  
---|---|---|---  
start_dim| `number`| `0`| the first dim to flatten  
end_dim| `number`| | the last dim to flatten  
  
* * *

###  tensor.view(...dims) ‚áí <code> Tensor </code>

Returns a new tensor with the same data as the `self` tensor but of a
different `shape`.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The tensor with the same data but different shape

Param| Type| Description  
---|---|---  
...dims| `number`| the desired size  
  
* * *

###  tensor.clamp_()

In-place version of @see [Tensor.clamp](Tensor.clamp)

**Kind** : instance method of `Tensor`

* * *

###  tensor.clamp(min, max) ‚áí <code> Tensor </code>

Clamps all elements in input into the range [ min, max ]

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- the output tensor.

Param| Type| Description  
---|---|---  
min| `number`| lower-bound of the range to be clamped to  
max| `number`| upper-bound of the range to be clamped to  
  
* * *

###  tensor.round_()

In-place version of @see [Tensor.round](Tensor.round)

**Kind** : instance method of `Tensor`

* * *

###  tensor.round() ‚áí <code> Tensor </code>

Rounds elements of input to the nearest integer.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- the output tensor.

* * *

###  tensor.to(type) ‚áí <code> Tensor </code>

Performs Tensor dtype conversion.

**Kind** : instance method of `Tensor`  
**Returns** : `Tensor` \- The converted tensor.

Param| Type| Description  
---|---|---  
type| `DataType`| The desired data type.  
  
* * *

##  utils/tensor.permute(tensor, axes) ‚áí <code> Tensor </code>

Permutes a tensor according to the provided axes.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The permuted tensor.

Param| Type| Description  
---|---|---  
tensor| `any`| The input tensor to permute.  
axes| `Array`| The axes to permute the tensor along.  
  
* * *

##  utils/tensor.interpolate(input, size, mode, align_corners) ‚áí <code> Tensor
</code>

Interpolates an Tensor to the given size.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The interpolated tensor.

Param| Type| Description  
---|---|---  
input| `Tensor`| The input tensor to interpolate. Data must be channel-first
(i.e., [c, h, w])  
size| `Array.<number>`| The output size of the image  
mode| `string`| The interpolation mode  
align_corners| `boolean`| Whether to align corners.  
  
* * *

##  utils/tensor.interpolate_4d(input, options) ‚áí <code> Promise. < Tensor >
</code>

Down/up samples the input. Inspired by
<https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html>.

**Kind** : static method of `utils/tensor`  
**Returns** : `Promise.<Tensor>` \- The interpolated tensor.

Param| Type| Default| Description  
---|---|---|---  
input| `Tensor`| | the input tensor  
options| `Object`| | the options for the interpolation  
[options.size]| `*`| ``| output spatial size.  
[options.mode]| `"bilinear"` | `"bicubic"`| `'bilinear'`| algorithm used for upsampling  
  
* * *

##  utils/tensor.matmul(a, b) ‚áí <code> Promise. < Tensor > </code>

Matrix product of two tensors. Inspired by
<https://pytorch.org/docs/stable/generated/torch.matmul.html>

**Kind** : static method of `utils/tensor`  
**Returns** : `Promise.<Tensor>` \- The matrix product of the two tensors.

Param| Type| Description  
---|---|---  
a| `Tensor`| the first tensor to be multiplied  
b| `Tensor`| the second tensor to be multiplied  
  
* * *

##  utils/tensor.rfft(x, a) ‚áí <code> Promise. < Tensor > </code>

Computes the one dimensional Fourier transform of real-valued input. Inspired
by <https://pytorch.org/docs/stable/generated/torch.fft.rfft.html>

**Kind** : static method of `utils/tensor`  
**Returns** : `Promise.<Tensor>` \- the output tensor.

Param| Type| Description  
---|---|---  
x| `Tensor`| the real input tensor  
a| `Tensor`| The dimension along which to take the one dimensional real FFT.  
  
* * *

##  utils/tensor.topk(x, k) ‚áí <code> * </code>

Returns the k largest elements of the given input tensor. Inspired by
<https://pytorch.org/docs/stable/generated/torch.topk.html>

**Kind** : static method of `utils/tensor`  
**Returns** : `*` \- the output tuple of (Tensor, LongTensor) of top-k
elements and their indices.

Param| Type| Description  
---|---|---  
x| `Tensor`| the input tensor  
k| `number`| the k in "top-k"  
  
* * *

##  utils/tensor.mean_pooling(last_hidden_state, attention_mask) ‚áí <code>
Tensor </code>

Perform mean pooling of the last hidden state followed by a normalization
step.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- Returns a new Tensor of shape [batchSize, embedDim].

Param| Type| Description  
---|---|---  
last_hidden_state| `Tensor`| Tensor of shape [batchSize, seqLength, embedDim]  
attention_mask| `Tensor`| Tensor of shape [batchSize, seqLength]  
  
* * *

##  utils/tensor.layer_norm(input, normalized_shape, options) ‚áí <code> Tensor
</code>

Apply Layer Normalization for last certain number of dimensions.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The normalized tensor.

Param| Type| Default| Description  
---|---|---|---  
input| `Tensor`| | The input tensor  
normalized_shape| `Array.<number>`| | input shape from an expected input of size  
options| `Object`| | The options for the layer normalization  
[options.eps]| `number`| `1e-5`| A value added to the denominator for
numerical stability.  
  
* * *

##  utils/tensor.cat(tensors, dim) ‚áí <code> Tensor </code>

Concatenates an array of tensors along a specified dimension.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The concatenated tensor.

Param| Type| Description  
---|---|---  
tensors| `Array.<Tensor>`| The array of tensors to concatenate.  
dim| `number`| The dimension to concatenate along.  
  
* * *

##  utils/tensor.stack(tensors, dim) ‚áí <code> Tensor </code>

Stack an array of tensors along a specified dimension.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The stacked tensor.

Param| Type| Description  
---|---|---  
tensors| `Array.<Tensor>`| The array of tensors to stack.  
dim| `number`| The dimension to stack along.  
  
* * *

##  utils/tensor.std_mean(input, dim, correction, keepdim) ‚áí <code> Array. <
Tensor > </code>

Calculates the standard deviation and mean over the dimensions specified by
dim. dim can be a single dimension or `null` to reduce over all dimensions.

**Kind** : static method of `utils/tensor`  
**Returns** : `Array.<Tensor>` \- A tuple of (std, mean) tensors.

Param| Type| Description  
---|---|---  
input| `Tensor`| the input tenso  
dim| `number` | `null`| the dimension to reduce. If None, all dimensions are reduced.  
correction| `number`| difference between the sample size and sample degrees of
freedom. Defaults to Bessel's correction, correction=1.  
keepdim| `boolean`| whether the output tensor has dim retained or not.  
  
* * *

##  utils/tensor.mean(input, dim, keepdim) ‚áí <code> Tensor </code>

Returns the mean value of each row of the input tensor in the given dimension
dim.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- A new tensor with means taken along the specified
dimension.

Param| Type| Description  
---|---|---  
input| `Tensor`| the input tensor.  
dim| `number` | `null`| the dimension to reduce.  
keepdim| `boolean`| whether the output tensor has dim retained or not.  
  
* * *

##  utils/tensor.full(size, fill_value) ‚áí <code> Tensor </code>

Creates a tensor of size size filled with fill_value. The tensor‚Äôs dtype is
inferred from fill_value.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The filled tensor.

Param| Type| Description  
---|---|---  
size| `Array.<number>`| A sequence of integers defining the shape of the
output tensor.  
fill_value| `number` | `bigint`| The value to fill the output tensor with.  
  
* * *

##  utils/tensor.ones(size) ‚áí <code> Tensor </code>

Returns a tensor filled with the scalar value 1, with the shape defined by the
variable argument size.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The ones tensor.

Param| Type| Description  
---|---|---  
size| `Array.<number>`| A sequence of integers defining the shape of the
output tensor.  
  
* * *

##  utils/tensor.ones_like(tensor) ‚áí <code> Tensor </code>

Returns a tensor filled with the scalar value 1, with the same size as input.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The ones tensor.

Param| Type| Description  
---|---|---  
tensor| `Tensor`| The size of input will determine size of the output tensor.  
  
* * *

##  utils/tensor.zeros(size) ‚áí <code> Tensor </code>

Returns a tensor filled with the scalar value 0, with the shape defined by the
variable argument size.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The zeros tensor.

Param| Type| Description  
---|---|---  
size| `Array.<number>`| A sequence of integers defining the shape of the
output tensor.  
  
* * *

##  utils/tensor.zeros_like(tensor) ‚áí <code> Tensor </code>

Returns a tensor filled with the scalar value 0, with the same size as input.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The zeros tensor.

Param| Type| Description  
---|---|---  
tensor| `Tensor`| The size of input will determine size of the output tensor.  
  
* * *

##  utils/tensor.quantize_embeddings(tensor, precision) ‚áí <code> Tensor
</code>

Quantizes the embeddings tensor to binary or unsigned binary precision.

**Kind** : static method of `utils/tensor`  
**Returns** : `Tensor` \- The quantized tensor.

Param| Type| Description  
---|---|---  
tensor| `Tensor`| The tensor to quantize.  
precision| `'binary'` | `'ubinary'`| The precision to use for quantization.  
  
* * *

##  utils/tensor~args[0] : <code> ONNXTensor </code>

**Kind** : inner property of `utils/tensor`

* * *

##  utils/tensor~reshape(data, dimensions) ‚áí <code> * </code>

Reshapes a 1-dimensional array into an n-dimensional array, according to the
provided dimensions.

**Kind** : inner method of `utils/tensor`  
**Returns** : `*` \- The reshaped array.

Param| Type| Description  
---|---|---  
data| `Array<T>` | `DataArray`| The input array to reshape.  
dimensions| `DIM`| The target shape/dimensions.  
  
**Example**

Copied

    
    
    reshape([10                    ], [1      ]); // Type: number[]      Value: [10]
      reshape([1, 2, 3, 4            ], [2, 2   ]); // Type: number[][]    Value: [[1, 2], [3, 4]]
      reshape([1, 2, 3, 4, 5, 6, 7, 8], [2, 2, 2]); // Type: number[][][]  Value: [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]
      reshape([1, 2, 3, 4, 5, 6, 7, 8], [4, 2   ]); // Type: number[][]    Value: [[1, 2], [3, 4], [5, 6], [7, 8]]

* * *

###  reshape~reshapedArray : <code> any </code>

**Kind** : inner property of `reshape`

* * *

##  utils/tensor~DataArray : <code> * </code>

**Kind** : inner typedef of `utils/tensor`

* * *

##  utils/tensor~NestArray : <code> * </code>

This creates a nested array of a given type and depth (see examples).

**Kind** : inner typedef of `utils/tensor`  
**Example**

Copied

    
    
    NestArray<string, 1>; // string[]

**Example**

Copied

    
    
    NestArray<number, 2>; // number[][]

**Example**

Copied

    
    
    NestArray<string, 3>; // string[][][] etc.

* * *

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/api/utils/tensor.md)

[‚ÜêAudio](/docs/transformers.js/v3.0.0/en/api/utils/audio)
[Maths‚Üí](/docs/transformers.js/v3.0.0/en/api/utils/maths)

utils/tensor utils/tensor.Tensor new Tensor(...args) tensor.dims : ` Array. < number > ` tensor.type : ` DataType ` tensor.data : ` DataArray ` tensor.size : ` number ` tensor.location : ` string ` tensor.Symbol.iterator() ‚áí ` Iterator ` tensor._getitem(index) ‚áí ` Tensor ` tensor.indexOf(item) ‚áí ` number ` tensor._subarray(index, iterSize, iterDims) ‚áí ` Tensor ` tensor.item() ‚áí ` number ` | ` bigint ` tensor.tolist() ‚áí ` Array ` tensor.sigmoid() ‚áí ` Tensor ` tensor.sigmoid_() ‚áí ` Tensor ` tensor.map(callback) ‚áí ` Tensor ` tensor.map_(callback) ‚áí ` Tensor ` tensor.mul(val) ‚áí ` Tensor ` tensor.mul_(val) ‚áí ` Tensor ` tensor.div(val) ‚áí ` Tensor ` tensor.div_(val) ‚áí ` Tensor ` tensor.add(val) ‚áí ` Tensor ` tensor.add_(val) ‚áí ` Tensor ` tensor.sub(val) ‚áí ` Tensor ` tensor.sub_(val) ‚áí ` Tensor ` tensor.permute(...dims) ‚áí ` Tensor ` tensor.sum([dim], keepdim) ‚áí tensor.norm([p], [dim], [keepdim]) ‚áí ` Tensor ` tensor.normalize_([p], [dim]) ‚áí ` Tensor ` tensor.normalize([p], [dim]) ‚áí ` Tensor ` tensor.stride() ‚áí ` Array. < number > ` tensor.squeeze([dim]) ‚áí ` Tensor ` tensor.squeeze_() tensor.unsqueeze(dim) ‚áí ` Tensor ` tensor.unsqueeze_() tensor.flatten_() tensor.flatten(start_dim, end_dim) ‚áí ` Tensor ` tensor.view(...dims) ‚áí ` Tensor ` tensor.clamp_() tensor.clamp(min, max) ‚áí ` Tensor ` tensor.round_() tensor.round() ‚áí ` Tensor ` tensor.to(type) ‚áí ` Tensor ` utils/tensor.permute(tensor, axes) ‚áí ` Tensor ` utils/tensor.interpolate(input, size, mode, align_corners) ‚áí ` Tensor ` utils/tensor.interpolate_4d(input, options) ‚áí ` Promise. < Tensor > ` utils/tensor.matmul(a, b) ‚áí ` Promise. < Tensor > ` utils/tensor.rfft(x, a) ‚áí ` Promise. < Tensor > ` utils/tensor.topk(x, k) ‚áí ` * ` utils/tensor.mean_pooling(last_hidden_state, attention_mask) ‚áí ` Tensor ` utils/tensor.layer_norm(input, normalized_shape, options) ‚áí ` Tensor ` utils/tensor.cat(tensors, dim) ‚áí ` Tensor ` utils/tensor.stack(tensors, dim) ‚áí ` Tensor ` utils/tensor.std_mean(input, dim, correction, keepdim) ‚áí ` Array. < Tensor > ` utils/tensor.mean(input, dim, keepdim) ‚áí ` Tensor ` utils/tensor.full(size, fill_value) ‚áí ` Tensor ` utils/tensor.ones(size) ‚áí ` Tensor ` utils/tensor.ones_like(tensor) ‚áí ` Tensor ` utils/tensor.zeros(size) ‚áí ` Tensor ` utils/tensor.zeros_like(tensor) ‚áí ` Tensor ` utils/tensor.quantize_embeddings(tensor, precision) ‚áí ` Tensor ` utils/tensor~args[0] : ` ONNXTensor ` utils/tensor~reshape(data, dimensions) ‚áí ` * ` reshape~reshapedArray : ` any ` utils/tensor~DataArray : ` * ` utils/tensor~NestArray : ` * `

The documentation page API/UTILS/TENSOR.UNSQUEEZE doesn‚Äôt exist in v3.0.0, but
exists on the main version. Click
[here](/docs/transformers.js/main/en/api/utils/Tensor.unsqueeze) to redirect
to the main version of the documentation.

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Use custom models

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Use custom models

By default, Transformers.js uses [hosted pretrained
models](https://huggingface.co/models?library=transformers.js) and
[precompiled WASM
binaries](https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.0.0/dist/),
which should work out-of-the-box. You can customize this as follows:

###  Settings

Copied

    
    
    import { env } from '@huggingface/transformers';
    
    // Specify a custom location for models (defaults to '/models/').
    env.localModelPath = '/path/to/models/';
    
    // Disable the loading of remote models from the Hugging Face Hub:
    env.allowRemoteModels = false;
    
    // Set location of .wasm files. Defaults to use a CDN.
    env.backends.onnx.wasm.wasmPaths = '/path/to/files/';

For a full list of available settings, check out the [API
Reference](./api/env).

###  Convert your models to ONNX

We recommend using our [conversion
script](https://github.com/huggingface/transformers.js/blob/v3.0.0/scripts/convert.py)
to convert your PyTorch, TensorFlow, or JAX models to ONNX in a single
command. Behind the scenes, it uses [ü§ó
Optimum](https://huggingface.co/docs/optimum) to perform conversion and
quantization of your model.

Copied

    
    
    python -m scripts.convert --quantize --model_id <model_name_or_path>

For example, convert and quantize [bert-base-
uncased](https://huggingface.co/bert-base-uncased) using:

Copied

    
    
    python -m scripts.convert --quantize --model_id bert-base-uncased

This will save the following files to `./models/`:

Copied

    
    
    bert-base-uncased/
    ‚îú‚îÄ‚îÄ config.json
    ‚îú‚îÄ‚îÄ tokenizer.json
    ‚îú‚îÄ‚îÄ tokenizer_config.json
    ‚îî‚îÄ‚îÄ onnx/
        ‚îú‚îÄ‚îÄ model.onnx
        ‚îî‚îÄ‚îÄ model_quantized.onnx

For the full list of supported architectures, see the [Optimum
documentation](https://huggingface.co/docs/optimum/v3.0.0/en/exporters/onnx/overview).

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/custom_usage.md)

[‚ÜêThe pipeline API](/docs/transformers.js/v3.0.0/en/pipelines) [Building a
Vanilla JS Application‚Üí](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)

Use custom models Settings Convert your models to ONNX

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Server-side Audio Processing in Node.js

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Server-side Audio Processing in Node.js

A major benefit of writing code for the web is that you can access the
multitude of APIs that are available in modern browsers. Unfortunately, when
writing server-side code, we are not afforded such luxury, so we have to find
another way. In this tutorial, we will design a simple Node.js application
that uses Transformers.js for speech recognition with
[Whisper](https://huggingface.co/Xenova/whisper-tiny.en), and in the process,
learn how to process audio on the server.

The main problem we need to solve is that the [Web Audio
API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) is not
available in Node.js, meaning we can‚Äôt use the
[`AudioContext`](https://developer.mozilla.org/en-
US/docs/Web/API/AudioContext) class to process audio. So, we will need to
install third-party libraries to obtain the raw audio data. For this example,
we will only consider `.wav` files, but the same principles apply to other
audio formats.

This tutorial will be written as an ES module, but you can easily adapt it to
use CommonJS instead. For more information, see the [node
tutorial](https://huggingface.co/docs/transformers.js/tutorials/node).

**Useful links:**

  * [Source code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/node-audio-processing)
  * [Documentation](https://huggingface.co/docs/transformers.js)

##  Prerequisites

  * [Node.js](https://nodejs.org/en/) version 18+
  * [npm](https://www.npmjs.com/) version 9+

##  Getting started

Let‚Äôs start by creating a new Node.js project and installing Transformers.js
via [NPM](https://www.npmjs.com/package/@huggingface/transformers):

Copied

    
    
    npm init -y
    npm i @huggingface/transformers

Remember to add `"type": "module"` to your `package.json` to indicate that
your project uses ECMAScript modules.

Next, let‚Äôs install the [`wavefile`](https://www.npmjs.com/package/wavefile)
package, which we will use for loading `.wav` files:

Copied

    
    
    npm i wavefile

##  Creating the application

Start by creating a new file called `index.js`, which will be the entry point
for our application. Let‚Äôs also import the necessary modules:

Copied

    
    
    import { pipeline } from '@huggingface/transformers';
    import wavefile from 'wavefile';

For this tutorial, we will use the `Xenova/whisper-tiny.en` model, but feel
free to choose one of the other whisper models from the [Hugging Face
Hub](https://huggingface.co/models?library=transformers.js&search=whisper).
Let‚Äôs create our pipeline with:

Copied

    
    
    let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');

Next, let‚Äôs load an audio file and convert it to the format required by
Transformers.js:

Copied

    
    
    // Load audio data
    let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/jfk.wav';
    let buffer = Buffer.from(await fetch(url).then(x => x.arrayBuffer()))
    
    // Read .wav file and convert it to required format
    let wav = new wavefile.WaveFile(buffer);
    wav.toBitDepth('32f'); // Pipeline expects input as a Float32Array
    wav.toSampleRate(16000); // Whisper expects audio with a sampling rate of 16000
    let audioData = wav.getSamples();
    if (Array.isArray(audioData)) {
      if (audioData.length > 1) {
        const SCALING_FACTOR = Math.sqrt(2);
    
        // Merge channels (into first channel to save memory)
        for (let i = 0; i < audioData[0].length; ++i) {
          audioData[0][i] = SCALING_FACTOR * (audioData[0][i] + audioData[1][i]) / 2;
        }
      }
    
      // Select first channel
      audioData = audioData[0];
    }

Finally, let‚Äôs run the model and measure execution duration.

Copied

    
    
    let start = performance.now();
    let output = await transcriber(audioData);
    let end = performance.now();
    console.log(`Execution duration: ${(end - start) / 1000} seconds`);
    console.log(output);

You can now run the application with `node index.js`. Note that when running
the script for the first time, it may take a while to download and cache the
model. Subsequent requests will use the cached model, and model loading will
be much faster.

You should see output similar to:

Copied

    
    
    Execution duration: 0.6460317999720574 seconds
    {
      text: ' And so my fellow Americans ask not what your country can do for you. Ask what you can do for your country.'
    }

That‚Äôs it! You‚Äôve successfully created a Node.js application that uses
Transformers.js for speech recognition with Whisper. You can now use this as a
starting point for your own applications.

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/guides/node-
audio-processing.md)

[‚ÜêAccessing Private/Gated
Models](/docs/transformers.js/v3.0.0/en/guides/private)
[Index‚Üí](/docs/transformers.js/v3.0.0/en/api/transformers)

Server-side Audio Processing in Node.js Prerequisites Getting started Creating
the application

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Accessing Private/Gated Models

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Accessing Private/Gated Models

Due to the possibility of leaking access tokens to users of your website or
web application, we only support accessing private/gated models from server-
side environments (e.g., Node.js) that have access to the process‚Äô environment
variables.

##  Step 1: Generating a User Access Token

[User Access Tokens](https://huggingface.co/docs/hub/security-tokens) are the
preferred way to authenticate an application to Hugging Face services.

To generate an access token, navigate to the [Access Tokens
tab](https://huggingface.co/settings/tokens) in your settings and click on the
**New token** button. Choose a name for your token and click **Generate a
token** (we recommend keeping the ‚ÄúRole‚Äù as read-only). You can then click the
**Copy** button next to your newly-created token to copy it to your clipboard.

![](https://huggingface.co/datasets/huggingface/documentation-
images/resolve/v3.0.0/hub/new-token.png)
![](https://huggingface.co/datasets/huggingface/documentation-
images/resolve/v3.0.0/hub/new-token-dark.png)

To delete or refresh User Access Tokens, you can click the **Manage** button.

##  Step 2: Using the access token in Transformers.js

Transformers.js will attach an Authorization header to requests made to the
Hugging Face Hub when the `HF_TOKEN` environment variable is set and visible
to the process.

One way to do this is to call your program with the environment variable set.
For example, let‚Äôs say you have a file called `llama.js` with the following
code:

Copied

    
    
    import { AutoTokenizer } from '@huggingface/transformers';
    
    // Load tokenizer for a gated repository.
    const tokenizer = await AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf');
    
    // Encode text.
    const text = 'Hello world!';
    const encoded = tokenizer.encode(text);
    console.log(encoded);

You can then use the following command to set the `HF_TOKEN` environment
variable and run the file:

Copied

    
    
    HF_TOKEN=hf_... node tests/llama.js

(remember to replace `hf_...` with your actual access token).

If done correctly, you should see the following output:

Copied

    
    
    [ 1, 15043, 3186, 29991 ]

Alternatively, you can set the environment variable directly in your code:

Copied

    
    
    // Set access token (NB: Keep this private!)
    process.env.HF_TOKEN = 'hf_...';
    
    // ... rest of your code

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/guides/private.md)

[‚ÜêServer-side Inference in
Node.js](/docs/transformers.js/v3.0.0/en/tutorials/node) [Server-side Audio
Processing in Node.js‚Üí](/docs/transformers.js/v3.0.0/en/guides/node-audio-
processing)

Accessing Private/Gated Models Step 1: Generating a User Access Token Step 2:
Using the access token in Transformers.js

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Transformers.js

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Transformers.js

State-of-the-art Machine Learning for the web. Run ü§ó Transformers directly in
your browser, with no need for a server!

Transformers.js is designed to be functionally equivalent to Hugging Face‚Äôs
[transformers](https://github.com/huggingface/transformers) python library,
meaning you can run the same pretrained models using a very similar API. These
models support common tasks in different modalities, such as:

  * üìù **Natural Language Processing** : text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.
  * üñºÔ∏è **Computer Vision** : image classification, object detection, segmentation, and depth estimation.
  * üó£Ô∏è **Audio** : automatic speech recognition, audio classification, and text-to-speech.
  * üêô **Multimodal** : embeddings, zero-shot audio classification, zero-shot image classification, and zero-shot object detection.

Transformers.js uses [ONNX Runtime](https://onnxruntime.ai/) to run models in
the browser. The best part about it, is that you can easily convert your
pretrained PyTorch, TensorFlow, or JAX models to ONNX using [ü§ó
Optimum](https://github.com/huggingface/optimum#onnx--onnx-runtime).

For more information, check out the full
[documentation](https://huggingface.co/docs/transformers.js).

##  Quick tour

It‚Äôs super simple to translate from existing code! Just like the python
library, we support the `pipeline` API. Pipelines group together a pretrained
model with preprocessing of inputs and postprocessing of outputs, making it
the easiest way to run models with the library.

**Python (original)** | **Javascript (ours)**  
---|---  
Copied

    
    
    from transformers import pipeline
    
    # Allocate a pipeline for sentiment-analysis
    pipe = pipeline('sentiment-analysis')
    
    out = pipe('I love transformers!')
    # [{'label': 'POSITIVE', 'score': 0.999806941}]

|  Copied

    
    
    import { pipeline } from '@huggingface/transformers';
    
    // Allocate a pipeline for sentiment-analysis
    let pipe = await pipeline('sentiment-analysis');
    
    let out = await pipe('I love transformers!');
    // [{'label': 'POSITIVE', 'score': 0.999817686}]  
  
You can also use a different model by specifying the model id or path as the
second argument to the `pipeline` function. For example:

Copied

    
    
    // Use a different model for sentiment-analysis
    let pipe = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment');

##  Contents

The documentation is organized into 4 sections:

  1. **GET STARTED** provides a quick tour of the library and installation instructions to get up and running.
  2. **TUTORIALS** are a great place to start if you‚Äôre a beginner! We also include sample applications for you to play around with!
  3. **DEVELOPER GUIDES** show you how to use the library to achieve a specific goal.
  4. **API REFERENCE** describes all classes and functions, as well as their available parameters and types.

##  Examples

Want to jump straight in? Get started with one of our sample
applications/templates:

Name | Description | Links  
---|---|---  
Whisper Web | Speech recognition w/ Whisper | [code](https://github.com/xenova/whisper-web), [demo](https://huggingface.co/spaces/Xenova/whisper-web)  
Doodle Dash | Real-time sketch-recognition game | [blog](https://huggingface.co/blog/ml-web-games), [code](https://github.com/xenova/doodle-dash), [demo](https://huggingface.co/spaces/Xenova/doodle-dash)  
Code Playground | In-browser code completion website | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/code-completion/), [demo](https://huggingface.co/spaces/Xenova/ai-code-playground)  
Semantic Image Search (client-side) | Search for images with text | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/semantic-image-search-client/), [demo](https://huggingface.co/spaces/Xenova/semantic-image-search-client)  
Semantic Image Search (server-side) | Search for images with text (Supabase) | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/semantic-image-search/), [demo](https://huggingface.co/spaces/Xenova/semantic-image-search)  
Vanilla JavaScript | In-browser object detection | [video](https://scrimba.com/scrim/cKm9bDAg), [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/vanilla-js/), [demo](https://huggingface.co/spaces/Scrimba/vanilla-js-object-detector)  
React | Multilingual translation website | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/react-translator/), [demo](https://huggingface.co/spaces/Xenova/react-translator)  
Text to speech (client-side) | In-browser speech synthesis | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/text-to-speech-client/), [demo](https://huggingface.co/spaces/Xenova/text-to-speech-client)  
Browser extension | Text classification extension | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/extension/)  
Electron | Text classification application | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/electron/)  
Next.js (client-side) | Sentiment analysis (in-browser inference) | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/next-client/), [demo](https://huggingface.co/spaces/Xenova/next-example-app)  
Next.js (server-side) | Sentiment analysis (Node.js inference) | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/next-server/), [demo](https://huggingface.co/spaces/Xenova/next-server-example-app)  
Node.js | Sentiment analysis API | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/node/)  
Demo site | A collection of demos | [code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/demo-site/), [demo](https://xenova.github.io/transformers.js/)  
  
Check out the Transformers.js [template](https://huggingface.co/new-
space?template=static-templates%2Ftransformers.js) on Hugging Face to get
started in one click!

##  Supported tasks/models

Here is the list of all tasks and architectures currently supported by
Transformers.js. If you don‚Äôt see your task/model listed here or it is not yet
supported, feel free to open up a feature request
[here](https://github.com/huggingface/transformers.js/issues/new/choose).

To find compatible models on the Hub, select the ‚Äútransformers.js‚Äù library tag
in the filter menu (or visit [this
link](https://huggingface.co/models?library=transformers.js)). You can refine
your search by selecting the task you‚Äôre interested in (e.g., [text-
classification](https://huggingface.co/models?pipeline_tag=text-
classification&library=transformers.js)).

###  Tasks

####  Natural Language Processing

Task | ID | Description | Supported?  
---|---|---|---  
[Fill-Mask](https://huggingface.co/tasks/fill-mask) | `fill-mask` | Masking some of the words in a sentence and predicting which words should replace those masks. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FillMaskPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=fill-
mask&library=transformers.js)  
[Question Answering](https://huggingface.co/tasks/question-answering) | `question-answering` | Retrieve the answer to a question from a given text. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.QuestionAnsweringPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=question-
answering&library=transformers.js)  
[Sentence Similarity](https://huggingface.co/tasks/sentence-similarity) | `sentence-similarity` | Determining how similar two texts are. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=feature-
extraction&library=transformers.js)  
[Summarization](https://huggingface.co/tasks/summarization) | `summarization` | Producing a shorter version of a document while preserving its important information. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.SummarizationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=summarization&library=transformers.js)  
[Table Question Answering](https://huggingface.co/tasks/table-question-answering) | `table-question-answering` | Answering a question about information from a given table. | ‚ùå  
[Text Classification](https://huggingface.co/tasks/text-classification) | `text-classification` or `sentiment-analysis` | Assigning a label or class to a given text. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=text-
classification&library=transformers.js)  
[Text Generation](https://huggingface.co/tasks/text-generation#completion-generation-models) | `text-generation` | Producing new text by predicting the next word in a sequence. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextGenerationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=text-
generation&library=transformers.js)  
[Text-to-text Generation](https://huggingface.co/tasks/text-generation#text-to-text-generation-models) | `text2text-generation` | Converting one text sequence into another text sequence. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.Text2TextGenerationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=text2text-
generation&library=transformers.js)  
[Token Classification](https://huggingface.co/tasks/token-classification) | `token-classification` or `ner` | Assigning a label to each token in a text. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TokenClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=token-
classification&library=transformers.js)  
[Translation](https://huggingface.co/tasks/translation) | `translation` | Converting text from one language to another. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TranslationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=translation&library=transformers.js)  
[Zero-Shot Classification](https://huggingface.co/tasks/zero-shot-classification) | `zero-shot-classification` | Classifying text into classes that are unseen during training. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=zero-shot-
classification&library=transformers.js)  
[Feature Extraction](https://huggingface.co/tasks/feature-extraction) | `feature-extraction` | Transforming raw data into numerical features that can be processed while preserving the information in the original dataset. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=feature-
extraction&library=transformers.js)  
  
####  Vision

Task | ID | Description | Supported?  
---|---|---|---  
[Depth Estimation](https://huggingface.co/tasks/depth-estimation) | `depth-estimation` | Predicting the depth of objects present in an image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DepthEstimationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=depth-
estimation&library=transformers.js)  
[Image Classification](https://huggingface.co/tasks/image-classification) | `image-classification` | Assigning a label or class to an entire image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-
classification&library=transformers.js)  
[Image Segmentation](https://huggingface.co/tasks/image-segmentation) | `image-segmentation` | Divides an image into segments where each pixel is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageSegmentationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-
segmentation&library=transformers.js)  
[Image-to-Image](https://huggingface.co/tasks/image-to-image) | `image-to-image` | Transforming a source image to match the characteristics of a target image or a target image domain. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToImagePipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-to-
image&library=transformers.js)  
[Mask Generation](https://huggingface.co/tasks/mask-generation) | `mask-generation` | Generate masks for the objects in an image. | ‚ùå  
[Object Detection](https://huggingface.co/tasks/object-detection) | `object-detection` | Identify objects of certain defined classes within an image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ObjectDetectionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=object-
detection&library=transformers.js)  
[Video Classification](https://huggingface.co/tasks/video-classification) | n/a | Assigning a label or class to an entire video. | ‚ùå  
[Unconditional Image Generation](https://huggingface.co/tasks/unconditional-image-generation) | n/a | Generating images with no condition in any context (like a prompt text or another image). | ‚ùå  
[Image Feature Extraction](https://huggingface.co/tasks/image-feature-extraction) | `image-feature-extraction` | Transforming raw data into numerical features that can be processed while preserving the information in the original image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageFeatureExtractionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-feature-
extraction&library=transformers.js)  
  
####  Audio

Task | ID | Description | Supported?  
---|---|---|---  
[Audio Classification](https://huggingface.co/tasks/audio-classification) | `audio-classification` | Assigning a label or class to a given audio. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AudioClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=audio-
classification&library=transformers.js)  
[Audio-to-Audio](https://huggingface.co/tasks/audio-to-audio) | n/a | Generating audio from an input audio source. | ‚ùå  
[Automatic Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition) | `automatic-speech-recognition` | Transcribing a given audio into text. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AutomaticSpeechRecognitionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=automatic-speech-
recognition&library=transformers.js)  
[Text-to-Speech](https://huggingface.co/tasks/text-to-speech) | `text-to-speech` or `text-to-audio` | Generating natural-sounding speech given text input. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextToAudioPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=text-to-
audio&library=transformers.js)  
  
####  Tabular

Task | ID | Description | Supported?  
---|---|---|---  
[Tabular Classification](https://huggingface.co/tasks/tabular-classification) | n/a | Classifying a target category (a group) based on set of attributes. | ‚ùå  
[Tabular Regression](https://huggingface.co/tasks/tabular-regression) | n/a | Predicting a numerical value given a set of attributes. | ‚ùå  
  
####  Multimodal

Task | ID | Description | Supported?  
---|---|---|---  
[Document Question Answering](https://huggingface.co/tasks/document-question-answering) | `document-question-answering` | Answering questions on document images. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DocumentQuestionAnsweringPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=document-question-
answering&library=transformers.js)  
[Image-to-Text](https://huggingface.co/tasks/image-to-text) | `image-to-text` | Output text from a given image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToTextPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-to-
text&library=transformers.js)  
[Text-to-Image](https://huggingface.co/tasks/text-to-image) | `text-to-image` | Generates images from input text. | ‚ùå  
[Visual Question Answering](https://huggingface.co/tasks/visual-question-answering) | `visual-question-answering` | Answering open-ended questions based on an image. | ‚ùå  
[Zero-Shot Audio Classification](https://huggingface.co/learn/audio-course/chapter4/classification_models#zero-shot-audio-classification) | `zero-shot-audio-classification` | Classifying audios into classes that are unseen during training. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotAudioClassificationPipeline)  
[(models)](https://huggingface.co/models?other=zero-shot-audio-
classification&library=transformers.js)  
[Zero-Shot Image Classification](https://huggingface.co/tasks/zero-shot-image-classification) | `zero-shot-image-classification` | Classifying images into classes that are unseen during training. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotImageClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=zero-shot-image-
classification&library=transformers.js)  
[Zero-Shot Object Detection](https://huggingface.co/tasks/zero-shot-object-detection) | `zero-shot-object-detection` | Identify objects of classes that are unseen during training. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotObjectDetectionPipeline)  
[(models)](https://huggingface.co/models?other=zero-shot-object-
detection&library=transformers.js)  
  
####  Reinforcement Learning

Task | ID | Description | Supported?  
---|---|---|---  
[Reinforcement Learning](https://huggingface.co/tasks/reinforcement-learning) | n/a | Learning from actions by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback. | ‚úÖ  
  
###  Models

  1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.
  2. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.
  3. **[BART](https://huggingface.co/docs/transformers/model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.
  4. **[BEiT](https://huggingface.co/docs/transformers/model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.
  5. **[BERT](https://huggingface.co/docs/transformers/model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.
  6. **[Blenderbot](https://huggingface.co/docs/transformers/model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.
  7. **[BlenderbotSmall](https://huggingface.co/docs/transformers/model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.
  8. **[BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).
  9. **[CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin _, Benjamin Muller_ , Pedro Javier Ortiz Su√°rez*, Yoann Dupont, Laurent Romary, √âric Villemonte de la Clergerie, Djam√© Seddah and Beno√Æt Sagot.
  10. **[Chinese-CLIP](https://huggingface.co/docs/transformers/model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.
  11. **[CLAP](https://huggingface.co/docs/transformers/model_doc/clap)** (from LAION-AI) released with the paper [Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation](https://arxiv.org/abs/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.
  12. **[CLIP](https://huggingface.co/docs/transformers/model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.
  13. **[CLIPSeg](https://huggingface.co/docs/transformers/model_doc/clipseg)** (from University of G√∂ttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L√ºddecke and Alexander Ecker.
  14. **[CodeGen](https://huggingface.co/docs/transformers/model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.
  15. **[CodeLlama](https://huggingface.co/docs/transformers/model_doc/llama_code)** (from MetaAI) released with the paper [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) by Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J√©r√©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D√©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, Gabriel Synnaeve.
  16. **[Cohere](https://huggingface.co/docs/transformers/v3.0.0/model_doc/cohere)** (from Cohere) released with the paper [Command-R: Retrieval Augmented Generation at Production Scale](https://txt.cohere.com/command-r/) by Cohere.
  17. **[ConvBERT](https://huggingface.co/docs/transformers/model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.
  18. **[ConvNeXT](https://huggingface.co/docs/transformers/model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.
  19. **[ConvNeXTV2](https://huggingface.co/docs/transformers/model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.
  20. **[DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.
  21. **[DeBERTa-v2](https://huggingface.co/docs/transformers/model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.
  22. **[Decision Transformer](https://huggingface.co/docs/transformers/model_doc/decision_transformer)** (from Berkeley/Facebook/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.
  23. **[DeiT](https://huggingface.co/docs/transformers/model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv√© J√©gou.
  24. **[Depth Anything](https://huggingface.co/docs/transformers/v3.0.0/model_doc/depth_anything)** (from University of Hong Kong and TikTok) released with the paper [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/abs/2401.10891) by Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, Hengshuang Zhao.
  25. **Depth Pro** (from Apple) released with the paper [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/abs/2410.02073) by Aleksei Bochkovskii, Ama√´l Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun.
  26. **[DETR](https://huggingface.co/docs/transformers/model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.
  27. **[DINOv2](https://huggingface.co/docs/transformers/model_doc/dinov2)** (from Meta AI) released with the paper [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193) by Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv√© Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.
  28. **[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers/tree/v3.0.0/examples/research_projects/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers/tree/v3.0.0/examples/research_projects/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers/tree/v3.0.0/examples/research_projects/distillation) and a German version of DistilBERT.
  29. **[DiT](https://huggingface.co/docs/transformers/model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.
  30. **[Donut](https://huggingface.co/docs/transformers/model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.
  31. **[DPT](https://huggingface.co/docs/transformers/master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Ren√© Ranftl, Alexey Bochkovskiy, Vladlen Koltun.
  32. **[EfficientNet](https://huggingface.co/docs/transformers/model_doc/efficientnet)** (from Google Brain) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) by Mingxing Tan, Quoc V. Le.
  33. **[ELECTRA](https://huggingface.co/docs/transformers/model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.
  34. **[ESM](https://huggingface.co/docs/transformers/model_doc/esm)** (from Meta AI) are transformer protein language models. **ESM-1b** was released with the paper [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. **ESM-1v** was released with the paper [Language models enable zero-shot prediction of the effects of mutations on protein function](https://doi.org/10.1101/2021.07.09.450648) by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. **ESM-2 and ESMFold** were released with the paper [Language models of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.
  35. **[Falcon](https://huggingface.co/docs/transformers/model_doc/falcon)** (from Technology Innovation Institute) by Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme.
  36. **FastViT** (from Apple) released with the paper [FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization](https://arxiv.org/abs/2303.14189) by Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel and Anurag Ranjan.
  37. **[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/v3.0.0/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei
  38. **Florence2** (from Microsoft) released with the paper [Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks](https://arxiv.org/abs/2311.06242) by Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, Lu Yuan.
  39. **[Gemma](https://huggingface.co/docs/transformers/v3.0.0/model_doc/gemma)** (from Google) released with the paper [Gemma: Open Models Based on Gemini Technology and Research](https://blog.google/technology/developers/gemma-open-models/) by the Gemma Google team.
  40. **[Gemma2](https://huggingface.co/docs/transformers/v3.0.0/model_doc/gemma2)** (from Google) released with the paper [Gemma2: Open Models Based on Gemini Technology and Research](https://blog.google/technology/developers/google-gemma-2/) by the Gemma Google team.
  41. **[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.
  42. **[GPT Neo](https://huggingface.co/docs/transformers/model_doc/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.
  43. **[GPT NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach
  44. **[GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://blog.openai.com/better-language-models/) by Alec Radford _, Jeffrey Wu_ , Rewon Child, David Luan, Dario Amodei**and Ilya Sutskever**.
  45. **[GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.
  46. **[GPTBigCode](https://huggingface.co/docs/transformers/model_doc/gpt_bigcode)** (from BigCode) released with the paper [SantaCoder: don‚Äôt reach for the stars!](https://arxiv.org/abs/2301.03988) by Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc√≠a del R√≠o, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.
  47. **[Granite](https://huggingface.co/docs/transformers/v3.0.0/model_doc/granite)** (from IBM) released with the paper [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, Rameswar Panda.
  48. **[GroupViT](https://huggingface.co/docs/transformers/model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.
  49. **[HerBERT](https://huggingface.co/docs/transformers/model_doc/herbert)** (from Allegro.pl, AGH University of Science and Technology) released with the paper [KLEJ: Comprehensive Benchmark for Polish Language Understanding](https://www.aclweb.org/anthology/2020.acl-main.111.pdf) by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, Ireneusz Gawlik.
  50. **[Hiera](https://huggingface.co/docs/transformers/model_doc/hiera)** (from Meta) released with the paper [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://arxiv.org/pdf/2306.00989) by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer.
  51. **[Hubert](https://huggingface.co/docs/transformers/model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.
  52. **JAIS** (from Core42) released with the paper [Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models](https://arxiv.org/pdf/2308.16149) by Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Xudong Han, Sondos Mahmoud Bsharat, Alham Fikri Aji, Zhiqiang Shen, Zhengzhong Liu, Natalia Vassilieva, Joel Hestness, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Hector Xuguang Ren, Preslav Nakov, Timothy Baldwin, Eric Xing.
  53. **[LongT5](https://huggingface.co/docs/transformers/model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.
  54. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.
  55. **[Llama2](https://huggingface.co/docs/transformers/model_doc/llama2)** (from The FAIR team of Meta AI) released with the paper [Llama2: Open Foundation and Fine-Tuned Chat Models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/XXX) by Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushka rMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing EllenTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom.
  56. **[LLaVa](https://huggingface.co/docs/transformers/model_doc/llava)** (from Microsoft Research & University of Wisconsin-Madison) released with the paper [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.
  57. **[M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.
  58. **[MarianMT](https://huggingface.co/docs/transformers/model_doc/marian)** Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by J√∂rg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.
  59. **[MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.
  60. **[mBART](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.
  61. **[mBART-50](https://huggingface.co/docs/transformers/model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.
  62. **[MusicGen](https://huggingface.co/docs/transformers/model_doc/musicgen)** (from Meta) released with the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre D√©fossez.
  63. **[Mistral](https://huggingface.co/docs/transformers/model_doc/mistral)** (from Mistral AI) by The [Mistral AI](https://mistral.ai) team: Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, L√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth√©e Lacroix, William El Sayed.
  64. **[MMS](https://huggingface.co/docs/transformers/model_doc/mms)** (from Facebook) released with the paper [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516) by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli.
  65. **[MobileBERT](https://huggingface.co/docs/transformers/model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
  66. **MobileCLIP** (from Apple) released with the paper [MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training](https://arxiv.org/abs/2311.17049) by Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel.
  67. **[MobileNetV1](https://huggingface.co/docs/transformers/model_doc/mobilenet_v1)** (from Google Inc.) released with the paper [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.
  68. **[MobileNetV2](https://huggingface.co/docs/transformers/model_doc/mobilenet_v2)** (from Google Inc.) released with the paper [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.
  69. **MobileNetV3** (from Google Inc.) released with the paper [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244) by Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, Hartwig Adam.
  70. **MobileNetV4** (from Google Inc.) released with the paper [MobileNetV4 - Universal Models for the Mobile Ecosystem](https://arxiv.org/abs/2404.10518) by Danfeng Qin, Chas Leichner, Manolis Delakis, Marco Fornoni, Shixin Luo, Fan Yang, Weijun Wang, Colby Banbury, Chengxi Ye, Berkin Akin, Vaibhav Aggarwal, Tenghui Zhu, Daniele Moro, Andrew Howard.
  71. **[MobileViT](https://huggingface.co/docs/transformers/model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari.
  72. **[MobileViTV2](https://huggingface.co/docs/transformers/model_doc/mobilevitv2)** (from Apple) released with the paper [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) by Sachin Mehta and Mohammad Rastegari.
  73. **Moondream1** released in the repository [moondream](https://github.com/vikhyat/moondream) by vikhyat.
  74. **[MPNet](https://huggingface.co/docs/transformers/model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.
  75. **[MPT](https://huggingface.co/docs/transformers/model_doc/mpt)** (from MosaiML) released with the repository [llm-foundry](https://github.com/mosaicml/llm-foundry/) by the MosaicML NLP Team.
  76. **[MT5](https://huggingface.co/docs/transformers/model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.
  77. **[NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.
  78. **[Nougat](https://huggingface.co/docs/transformers/model_doc/nougat)** (from Meta AI) released with the paper [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418) by Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic.
  79. **OpenELM** (from Apple) released with the paper [OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework](https://arxiv.org/abs/2404.14619) by Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari.
  80. **[OPT](https://huggingface.co/docs/transformers/master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.
  81. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.
  82. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
  83. **[Phi](https://huggingface.co/docs/transformers/v3.0.0/model_doc/phi)** (from Microsoft) released with the papers - [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) by Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S√©bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee and Yuanzhi Li, [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) by Yuanzhi Li, S√©bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar and Yin Tat Lee.
  84. **[Phi3](https://huggingface.co/docs/transformers/v3.0.0/model_doc/phi3)** (from Microsoft) released with the paper [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) by Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, S√©bastien Bubeck, Martin Cai, Caio C√©sar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, Xiren Zhou.
  85. **[PVT](https://huggingface.co/docs/transformers/v3.0.0/model_doc/pvt)** (from Nanjing University, The University of Hong Kong etc.) released with the paper [Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/pdf/2102.12122.pdf) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao.
  86. **PyAnnote** released in the repository [pyannote/pyannote-audio](https://github.com/pyannote/pyannote-audio) by Herv√© Bredin.
  87. **[Qwen2](https://huggingface.co/docs/transformers/model_doc/qwen2)** (from the Qwen team, Alibaba Group) released with the paper [Qwen Technical Report](https://arxiv.org/abs/2309.16609) by Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou and Tianhang Zhu.
  88. **[ResNet](https://huggingface.co/docs/transformers/model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.
  89. **[RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
  90. **[RoFormer](https://huggingface.co/docs/transformers/model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.
  91. **[RT-DETR](https://huggingface.co/docs/transformers/model_doc/rt_detr)** (from Baidu), released together with the paper [DETRs Beat YOLOs on Real-time Object Detection](https://arxiv.org/abs/2304.08069) by Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen.
  92. **Sapiens** (from Meta AI) released with the paper [Sapiens: Foundation for Human Vision Models](https://arxiv.org/pdf/2408.12569) by Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, Shunsuke Saito.
  93. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.
  94. **[Segment Anything](https://huggingface.co/docs/transformers/model_doc/sam)** (from Meta AI) released with the paper [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.
  95. **[SigLIP](https://huggingface.co/docs/transformers/v3.0.0/model_doc/siglip)** (from Google AI) released with the paper [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) by Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer.
  96. **[SpeechT5](https://huggingface.co/docs/transformers/model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.
  97. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.
  98. **[StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm)** (from Stability AI) released with the paper [StableLM 3B 4E1T (Technical Report)](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) by Jonathan Tow, Marco Bellagente, Dakota Mahan, Carlos Riquelme Ruiz, Duy Phung, Maksym Zhuravinskyi, Nathan Cooper, Nikhil Pinnaparaju, Reshinth Adithyan, and James Baicoianu.
  99. **[Starcoder2](https://huggingface.co/docs/transformers/v3.0.0/model_doc/starcoder2)** (from BigCode team) released with the paper [StarCoder 2 and The Stack v2: The Next Generation](https://arxiv.org/abs/2402.19173) by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krau√ü, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Mu√±oz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.
  100. **[Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.
  101. **[Swin2SR](https://huggingface.co/docs/transformers/model_doc/swin2sr)** (from University of W√ºrzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.
  102. **[T5](https://huggingface.co/docs/transformers/model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.
  103. **[T5v1.1](https://huggingface.co/docs/transformers/model_doc/t5v1.1)** (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/v3.0.0/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.
  104. **[Table Transformer](https://huggingface.co/docs/transformers/model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.
  105. **[TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.
  106. **[UniSpeech](https://huggingface.co/docs/transformers/model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.
  107. **[UniSpeechSat](https://huggingface.co/docs/transformers/model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.
  108. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.
  109. **[ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, Ross Girshick.
  110. **[ViTMatte](https://huggingface.co/docs/transformers/model_doc/vitmatte)** (from HUST-VL) released with the paper [ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.
  111. **[ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn)** (from Meta AI) released with the paper [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.
  112. **[VITS](https://huggingface.co/docs/transformers/model_doc/vits)** (from Kakao Enterprise) released with the paper [Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech](https://arxiv.org/abs/2106.06103) by Jaehyeon Kim, Jungil Kong, Juhee Son.
  113. **[Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.
  114. **[Wav2Vec2-BERT](https://huggingface.co/docs/transformers/v3.0.0/model_doc/wav2vec2-bert)** (from Meta AI) released with the paper [Seamless: Multilingual Expressive and Streaming Speech Translation](https://ai.meta.com/research/publications/seamless-multilingual-expressive-and-streaming-speech-translation/) by the Seamless Communication team.
  115. **[WavLM](https://huggingface.co/docs/transformers/model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.
  116. **[Whisper](https://huggingface.co/docs/transformers/model_doc/whisper)** (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.
  117. **[XLM](https://huggingface.co/docs/transformers/model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.
  118. **[XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau _, Kartikay Khandelwal_ , Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.
  119. **[YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/index.md)

[Installation‚Üí](/docs/transformers.js/v3.0.0/en/installation)

Transformers.js Quick tour Contents Examples Supported tasks/models Tasks
Natural Language Processing Vision Audio Tabular Multimodal Reinforcement
Learning Models

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Installation

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Installation

To install via [NPM](https://www.npmjs.com/package/@huggingface/transformers),
run:

Copied

    
    
    npm i @huggingface/transformers

Alternatively, you can use it in vanilla JS, without any bundler, by using a
CDN or static hosting. For example, using [ES
Modules](https://developer.mozilla.org/en-
US/docs/Web/JavaScript/Guide/Modules), you can import the library with:

Copied

    
    
    <script type="module">
        import { pipeline } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.0.0';
    </script>

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/installation.md)

[‚Üêü§ó Transformers.js](/docs/transformers.js/v3.0.0/en/index) [The pipeline
API‚Üí](/docs/transformers.js/v3.0.0/en/pipelines)

Installation

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

The pipeline API

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  The pipeline API

Just like the [transformers Python
library](https://github.com/huggingface/transformers), Transformers.js
provides users with a simple way to leverage the power of transformers. The
`pipeline()` function is the easiest and fastest way to use a pretrained model
for inference.

For the full list of available tasks/pipelines, check out this table.

##  The basics

Start by creating an instance of `pipeline()` and specifying a task you want
to use it for. For example, to create a sentiment analysis pipeline, you can
do:

Copied

    
    
    import { pipeline } from '@huggingface/transformers';
    
    let classifier = await pipeline('sentiment-analysis');

When running for the first time, the `pipeline` will download and cache the
default pretrained model associated with the task. This can take a while, but
subsequent calls will be much faster.

By default, models will be downloaded from the [Hugging Face
Hub](https://huggingface.co/models) and stored in [browser
cache](https://developer.mozilla.org/en-US/docs/Web/API/Cache), but there are
ways to specify custom models and cache locations. For more information see
[here](./custom_usage).

You can now use the classifier on your target text by calling it as a
function:

Copied

    
    
    let result = await classifier('I love transformers!');
    // [{'label': 'POSITIVE', 'score': 0.9998}]

If you have multiple inputs, you can pass them as an array:

Copied

    
    
    let result = await classifier(['I love transformers!', 'I hate transformers!']);
    // [{'label': 'POSITIVE', 'score': 0.9998}, {'label': 'NEGATIVE', 'score': 0.9982}]

You can also specify a different model to use for the pipeline by passing it
as the second argument to the `pipeline()` function. For example, to use a
different model for sentiment analysis (like one trained to predict sentiment
of a review as a number of stars between 1 and 5), you can do:

Copied

    
    
    let reviewer = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment');
    
    let result = await reviewer('The Shawshank Redemption is a true masterpiece of cinema.');
    // [{label: '5 stars', score: 0.8167929649353027}]

Transformers.js supports loading any model hosted on the Hugging Face Hub,
provided it has ONNX weights (located in a subfolder called `onnx`). For more
information on how to convert your PyTorch, TensorFlow, or JAX model to ONNX,
see the [conversion section](./custom_usage#convert-your-models-to-onnx).

The `pipeline()` function is a great way to quickly use a pretrained model for
inference, as it takes care of all the preprocessing and postprocessing for
you. For example, if you want to perform Automatic Speech Recognition (ASR)
using OpenAI‚Äôs Whisper model, you can do:

Copied

    
    
    // Allocate a pipeline for Automatic Speech Recognition
    let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-small.en');
    
    // Transcribe an audio file, loaded from a URL.
    let result = await transcriber('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/v3.0.0/mlk.flac');
    // {text: ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}

##  Pipeline options

###  Loading

We offer a variety of options to control how models are loaded from the
Hugging Face Hub (or locally). By default, the _quantized_ version of the
model is used, which is smaller and faster, but usually less accurate. To
override this behaviour (i.e., use the unquantized model), you can use a
custom `PretrainedOptions` object as the third parameter to the `pipeline`
function:

Copied

    
    
    // Allocation a pipeline for feature extraction, using the unquantized model
    const pipe = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2', {
        quantized: false,
    });

You can also specify which revision of the model to use, by passing a
`revision` parameter. Since the Hugging Face Hub uses a git-based versioning
system, you can use any valid git revision specifier (e.g., branch name or
commit hash)

Copied

    
    
    let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en', {
        revision: 'output_attentions',
    });

For the full list of options, check out the
[PretrainedOptions](./api/utils/hub#module_utils/hub..PretrainedOptions)
documentation.

###  Running

Many pipelines have additional options that you can specify. For example, when
using a model that does multilingual translation, you can specify the source
and target languages like this:

Copied

    
    
    // Allocation a pipeline for translation
    let translator = await pipeline('translation', 'Xenova/nllb-200-distilled-600M');
    
    // Translate from English to Greek
    let result = await translator('I like to walk my dog.', {
        src_lang: 'eng_Latn',
        tgt_lang: 'ell_Grek'
    });
    // [ { translation_text: 'ŒúŒøœÖ Œ±œÅŒ≠œÉŒµŒπ ŒΩŒ± œÄŒµœÅœÄŒ±œÑŒ¨œâ œÑŒø œÉŒ∫œÖŒªŒØ ŒºŒøœÖ.' } ]
    
    // Translate back to English
    let result2 = await translator(result[0].translation_text, {
        src_lang: 'ell_Grek',
        tgt_lang: 'eng_Latn'
    });
    // [ { translation_text: 'I like to walk my dog.' } ]

When using models that support auto-regressive generation, you can specify
generation parameters like the number of new tokens, sampling methods,
temperature, repetition penalty, and much more. For a full list of available
parameters, see to the
[GenerationConfig](./api/utils/generation#module_utils/generation.GenerationConfig)
class.

For example, to generate a poem using `LaMini-Flan-T5-783M`, you can do:

Copied

    
    
    // Allocate a pipeline for text2text-generation
    let poet = await pipeline('text2text-generation', 'Xenova/LaMini-Flan-T5-783M');
    let result = await poet('Write me a love poem about cheese.', {
        max_new_tokens: 200,
        temperature: 0.9,
        repetition_penalty: 2.0,
        no_repeat_ngram_size: 3,
    });

Logging `result[0].generated_text` to the console gives:

Copied

    
    
    Cheese, oh cheese! You're the perfect comfort food.
    Your texture so smooth and creamy you can never get old.
    With every bite it melts in your mouth like buttery delights
    that make me feel right at home with this sweet treat of mine. 
    
    From classic to bold flavor combinations,
    I love how versatile you are as an ingredient too?
    Cheddar is my go-to for any occasion or mood; 
    It adds depth and richness without being overpowering its taste buds alone

For more information on the available options for each pipeline, refer to the
[API Reference](./api/pipelines). If you would like more control over the
inference process, you can use the [`AutoModel`](./api/models),
[`AutoTokenizer`](./api/tokenizers), or [`AutoProcessor`](./api/processors)
classes instead.

##  Available tasks

###  Tasks

####  Natural Language Processing

Task | ID | Description | Supported?  
---|---|---|---  
[Fill-Mask](https://huggingface.co/tasks/fill-mask) | `fill-mask` | Masking some of the words in a sentence and predicting which words should replace those masks. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FillMaskPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=fill-
mask&library=transformers.js)  
[Question Answering](https://huggingface.co/tasks/question-answering) | `question-answering` | Retrieve the answer to a question from a given text. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.QuestionAnsweringPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=question-
answering&library=transformers.js)  
[Sentence Similarity](https://huggingface.co/tasks/sentence-similarity) | `sentence-similarity` | Determining how similar two texts are. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=feature-
extraction&library=transformers.js)  
[Summarization](https://huggingface.co/tasks/summarization) | `summarization` | Producing a shorter version of a document while preserving its important information. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.SummarizationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=summarization&library=transformers.js)  
[Table Question Answering](https://huggingface.co/tasks/table-question-answering) | `table-question-answering` | Answering a question about information from a given table. | ‚ùå  
[Text Classification](https://huggingface.co/tasks/text-classification) | `text-classification` or `sentiment-analysis` | Assigning a label or class to a given text. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=text-
classification&library=transformers.js)  
[Text Generation](https://huggingface.co/tasks/text-generation#completion-generation-models) | `text-generation` | Producing new text by predicting the next word in a sequence. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextGenerationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=text-
generation&library=transformers.js)  
[Text-to-text Generation](https://huggingface.co/tasks/text-generation#text-to-text-generation-models) | `text2text-generation` | Converting one text sequence into another text sequence. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.Text2TextGenerationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=text2text-
generation&library=transformers.js)  
[Token Classification](https://huggingface.co/tasks/token-classification) | `token-classification` or `ner` | Assigning a label to each token in a text. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TokenClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=token-
classification&library=transformers.js)  
[Translation](https://huggingface.co/tasks/translation) | `translation` | Converting text from one language to another. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TranslationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=translation&library=transformers.js)  
[Zero-Shot Classification](https://huggingface.co/tasks/zero-shot-classification) | `zero-shot-classification` | Classifying text into classes that are unseen during training. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=zero-shot-
classification&library=transformers.js)  
[Feature Extraction](https://huggingface.co/tasks/feature-extraction) | `feature-extraction` | Transforming raw data into numerical features that can be processed while preserving the information in the original dataset. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.FeatureExtractionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=feature-
extraction&library=transformers.js)  
  
####  Vision

Task | ID | Description | Supported?  
---|---|---|---  
[Depth Estimation](https://huggingface.co/tasks/depth-estimation) | `depth-estimation` | Predicting the depth of objects present in an image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DepthEstimationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=depth-
estimation&library=transformers.js)  
[Image Classification](https://huggingface.co/tasks/image-classification) | `image-classification` | Assigning a label or class to an entire image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-
classification&library=transformers.js)  
[Image Segmentation](https://huggingface.co/tasks/image-segmentation) | `image-segmentation` | Divides an image into segments where each pixel is mapped to an object. This task has multiple variants such as instance segmentation, panoptic segmentation and semantic segmentation. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageSegmentationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-
segmentation&library=transformers.js)  
[Image-to-Image](https://huggingface.co/tasks/image-to-image) | `image-to-image` | Transforming a source image to match the characteristics of a target image or a target image domain. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToImagePipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-to-
image&library=transformers.js)  
[Mask Generation](https://huggingface.co/tasks/mask-generation) | `mask-generation` | Generate masks for the objects in an image. | ‚ùå  
[Object Detection](https://huggingface.co/tasks/object-detection) | `object-detection` | Identify objects of certain defined classes within an image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ObjectDetectionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=object-
detection&library=transformers.js)  
[Video Classification](https://huggingface.co/tasks/video-classification) | n/a | Assigning a label or class to an entire video. | ‚ùå  
[Unconditional Image Generation](https://huggingface.co/tasks/unconditional-image-generation) | n/a | Generating images with no condition in any context (like a prompt text or another image). | ‚ùå  
[Image Feature Extraction](https://huggingface.co/tasks/image-feature-extraction) | `image-feature-extraction` | Transforming raw data into numerical features that can be processed while preserving the information in the original image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageFeatureExtractionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-feature-
extraction&library=transformers.js)  
  
####  Audio

Task | ID | Description | Supported?  
---|---|---|---  
[Audio Classification](https://huggingface.co/tasks/audio-classification) | `audio-classification` | Assigning a label or class to a given audio. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AudioClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=audio-
classification&library=transformers.js)  
[Audio-to-Audio](https://huggingface.co/tasks/audio-to-audio) | n/a | Generating audio from an input audio source. | ‚ùå  
[Automatic Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition) | `automatic-speech-recognition` | Transcribing a given audio into text. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.AutomaticSpeechRecognitionPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=automatic-speech-
recognition&library=transformers.js)  
[Text-to-Speech](https://huggingface.co/tasks/text-to-speech) | `text-to-speech` or `text-to-audio` | Generating natural-sounding speech given text input. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.TextToAudioPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=text-to-
audio&library=transformers.js)  
  
####  Tabular

Task | ID | Description | Supported?  
---|---|---|---  
[Tabular Classification](https://huggingface.co/tasks/tabular-classification) | n/a | Classifying a target category (a group) based on set of attributes. | ‚ùå  
[Tabular Regression](https://huggingface.co/tasks/tabular-regression) | n/a | Predicting a numerical value given a set of attributes. | ‚ùå  
  
####  Multimodal

Task | ID | Description | Supported?  
---|---|---|---  
[Document Question Answering](https://huggingface.co/tasks/document-question-answering) | `document-question-answering` | Answering questions on document images. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.DocumentQuestionAnsweringPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=document-question-
answering&library=transformers.js)  
[Image-to-Text](https://huggingface.co/tasks/image-to-text) | `image-to-text` | Output text from a given image. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ImageToTextPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=image-to-
text&library=transformers.js)  
[Text-to-Image](https://huggingface.co/tasks/text-to-image) | `text-to-image` | Generates images from input text. | ‚ùå  
[Visual Question Answering](https://huggingface.co/tasks/visual-question-answering) | `visual-question-answering` | Answering open-ended questions based on an image. | ‚ùå  
[Zero-Shot Audio Classification](https://huggingface.co/learn/audio-course/chapter4/classification_models#zero-shot-audio-classification) | `zero-shot-audio-classification` | Classifying audios into classes that are unseen during training. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotAudioClassificationPipeline)  
[(models)](https://huggingface.co/models?other=zero-shot-audio-
classification&library=transformers.js)  
[Zero-Shot Image Classification](https://huggingface.co/tasks/zero-shot-image-classification) | `zero-shot-image-classification` | Classifying images into classes that are unseen during training. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotImageClassificationPipeline)  
[(models)](https://huggingface.co/models?pipeline_tag=zero-shot-image-
classification&library=transformers.js)  
[Zero-Shot Object Detection](https://huggingface.co/tasks/zero-shot-object-detection) | `zero-shot-object-detection` | Identify objects of classes that are unseen during training. | ‚úÖ [(docs)](https://huggingface.co/docs/transformers.js/api/pipelines#module_pipelines.ZeroShotObjectDetectionPipeline)  
[(models)](https://huggingface.co/models?other=zero-shot-object-
detection&library=transformers.js)  
  
####  Reinforcement Learning

Task | ID | Description | Supported?  
---|---|---|---  
[Reinforcement Learning](https://huggingface.co/tasks/reinforcement-learning) | n/a | Learning from actions by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback. | ‚úÖ  
[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/pipelines.md)

[‚ÜêInstallation](/docs/transformers.js/v3.0.0/en/installation) [Custom
usage‚Üí](/docs/transformers.js/v3.0.0/en/custom_usage)

The pipeline API The basics Pipeline options Loading Running Available tasks
Tasks Natural Language Processing Vision Audio Tabular Multimodal
Reinforcement Learning

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Building a browser extension

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Building a browser extension

_Full tutorial coming soon‚Ä¶_ In the meantime, check out the example
application:
<https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/extension>

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/tutorials/browser-
extension.md)

[‚ÜêBuilding a Next.js
Application](/docs/transformers.js/v3.0.0/en/tutorials/next) [Building an
Electron Application‚Üí](/docs/transformers.js/v3.0.0/en/tutorials/electron)

Building a browser extension

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Building an Electron application

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Building an Electron application

_Full tutorial coming soon‚Ä¶_ In the meantime, check out the example
application:
<https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/electron>

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/tutorials/electron.md)

[‚ÜêBuilding a Browser
Extension](/docs/transformers.js/v3.0.0/en/tutorials/browser-extension)
[Server-side Inference in
Node.js‚Üí](/docs/transformers.js/v3.0.0/en/tutorials/node)

Building an Electron application

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Building a Next.js application

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Building a Next.js application

In this tutorial, we‚Äôll build a simple Next.js application that performs
sentiment analysis using Transformers.js! Since Transformers.js can run in the
browser or in Node.js, you can choose whether you want to perform inference
client-side or server-side (we‚Äôll show you how to do both). In either case, we
will be developing with the new [App Router](https://nextjs.org/docs/app)
paradigm. The final product will look something like this:

![Demo](https://huggingface.co/datasets/Xenova/transformers.js-
docs/resolve/v3.0.0/next-demo.gif)

Useful links:

  * Demo site: [client-side](https://huggingface.co/spaces/Xenova/next-example-app) or [server-side](https://huggingface.co/spaces/Xenova/next-server-example-app)
  * Source code: [client-side](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/next-client) or [server-side](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/next-server)

##  Prerequisites

  * [Node.js](https://nodejs.org/en/) version 18+
  * [npm](https://www.npmjs.com/) version 9+

##  Client-side inference

###  Step 1: Initialise the project

Start by creating a new Next.js application using `create-next-app`:

Copied

    
    
    npx create-next-app@latest

On installation, you‚Äôll see various prompts. For this demo, we‚Äôll be selecting
those shown below in bold:

    
    
    ‚àö What is your project named? ... next
    ‚àö Would you like to use TypeScript? ... **No** / Yes
    ‚àö Would you like to use ESLint? ... No / **Yes**
    ‚àö Would you like to use Tailwind CSS? ... No / **Yes**
    ‚àö Would you like to use `src/` directory? ... No / **Yes**
    ‚àö Would you like to use App Router? (recommended) ... No / **Yes**
    ‚àö Would you like to customize the default import alias? ... **No** / Yes
    

###  Step 2: Install and configure Transformers.js

You can install Transformers.js from
[NPM](https://www.npmjs.com/package/@huggingface/transformers) with the
following command:

Copied

    
    
    npm i @huggingface/transformers

We also need to update the `next.config.js` file to ignore node-specific
modules when bundling for the browser:

Copied

    
    
    /** @type {import('next').NextConfig} */
    const nextConfig = {
        // (Optional) Export as a static site
        // See https://nextjs.org/docs/pages/building-your-application/deploying/static-exports#configuration
        output: 'export', // Feel free to modify/remove this option
    
        // Override the default webpack configuration
        webpack: (config) => {
            // See https://webpack.js.org/configuration/resolve/#resolvealias
            config.resolve.alias = {
                ...config.resolve.alias,
                "sharp$": false,
                "onnxruntime-node$": false,
            }
            return config;
        },
    }
    
    module.exports = nextConfig

Next, we‚Äôll create a new [Web Worker](https://developer.mozilla.org/en-
US/docs/Web/API/Web_Workers_API/Using_web_workers) script where we‚Äôll place
all ML-related code. This is to ensure that the main thread is not blocked
while the model is loading and performing inference. For this application,
we‚Äôll be using [`Xenova/distilbert-base-uncased-finetuned-
sst-2-english`](https://huggingface.co/Xenova/distilbert-base-uncased-
finetuned-sst-2-english), a ~67M parameter model finetuned on the [Stanford
Sentiment Treebank](https://huggingface.co/datasets/sst) dataset. Add the
following code to `./src/app/worker.js`:

Copied

    
    
    import { pipeline, env } from "@huggingface/transformers";
    
    // Skip local model check
    env.allowLocalModels = false;
    
    // Use the Singleton pattern to enable lazy construction of the pipeline.
    class PipelineSingleton {
        static task = 'text-classification';
        static model = 'Xenova/distilbert-base-uncased-finetuned-sst-2-english';
        static instance = null;
    
        static async getInstance(progress_callback = null) {
            if (this.instance === null) {
                this.instance = pipeline(this.task, this.model, { progress_callback });
            }
            return this.instance;
        }
    }
    
    // Listen for messages from the main thread
    self.addEventListener('message', async (event) => {
        // Retrieve the classification pipeline. When called for the first time,
        // this will load the pipeline and save it for future use.
        let classifier = await PipelineSingleton.getInstance(x => {
            // We also add a progress callback to the pipeline so that we can
            // track model loading.
            self.postMessage(x);
        });
    
        // Actually perform the classification
        let output = await classifier(event.data.text);
    
        // Send the output back to the main thread
        self.postMessage({
            status: 'complete',
            output: output,
        });
    });
    

###  Step 3: Design the user interface

We‚Äôll now modify the default `./src/app/page.js` file so that it connects to
our worker thread. Since we‚Äôll only be performing in-browser inference, we can
opt-in to Client components using the [`'use client'`
directive](https://nextjs.org/docs/getting-started/react-essentials#the-use-
client-directive).

Copied

    
    
    'use client'
    
    import { useState, useEffect, useRef, useCallback } from 'react'
    
    export default function Home() {
      /* TODO: Add state variables */
    
      // Create a reference to the worker object.
      const worker = useRef(null);
    
      // We use the `useEffect` hook to set up the worker as soon as the `App` component is mounted.
      useEffect(() => {
        if (!worker.current) {
          // Create the worker if it does not yet exist.
          worker.current = new Worker(new URL('./worker.js', import.meta.url), {
            type: 'module'
          });
        }
    
        // Create a callback function for messages from the worker thread.
        const onMessageReceived = (e) => { /* TODO: See below */};
    
        // Attach the callback function as an event listener.
        worker.current.addEventListener('message', onMessageReceived);
    
        // Define a cleanup function for when the component is unmounted.
        return () => worker.current.removeEventListener('message', onMessageReceived);
      });
    
      const classify = useCallback((text) => {
        if (worker.current) {
          worker.current.postMessage({ text });
        }
      }, []);
    
      return ( /* TODO: See below */ )
    }

Initialise the following state variables at the beginning of the `Home`
component:

Copied

    
    
    // Keep track of the classification result and the model loading status.
    const [result, setResult] = useState(null);
    const [ready, setReady] = useState(null);

and fill in the `onMessageReceived` function to update these variables when
the worker thread sends a message:

Copied

    
    
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case 'initiate':
          setReady(false);
          break;
        case 'ready':
          setReady(true);
          break;
        case 'complete':
          setResult(e.data.output[0])
          break;
      }
    };

Finally, we can add a simple UI to the `Home` component, consisting of an
input textbox and a preformatted text element to display the classification
result:

Copied

    
    
    <main className="flex min-h-screen flex-col items-center justify-center p-12">
      <h1 className="text-5xl font-bold mb-2 text-center">Transformers.js</h1>
      <h2 className="text-2xl mb-4 text-center">Next.js template</h2>
    
      <input
        className="w-full max-w-xs p-2 border border-gray-300 rounded mb-4"
        type="text"
        placeholder="Enter text here"
        onInput={e => {
            classify(e.target.value);
        }}
      />
    
      {ready !== null && (
        <pre className="bg-gray-100 p-2 rounded">
          { (!ready || !result) ? 'Loading...' : JSON.stringify(result, null, 2) }
        </pre>
      )}
    </main>

You can now run your application using the following command:

Copied

    
    
    npm run dev

Visit the URL shown in the terminal (e.g., <http://localhost:3000/>) to see
your application in action!

###  (Optional) Step 4: Build and deploy

To build your application, simply run:

Copied

    
    
    npm run build

This will bundle your application and output the static files to the `out`
folder.

For this demo, we will deploy our application as a static [Hugging Face
Space](https://huggingface.co/docs/hub/spaces), but you can deploy it anywhere
you like! If you haven‚Äôt already, you can create a free Hugging Face account
[here](https://huggingface.co/join).

  1. Visit <https://huggingface.co/new-space> and fill in the form. Remember to select ‚ÄúStatic‚Äù as the space type.
  2. Click the ‚ÄúCreate space‚Äù button at the bottom of the page.
  3. Go to ‚ÄúFiles‚Äù ‚Üí ‚ÄúAdd file‚Äù ‚Üí ‚ÄúUpload files‚Äù. Drag the files from the `out` folder into the upload box and click ‚ÄúUpload‚Äù. After they have uploaded, scroll down to the button and click ‚ÄúCommit changes to main‚Äù.

**That‚Äôs it!** Your application should now be live at
`https://huggingface.co/spaces/<your-username>/<your-space-name>`!

##  Server-side inference

While there are many different ways to perform server-side inference, the
simplest (which we will discuss in this tutorial) is using the new [Route
Handlers](https://nextjs.org/docs/app/building-your-
application/routing/router-handlers) feature.

###  Step 1: Initialise the project

Start by creating a new Next.js application using `create-next-app`:

Copied

    
    
    npx create-next-app@latest

On installation, you‚Äôll see various prompts. For this demo, we‚Äôll be selecting
those shown below in bold:

    
    
    ‚àö What is your project named? ... next
    ‚àö Would you like to use TypeScript? ... **No** / Yes
    ‚àö Would you like to use ESLint? ... No / **Yes**
    ‚àö Would you like to use Tailwind CSS? ... No / **Yes**
    ‚àö Would you like to use `src/` directory? ... No / **Yes**
    ‚àö Would you like to use App Router? (recommended) ... No / **Yes**
    ‚àö Would you like to customize the default import alias? ... **No** / Yes
    

###  Step 2: Install and configure Transformers.js

You can install Transformers.js from
[NPM](https://www.npmjs.com/package/@huggingface/transformers) with the
following command:

Copied

    
    
    npm i @huggingface/transformers

We also need to update the `next.config.js` file to prevent Webpack from
bundling certain packages:

Copied

    
    
    /** @type {import('next').NextConfig} */
    const nextConfig = {
        // (Optional) Export as a standalone site
        // See https://nextjs.org/docs/pages/api-reference/next-config-js/output#automatically-copying-traced-files
        output: 'standalone', // Feel free to modify/remove this option
        
        // Indicate that these packages should not be bundled by webpack
        experimental: {
            serverComponentsExternalPackages: ['sharp', 'onnxruntime-node'],
        },
    };
    
    module.exports = nextConfig

Next, let‚Äôs set up our Route Handler. We can do this by creating two files in
a new `./src/app/classify/` directory:

  1. `pipeline.js` \- to handle the construction of our pipeline.

Copied

    
        import { pipeline } from "@huggingface/transformers";
    
    // Use the Singleton pattern to enable lazy construction of the pipeline.
    // NOTE: We wrap the class in a function to prevent code duplication (see below).
    const P = () => class PipelineSingleton {
        static task = 'text-classification';
        static model = 'Xenova/distilbert-base-uncased-finetuned-sst-2-english';
        static instance = null;
    
        static async getInstance(progress_callback = null) {
            if (this.instance === null) {
                this.instance = pipeline(this.task, this.model, { progress_callback });
            }
            return this.instance;
        }
    }
    
    let PipelineSingleton;
    if (process.env.NODE_ENV !== 'production') {
        // When running in development mode, attach the pipeline to the
        // global object so that it's preserved between hot reloads.
        // For more information, see https://vercel.com/guides/nextjs-prisma-postgres
        if (!global.PipelineSingleton) {
            global.PipelineSingleton = P();
        }
        PipelineSingleton = global.PipelineSingleton;
    } else {
        PipelineSingleton = P();
    }
    export default PipelineSingleton;

  2. `route.js` \- to process requests made to the `/classify` route.

Copied

    
        import { NextResponse } from 'next/server'
    import PipelineSingleton from './pipeline.js';
    
    export async function GET(request) {
        const text = request.nextUrl.searchParams.get('text');
        if (!text) {
            return NextResponse.json({
                error: 'Missing text parameter',
            }, { status: 400 });
        }
        // Get the classification pipeline. When called for the first time,
        // this will load the pipeline and cache it for future use.
        const classifier = await PipelineSingleton.getInstance();
    
        // Actually perform the classification
        const result = await classifier(text);
    
        return NextResponse.json(result);
    }

###  Step 3: Design the user interface

We‚Äôll now modify the default `./src/app/page.js` file to make requests to our
newly-created Route Handler.

Copied

    
    
    'use client'
    
    import { useState } from 'react'
    
    export default function Home() {
    
      // Keep track of the classification result and the model loading status.
      const [result, setResult] = useState(null);
      const [ready, setReady] = useState(null);
    
      const classify = async (text) => {
        if (!text) return;
        if (ready === null) setReady(false);
    
        // Make a request to the /classify route on the server.
        const result = await fetch(`/classify?text=${encodeURIComponent(text)}`);
    
        // If this is the first time we've made a request, set the ready flag.
        if (!ready) setReady(true);
    
        const json = await result.json();
        setResult(json);
      };
      return (
        <main className="flex min-h-screen flex-col items-center justify-center p-12">
          <h1 className="text-5xl font-bold mb-2 text-center">Transformers.js</h1>
          <h2 className="text-2xl mb-4 text-center">Next.js template (server-side)</h2>
          <input
            type="text"
            className="w-full max-w-xs p-2 border border-gray-300 rounded mb-4"
            placeholder="Enter text here"
            onInput={e => {
              classify(e.target.value);
            }}
          />
    
          {ready !== null && (
            <pre className="bg-gray-100 p-2 rounded">
              {
                (!ready || !result) ? 'Loading...' : JSON.stringify(result, null, 2)}
            </pre>
          )}
        </main>
      )
    }

You can now run your application using the following command:

Copied

    
    
    npm run dev

Visit the URL shown in the terminal (e.g., <http://localhost:3000/>) to see
your application in action!

###  (Optional) Step 4: Build and deploy

For this demo, we will build and deploy our application to [Hugging Face
Spaces](https://huggingface.co/docs/hub/spaces). If you haven‚Äôt already, you
can create a free Hugging Face account [here](https://huggingface.co/join).

  1. Create a new `Dockerfile` in your project‚Äôs root folder. You can use our [example Dockerfile](https://github.com/huggingface/transformers.js/blob/v3.0.0/examples/next-server/Dockerfile) as a template.
  2. Visit <https://huggingface.co/new-space> and fill in the form. Remember to select ‚ÄúDocker‚Äù as the space type (you can choose the ‚ÄúBlank‚Äù Docker template).
  3. Click the ‚ÄúCreate space‚Äù button at the bottom of the page.
  4. Go to ‚ÄúFiles‚Äù ‚Üí ‚ÄúAdd file‚Äù ‚Üí ‚ÄúUpload files‚Äù. Drag the files from your project folder (excluding `node_modules` and `.next`, if present) into the upload box and click ‚ÄúUpload‚Äù. After they have uploaded, scroll down to the button and click ‚ÄúCommit changes to main‚Äù.
  5. Add the following lines to the top of your `README.md`: 

Copied

    
        ---
    title: Next Server Example App
    emoji: üî•
    colorFrom: yellow
    colorTo: red
    sdk: docker
    pinned: false
    app_port: 3000
    ---

**That‚Äôs it!** Your application should now be live at
`https://huggingface.co/spaces/<your-username>/<your-space-name>`!

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/tutorials/next.md)

[‚ÜêBuilding a React
Application](/docs/transformers.js/v3.0.0/en/tutorials/react) [Building a
Browser Extension‚Üí](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)

Building a Next.js application Prerequisites Client-side inference Step 1:
Initialise the project Step 2: Install and configure Transformers.js Step 3:
Design the user interface (Optional) Step 4: Build and deploy Server-side
inference Step 1: Initialise the project Step 2: Install and configure
Transformers.js Step 3: Design the user interface (Optional) Step 4: Build and
deploy

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Server-side Inference in Node.js

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Server-side Inference in Node.js

Although Transformers.js was originally designed to be used in the browser,
it‚Äôs also able to run inference on the server. In this tutorial, we will
design a simple Node.js API that uses Transformers.js for sentiment analysis.

We‚Äôll also show you how to use the library in both CommonJS and ECMAScript
modules, so you can choose the module system that works best for your project:

  * ECMAScript modules (ESM) \- The official standard format to package JavaScript code for reuse. It‚Äôs the default module system in modern browsers, with modules imported using `import` and exported using `export`. Fortunately, starting with version 13.2.0, Node.js has stable support of ES modules.
  * CommonJS \- The default module system in Node.js. In this system, modules are imported using `require()` and exported using `module.exports`.

Although you can always use the [Python
library](https://github.com/huggingface/transformers) for server-side
inference, using Transformers.js means that you can write all of your code in
JavaScript (instead of having to set up and communicate with a separate Python
process).

**Useful links:**

  * Source code ([ESM](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/node/esm/app.js) or [CommonJS](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/node/commonjs/app.js))
  * [Documentation](https://huggingface.co/docs/transformers.js)

##  Prerequisites

  * [Node.js](https://nodejs.org/en/) version 18+
  * [npm](https://www.npmjs.com/) version 9+

##  Getting started

Let‚Äôs start by creating a new Node.js project and installing Transformers.js
via [NPM](https://www.npmjs.com/package/@huggingface/transformers):

Copied

    
    
    npm init -y
    npm i @huggingface/transformers

Next, create a new file called `app.js`, which will be the entry point for our
application. Depending on whether you‚Äôre using ECMAScript modules or CommonJS,
you will need to do some things differently (see below).

We‚Äôll also create a helper class called `MyClassificationPipeline` control the
loading of the pipeline. It uses the [singleton
pattern](https://en.wikipedia.org/wiki/Singleton_pattern) to lazily create a
single instance of the pipeline when `getInstance` is first called, and uses
this pipeline for all subsequent calls:

###  ECMAScript modules (ESM)

To indicate that your project uses ECMAScript modules, you need to add
`"type": "module"` to your `package.json`:

Copied

    
    
    {
      ...
      "type": "module",
      ...
    }

Next, you will need to add the following imports to the top of `app.js`:

Copied

    
    
    import http from 'http';
    import querystring from 'querystring';
    import url from 'url';

Following that, let‚Äôs import Transformers.js and define the
`MyClassificationPipeline` class.

Copied

    
    
    import { pipeline, env } from '@huggingface/transformers';
    
    class MyClassificationPipeline {
      static task = 'text-classification';
      static model = 'Xenova/distilbert-base-uncased-finetuned-sst-2-english';
      static instance = null;
    
      static async getInstance(progress_callback = null) {
        if (this.instance === null) {
          // NOTE: Uncomment this to change the cache directory
          // env.cacheDir = './.cache';
    
          this.instance = pipeline(this.task, this.model, { progress_callback });
        }
    
        return this.instance;
      }
    }

###  CommonJS

Start by adding the following imports to the top of `app.js`:

Copied

    
    
    const http = require('http');
    const querystring = require('querystring');
    const url = require('url');

Following that, let‚Äôs import Transformers.js and define the
`MyClassificationPipeline` class. Since Transformers.js is an ESM module, we
will need to dynamically import the library using the
[`import()`](https://developer.mozilla.org/en-
US/docs/Web/JavaScript/Reference/Operators/import) function:

Copied

    
    
    class MyClassificationPipeline {
      static task = 'text-classification';
      static model = 'Xenova/distilbert-base-uncased-finetuned-sst-2-english';
      static instance = null;
    
      static async getInstance(progress_callback = null) {
        if (this.instance === null) {
          // Dynamically import the Transformers.js library
          let { pipeline, env } = await import('@huggingface/transformers');
    
          // NOTE: Uncomment this to change the cache directory
          // env.cacheDir = './.cache';
    
          this.instance = pipeline(this.task, this.model, { progress_callback });
        }
    
        return this.instance;
      }
    }

##  Creating a basic HTTP server

Next, let‚Äôs create a basic server with the built-in
[HTTP](https://nodejs.org/api/http.html#http) module. We will listen for
requests made to the server (using the `/classify` endpoint), extract the
`text` query parameter, and run this through the pipeline.

Copied

    
    
    // Define the HTTP server
    const server = http.createServer();
    const hostname = '127.0.0.1';
    const port = 3000;
    
    // Listen for requests made to the server
    server.on('request', async (req, res) => {
      // Parse the request URL
      const parsedUrl = url.parse(req.url);
    
      // Extract the query parameters
      const { text } = querystring.parse(parsedUrl.query);
    
      // Set the response headers
      res.setHeader('Content-Type', 'application/json');
    
      let response;
      if (parsedUrl.pathname === '/classify' && text) {
        const classifier = await MyClassificationPipeline.getInstance();
        response = await classifier(text);
        res.statusCode = 200;
      } else {
        response = { 'error': 'Bad request' }
        res.statusCode = 400;
      }
    
      // Send the JSON response
      res.end(JSON.stringify(response));
    });
    
    server.listen(port, hostname, () => {
      console.log(`Server running at http://${hostname}:${port}/`);
    });
    

Since we use lazy loading, the first request made to the server will also be
responsible for loading the pipeline. If you would like to begin loading the
pipeline as soon as the server starts running, you can add the following line
of code after defining `MyClassificationPipeline`:

Copied

    
    
    MyClassificationPipeline.getInstance();

To start the server, run the following command:

Copied

    
    
    node app.js

The server should be live at <http://127.0.0.1:3000/>, which you can visit in
your web browser. You should see the following message:

Copied

    
    
    {"error":"Bad request"}

This is because we aren‚Äôt targeting the `/classify` endpoint with a valid
`text` query parameter. Let‚Äôs try again, this time with a valid request. For
example, you can visit
<http://127.0.0.1:3000/classify?text=I%20love%20Transformers.js> and you
should see:

Copied

    
    
    [{"label":"POSITIVE","score":0.9996721148490906}]

Great! We‚Äôve successfully created a basic HTTP server that uses
Transformers.js to classify text.

##  (Optional) Customization

###  Model caching

By default, the first time you run the application, it will download the model
files and cache them on your file system (in
`./node_modules/@huggingface/transformers/.cache/`). All subsequent requests
will then use this model. You can change the location of the cache by setting
`env.cacheDir`. For example, to cache the model in the `.cache` directory in
the current working directory, you can add:

Copied

    
    
    env.cacheDir = './.cache';

###  Use local models

If you want to use local model files, you can set `env.localModelPath` as
follows:

Copied

    
    
    // Specify a custom location for models (defaults to '/models/').
    env.localModelPath = '/path/to/models/';

You can also disable loading of remote models by setting
`env.allowRemoteModels` to `false`:

Copied

    
    
    // Disable the loading of remote models from the Hugging Face Hub:
    env.allowRemoteModels = false;

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/tutorials/node.md)

[‚ÜêBuilding an Electron
Application](/docs/transformers.js/v3.0.0/en/tutorials/electron) [Accessing
Private/Gated Models‚Üí](/docs/transformers.js/v3.0.0/en/guides/private)

Server-side Inference in Node.js Prerequisites Getting started ECMAScript
modules (ESM) CommonJS Creating a basic HTTP server (Optional) Customization
Model caching Use local models

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Building a React application

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Building a React application

In this tutorial, we‚Äôll be building a simple React application that performs
multilingual translation using Transformers.js! The final product will look
something like this:

![Demo](https://huggingface.co/datasets/Xenova/transformers.js-
docs/resolve/v3.0.0/react-translator-demo.gif)

Useful links:

  * [Demo site](https://huggingface.co/spaces/Xenova/react-translator)
  * [Source code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/react-translator)

##  Prerequisites

  * [Node.js](https://nodejs.org/en/) version 18+
  * [npm](https://www.npmjs.com/) version 9+

##  Step 1: Initialise the project

For this tutorial, we will use [Vite](https://vitejs.dev/) to initialise our
project. Vite is a build tool that allows us to quickly set up a React
application with minimal configuration. Run the following command in your
terminal:

Copied

    
    
    npm create vite@latest react-translator -- --template react

If prompted to install `create-vite`, type `y` and press `Enter`.

Next, enter the project directory and install the necessary development
dependencies:

Copied

    
    
    cd react-translator
    npm install

To test that our application is working, we can run the following command:

Copied

    
    
    npm run dev

Visiting the URL shown in the terminal (e.g., <http://localhost:5173/>) should
show the default ‚ÄúReact + Vite‚Äù landing page. You can stop the development
server by pressing `Ctrl` \+ `C` in the terminal.

##  Step 2: Install and configure Transformers.js

Now we get to the fun part: adding machine learning to our application! First,
install Transformers.js from
[NPM](https://www.npmjs.com/package/@huggingface/transformers) with the
following command:

Copied

    
    
    npm install @huggingface/transformers

For this application, we will use the
[Xenova/nllb-200-distilled-600M](https://huggingface.co/Xenova/nllb-200-distilled-600M)
model, which can perform multilingual translation among 200 languages. Before
we start, there are 2 things we need to take note of:

  1. ML inference can be quite computationally intensive, so it‚Äôs better to load and run the models in a separate thread from the main (UI) thread.
  2. Since the model is quite large (>1 GB), we don‚Äôt want to download it until the user clicks the ‚ÄúTranslate‚Äù button.

We can achieve both of these goals by using a [Web
Worker](https://developer.mozilla.org/en-
US/docs/Web/API/Web_Workers_API/Using_web_workers) and some [React
hooks](https://react.dev/reference/react).

  1. Create a file called `worker.js` in the `src` directory. This script will do all the heavy-lifing for us, including loading and running of the translation pipeline. To ensure the model is only loaded once, we will create the `MyTranslationPipeline` class which use the [singleton pattern](https://en.wikipedia.org/wiki/Singleton_pattern) to lazily create a single instance of the pipeline when `getInstance` is first called, and use this pipeline for all subsequent calls:

Copied

    
        import { pipeline } from '@huggingface/transformers';
    
    class MyTranslationPipeline {
      static task = 'translation';
      static model = 'Xenova/nllb-200-distilled-600M';
      static instance = null;
    
      static async getInstance(progress_callback = null) {
        if (this.instance === null) {
          this.instance = pipeline(this.task, this.model, { progress_callback });
        }
    
        return this.instance;
      }
    }

  2. Modify `App.jsx` in the `src` directory. This file is automatically created when initializing our React project, and will contain some boilerplate code. Inside the `App` function, let‚Äôs create the web worker and store a reference to it using the `useRef` hook:

Copied

    
        // Remember to import the relevant hooks
    import { useEffect, useRef, useState } from 'react'
    
    function App() {
      // Create a reference to the worker object.
      const worker = useRef(null);
    
      // We use the `useEffect` hook to setup the worker as soon as the `App` component is mounted.
      useEffect(() => {
        if (!worker.current) {
          // Create the worker if it does not yet exist.
          worker.current = new Worker(new URL('./worker.js', import.meta.url), {
              type: 'module'
          });
        }
    
        // Create a callback function for messages from the worker thread.
        const onMessageReceived = (e) => {
          // TODO: Will fill in later
        };
    
        // Attach the callback function as an event listener.
        worker.current.addEventListener('message', onMessageReceived);
    
        // Define a cleanup function for when the component is unmounted.
        return () => worker.current.removeEventListener('message', onMessageReceived);
      });
    
      return (
        // TODO: Rest of our app goes here...
      )
    }
    
    export default App
    

##  Step 3: Design the user interface

We recommend starting the development server again with `npm run dev` (if not
already running) so that you can see your changes in real-time.

First, let‚Äôs define our components. Create a folder called `components` in the
`src` directory, and create the following files:

  1. `LanguageSelector.jsx`: This component will allow the user to select the input and output languages. Check out the full list of languages [here](https://github.com/huggingface/transformers.js/blob/v3.0.0/examples/react-translator/src/components/LanguageSelector.jsx).

Copied

    
        const LANGUAGES = {
      "Acehnese (Arabic script)": "ace_Arab",
      "Acehnese (Latin script)": "ace_Latn",
      "Afrikaans": "afr_Latn",
      ...
      "Zulu": "zul_Latn",
    }
    
    export default function LanguageSelector({ type, onChange, defaultLanguage }) {
      return (
        <div className='language-selector'>
          <label>{type}: </label>
          <select onChange={onChange} defaultValue={defaultLanguage}>
            {Object.entries(LANGUAGES).map(([key, value]) => {
              return <option key={key} value={value}>{key}</option>
            })}
          </select>
        </div>
      )
    }

  2. `Progress.jsx`: This component will display the progress for downloading each model file. 

Copied

    
        export default function Progress({ text, percentage }) {
      percentage = percentage ?? 0;
      return (
        <div className="progress-container">
          <div className='progress-bar' style={{ 'width': `${percentage}%` }}>
            {text} ({`${percentage.toFixed(2)}%`})
          </div>
        </div>
      );
    }

We can now use these components in `App.jsx` by adding these imports to the
top of the file:

Copied

    
    
    import LanguageSelector from './components/LanguageSelector';
    import Progress from './components/Progress';

Let‚Äôs also add some state variables to keep track of a few things in our
application, like model loading, languages, input text, and output text. Add
the following code to the beginning of the `App` function in `src/App.jsx`:

Copied

    
    
    function App() {
    
      // Model loading
      const [ready, setReady] = useState(null);
      const [disabled, setDisabled] = useState(false);
      const [progressItems, setProgressItems] = useState([]);
    
      // Inputs and outputs
      const [input, setInput] = useState('I love walking my dog.');
      const [sourceLanguage, setSourceLanguage] = useState('eng_Latn');
      const [targetLanguage, setTargetLanguage] = useState('fra_Latn');
      const [output, setOutput] = useState('');
    
      // rest of the code...
    }

Next, we can add our custom components to the main `App` component. We will
also add two `textarea` elements for input and output text, and a `button` to
trigger the translation. Modify the `return` statement to look like this:

Copied

    
    
    return (
      <>
        <h1>Transformers.js</h1>
        <h2>ML-powered multilingual translation in React!</h2>
    
        <div className='container'>
          <div className='language-container'>
            <LanguageSelector type={"Source"} defaultLanguage={"eng_Latn"} onChange={x => setSourceLanguage(x.target.value)} />
            <LanguageSelector type={"Target"} defaultLanguage={"fra_Latn"} onChange={x => setTargetLanguage(x.target.value)} />
          </div>
    
          <div className='textbox-container'>
            <textarea value={input} rows={3} onChange={e => setInput(e.target.value)}></textarea>
            <textarea value={output} rows={3} readOnly></textarea>
          </div>
        </div>
    
        <button disabled={disabled} onClick={translate}>Translate</button>
    
        <div className='progress-bars-container'>
          {ready === false && (
            <label>Loading models... (only run once)</label>
          )}
          {progressItems.map(data => (
            <div key={data.file}>
              <Progress text={data.file} percentage={data.progress} />
            </div>
          ))}
        </div>
      </>
    )

Don‚Äôt worry about the `translate` function for now. We will define it in the
next section.

Finally, we can add some CSS to make our app look a little nicer. Modify the
following files in the `src` directory:

  1. `index.css`:

View code

Copied

    
        :root {
      font-family: Inter, system-ui, Avenir, Helvetica, Arial, sans-serif;
      line-height: 1.5;
      font-weight: 400;
      color: #213547;
      background-color: #ffffff;
    
      font-synthesis: none;
      text-rendering: optimizeLegibility;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
      -webkit-text-size-adjust: 100%;
    }
    
    body {
      margin: 0;
      display: flex;
      place-items: center;
      min-width: 320px;
      min-height: 100vh;
    }
    
    h1 {
      font-size: 3.2em;
      line-height: 1;
    }
    
    h1,
    h2 {
      margin: 8px;
    }
    
    select {
      padding: 0.3em;
      cursor: pointer;
    }
    
    textarea {
      padding: 0.6em;
    }
    
    button {
      padding: 0.6em 1.2em;
      cursor: pointer;
      font-weight: 500;
    }
    
    button[disabled] {
      cursor: not-allowed;
    }
    
    select,
    textarea,
    button {
      border-radius: 8px;
      border: 1px solid transparent;
      font-size: 1em;
      font-family: inherit;
      background-color: #f9f9f9;
      transition: border-color 0.25s;
    }
    
    select:hover,
    textarea:hover,
    button:not([disabled]):hover {
      border-color: #646cff;
    }
    
    select:focus,
    select:focus-visible,
    textarea:focus,
    textarea:focus-visible,
    button:focus,
    button:focus-visible {
      outline: 4px auto -webkit-focus-ring-color;
    }

  2. `App.css`

View code

Copied

    
        #root {
      max-width: 1280px;
      margin: 0 auto;
      padding: 2rem;
      text-align: center;
    }
    
    .language-container {
      display: flex;
      gap: 20px;
    }
    
    .textbox-container {
      display: flex;
      justify-content: center;
      gap: 20px;
      width: 800px;
    }
    
    .textbox-container>textarea, .language-selector {
      width: 50%;
    }
    
    .language-selector>select {
      width: 150px;
    }
    
    .progress-container {
      position: relative;
      font-size: 14px;
      color: white;
      background-color: #e9ecef;
      border: solid 1px;
      border-radius: 8px;
      text-align: left;
      overflow: hidden;
    }
    
    .progress-bar {
      padding: 0 4px;
      z-index: 0;
      top: 0;
      width: 1%;
      height: 100%;
      overflow: hidden;
      background-color: #007bff;
      white-space: nowrap;
    }
    
    .progress-text {
      z-index: 2;
    }
    
    .selector-container {
      display: flex;
      gap: 20px;
    }
    
    .progress-bars-container {
      padding: 8px;
      height: 140px;
    }
    
    .container {
      margin: 25px;
      display: flex;
      flex-direction: column;
      gap: 10px;
    }

##  Step 4: Connecting everything together

Now that we have a basic user interface set up, we can finally connect
everything together.

First, let‚Äôs define the `translate` function, which will be called when the
user clicks the `Translate` button. This sends a message (containing the input
text, source language, and target language) to the worker thread for
processing. We will also disable the button so the user doesn‚Äôt click it
multiple times. Add the following code just before the `return` statement in
the `App` function:

Copied

    
    
    const translate = () => {
      setDisabled(true);
      worker.current.postMessage({
        text: input,
        src_lang: sourceLanguage,
        tgt_lang: targetLanguage,
      });
    }

Now, let‚Äôs add an event listener in `src/worker.js` to listen for messages
from the main thread. We will send back messages (e.g., for model loading
progress and text streaming) to the main thread with `self.postMessage`.

Copied

    
    
    // Listen for messages from the main thread
    self.addEventListener('message', async (event) => {
      // Retrieve the translation pipeline. When called for the first time,
      // this will load the pipeline and save it for future use.
      let translator = await MyTranslationPipeline.getInstance(x => {
          // We also add a progress callback to the pipeline so that we can
          // track model loading.
          self.postMessage(x);
      });
    
      // Actually perform the translation
      let output = await translator(event.data.text, {
          tgt_lang: event.data.tgt_lang,
          src_lang: event.data.src_lang,
    
          // Allows for partial output
          callback_function: x => {
              self.postMessage({
                  status: 'update',
                  output: translator.tokenizer.decode(x[0].output_token_ids, { skip_special_tokens: true })
              });
          }
      });
    
      // Send the output back to the main thread
      self.postMessage({
          status: 'complete',
          output: output,
      });
    });

Finally, let‚Äôs fill in our `onMessageReceived` function, which will update the
application state in response to messages from the worker thread. Add the
following code inside the `useEffect` hook we defined earlier:

Copied

    
    
    const onMessageReceived = (e) => {
      switch (e.data.status) {
        case 'initiate':
          // Model file start load: add a new progress item to the list.
          setReady(false);
          setProgressItems(prev => [...prev, e.data]);
          break;
    
        case 'progress':
          // Model file progress: update one of the progress items.
          setProgressItems(
            prev => prev.map(item => {
              if (item.file === e.data.file) {
                return { ...item, progress: e.data.progress }
              }
              return item;
            })
          );
          break;
    
        case 'done':
          // Model file loaded: remove the progress item from the list.
          setProgressItems(
            prev => prev.filter(item => item.file !== e.data.file)
          );
          break;
    
        case 'ready':
          // Pipeline ready: the worker is ready to accept messages.
          setReady(true);
          break;
    
        case 'update':
          // Generation update: update the output text.
          setOutput(e.data.output);
          break;
    
        case 'complete':
          // Generation complete: re-enable the "Translate" button
          setDisabled(false);
          break;
      }
    };

You can now run the application with `npm run dev` and perform multilingual
translation directly in your browser!

##  (Optional) Step 5: Build and deploy

To build your application, simply run `npm run build`. This will bundle your
application and output the static files to the `dist` folder.

For this demo, we will deploy our application as a static [Hugging Face
Space](https://huggingface.co/docs/hub/spaces), but you can deploy it anywhere
you like! If you haven‚Äôt already, you can create a free Hugging Face account
[here](https://huggingface.co/join).

  1. Visit <https://huggingface.co/new-space> and fill in the form. Remember to select ‚ÄúStatic‚Äù as the space type.
  2. Go to ‚ÄúFiles‚Äù ‚Üí ‚ÄúAdd file‚Äù ‚Üí ‚ÄúUpload files‚Äù. Drag the `index.html` file and `public/` folder from the `dist` folder into the upload box and click ‚ÄúUpload‚Äù. After they have uploaded, scroll down to the button and click ‚ÄúCommit changes to main‚Äù.

**That‚Äôs it!** Your application should now be live at
`https://huggingface.co/spaces/<your-username>/<your-space-name>`!

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/tutorials/react.md)

[‚ÜêBuilding a Vanilla JS
Application](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js) [Building a
Next.js Application‚Üí](/docs/transformers.js/v3.0.0/en/tutorials/next)

Building a React application Prerequisites Step 1: Initialise the project Step
2: Install and configure Transformers.js Step 3: Design the user interface
Step 4: Connecting everything together (Optional) Step 5: Build and deploy

[![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg) Hugging
Face](/)

  * [ Models](/models)
  * [ Datasets](/datasets)
  * [ Spaces](/spaces)
  * [ Posts](/posts)
  * [ Docs](/docs)
  * Solutions 

  * [Pricing ](/pricing)
  *   * * * *

  * [Log In ](/login)
  * [Sign Up ](/join)

Transformers.js documentation

Building a Vanilla JavaScript Application

# Transformers.js

üè° View all docsAWS Trainium & InferentiaAccelerateAmazon
SageMakerArgillaAutoTrainBitsandbytesChat UICompetitionsDataset
viewerDatasetsDiffusersDistilabelEvaluateGoogle CloudGoogle TPUsGradioHubHub
Python LibraryHugging Face Generative AI Services
(HUGS)Huggingface.jsInference API (serverless)Inference Endpoints
(dedicated)LeaderboardsOptimumPEFTSafetensorsSentence TransformersTRLTasksText
Embeddings InferenceText Generation
InferenceTokenizersTransformersTransformers.jstimm

Search documentation

mainv3.0.0v2.17.2 EN

[ ](https://github.com/huggingface/transformers.js)

[ü§ó Transformers.js ](/docs/transformers.js/v3.0.0/en/index)

Get started

[Installation ](/docs/transformers.js/v3.0.0/en/installation)[The pipeline API
](/docs/transformers.js/v3.0.0/en/pipelines)[Custom usage
](/docs/transformers.js/v3.0.0/en/custom_usage)

Tutorials

[Building a Vanilla JS Application
](/docs/transformers.js/v3.0.0/en/tutorials/vanilla-js)[Building a React
Application ](/docs/transformers.js/v3.0.0/en/tutorials/react)[Building a
Next.js Application ](/docs/transformers.js/v3.0.0/en/tutorials/next)[Building
a Browser Extension ](/docs/transformers.js/v3.0.0/en/tutorials/browser-
extension)[Building an Electron Application
](/docs/transformers.js/v3.0.0/en/tutorials/electron)[Server-side Inference in
Node.js ](/docs/transformers.js/v3.0.0/en/tutorials/node)

Developer Guides

[Accessing Private/Gated Models
](/docs/transformers.js/v3.0.0/en/guides/private)[Server-side Audio Processing
in Node.js ](/docs/transformers.js/v3.0.0/en/guides/node-audio-processing)

API Reference

[Index ](/docs/transformers.js/v3.0.0/en/api/transformers)[Pipelines
](/docs/transformers.js/v3.0.0/en/api/pipelines)[Models
](/docs/transformers.js/v3.0.0/en/api/models)[Tokenizers
](/docs/transformers.js/v3.0.0/en/api/tokenizers)[Processors
](/docs/transformers.js/v3.0.0/en/api/processors)[Configs
](/docs/transformers.js/v3.0.0/en/api/configs)[Environment variables
](/docs/transformers.js/v3.0.0/en/api/env)

Backends

Generation

Utilities

![Hugging Face's logo](/front/assets/huggingface_logo-noborder.svg)

Join the Hugging Face community

and get access to the augmented documentation experience

Collaborate on models, datasets and Spaces

Faster examples with accelerated inference

Switch between documentation themes

[Sign Up](/join)

to get started

#  Building a Vanilla JavaScript Application

In this tutorial, you‚Äôll build a simple web application that detects objects
in images using Transformers.js! To follow along, all you need is a code
editor, a browser, and a simple server (e.g., VS Code Live Server).

Here‚Äôs how it works: the user clicks ‚ÄúUpload image‚Äù and selects an image using
an input dialog. After analysing the image with an object detection model, the
predicted bounding boxes are overlaid on top of the image, like this:

![Demo](https://huggingface.co/datasets/Xenova/transformers.js-
docs/resolve/v3.0.0/js-detection-interence-zebra.png)

Useful links:

  * [Demo site](https://huggingface.co/spaces/Scrimba/vanilla-js-object-detector)
  * [Interactive code walk-through (scrim)](https://scrimba.com/scrim/cKm9bDAg)
  * [Source code](https://github.com/huggingface/transformers.js/tree/v3.0.0/examples/vanilla-js)

##  Step 1: HTML and CSS setup

Before we start building with Transformers.js, we first need to lay the
groundwork with some markup and styling. Create an `index.html` file with a
basic HTML skeleton, and add the following `<main>` tag to the `<body>`:

Copied

    
    
    <main class="container">
      <label class="custom-file-upload">
        <input id="file-upload" type="file" accept="image/*" />
        <img class="upload-icon" src="https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/v3.0.0/upload-icon.png" />
        Upload image
      </label>
      <div id="image-container"></div>
      <p id="status"></p>
    </main>

Click here to see a breakdown of this markup.

We‚Äôre adding an `<input>` element with `type="file"` that accepts images. This
allows the user to select an image from their local file system using a popup
dialog. The default styling for this element looks quite bad, so let‚Äôs add
some styling. The easiest way to achieve this is to wrap the `<input>` element
in a `<label>`, hide the input, and then style the label as a button.

We‚Äôre also adding an empty `<div>` container for displaying the image, plus an
empty `<p>` tag that we‚Äôll use to give status updates to the user while we
download and run the model, since both of these operations take some time.

Next, add the following CSS rules in a `style.css` file and link it to the
HTML:

Copied

    
    
    html,
    body {
        font-family: Arial, Helvetica, sans-serif;
    }
    
    .container {
        margin: 40px auto;
        width: max(50vw, 400px);
        display: flex;
        flex-direction: column;
        align-items: center;
    }
    
    .custom-file-upload {
        display: flex;
        align-items: center;
        gap: 10px;
        border: 2px solid black;
        padding: 8px 16px;
        cursor: pointer;
        border-radius: 6px;
    }
    
    #file-upload {
        display: none;
    }
    
    .upload-icon {
        width: 30px;
    }
    
    #image-container {
        width: 100%;
        margin-top: 20px;
        position: relative;
    }
    
    #image-container>img {
        width: 100%;
    }

Here‚Äôs how the UI looks at this point:

![Demo](https://huggingface.co/datasets/Xenova/transformers.js-
docs/resolve/v3.0.0/js-detection-btn.png)

##  Step 2: JavaScript setup

With the _boring_ part out of the way, let‚Äôs start writing some JavaScript
code! Create a file called `index.js` and link to it in `index.html` by adding
the following to the end of the `<body>`:

Copied

    
    
    <script src="./index.js" type="module"></script>

The `type="module"` attribute is important, as it turns our file into a
[JavaScript module](https://developer.mozilla.org/en-
US/docs/Web/JavaScript/Guide/Modules), meaning that we‚Äôll be able to use
imports and exports.

Moving into `index.js`, let‚Äôs import Transformers.js by adding the following
line to the top of the file:

Copied

    
    
    import { pipeline, env } from "https://cdn.jsdelivr.net/npm/@huggingface/transformers";

Since we will be downloading the model from the Hugging Face Hub, we can skip
the local model check by setting:

Copied

    
    
    env.allowLocalModels = false;

Next, let‚Äôs create references to the various DOM elements we will access
later:

Copied

    
    
    const fileUpload = document.getElementById("file-upload");
    const imageContainer = document.getElementById("image-container");
    const status = document.getElementById("status");

##  Step 3: Create an object detection pipeline

We‚Äôre finally ready to create our object detection pipeline! As a reminder, a
[pipeline](../pipelines). is a high-level interface provided by the library to
perform a specific task. In our case, we will instantiate an object detection
pipeline with the `pipeline()` helper function.

Since this can take some time (especially the first time when we have to
download the ~40MB model), we first update the `status` paragraph so that the
user knows that we‚Äôre about to load the model.

Copied

    
    
    status.textContent = "Loading model...";

To keep this tutorial simple, we‚Äôll be loading and running the model in the
main (UI) thread. This is not recommended for production applications, since
the UI will freeze when we‚Äôre performing these actions. This is because
JavaScript is a single-threaded language. To overcome this, you can use a [web
worker](https://developer.mozilla.org/en-
US/docs/Web/API/Web_Workers_API/Using_web_workers) to download and run the
model in the background. However, we‚Äôre not going to do cover that in this
tutorial‚Ä¶

We can now call the `pipeline()` function that we imported at the top of our
file, to create our object detection pipeline:

Copied

    
    
    const detector = await pipeline("object-detection", "Xenova/detr-resnet-50");

We‚Äôre passing two arguments into the `pipeline()` function: (1) task and (2)
model.

  1. The first tells Transformers.js what kind of task we want to perform. In our case, that is `object-detection`, but there are many other tasks that the library supports, including `text-generation`, `sentiment-analysis`, `summarization`, or `automatic-speech-recognition`. See [here](https://huggingface.co/docs/transformers.js/pipelines#tasks) for the full list.

  2. The second argument specifies which model we would like to use to solve the given task. We will use [`Xenova/detr-resnet-50`](https://huggingface.co/Xenova/detr-resnet-50), as it is a relatively small (~40MB) but powerful model for detecting objects in an image.

Once the function returns, we‚Äôll tell the user that the app is ready to be
used.

Copied

    
    
    status.textContent = "Ready";

##  Step 4: Create the image uploader

The next step is to support uploading/selection of images. To achieve this, we
will listen for ‚Äúchange‚Äù events from the `fileUpload` element. In the callback
function, we use a `FileReader()` to read the contents of the image if one is
selected (and nothing otherwise).

Copied

    
    
    fileUpload.addEventListener("change", function (e) {
      const file = e.target.files[0];
      if (!file) {
        return;
      }
    
      const reader = new FileReader();
    
      // Set up a callback when the file is loaded
      reader.onload = function (e2) {
        imageContainer.innerHTML = "";
        const image = document.createElement("img");
        image.src = e2.target.result;
        imageContainer.appendChild(image);
        // detect(image); // Uncomment this line to run the model
      };
      reader.readAsDataURL(file);
    });

Once the image has been loaded into the browser, the `reader.onload` callback
function will be invoked. In it, we append the new `<img>` element to the
`imageContainer` to be displayed to the user.

Don‚Äôt worry about the `detect(image)` function call (which is commented out) -
we will explain it later! For now, try to run the app and upload an image to
the browser. You should see your image displayed under the button like this:

![Demo](https://huggingface.co/datasets/Xenova/transformers.js-
docs/resolve/v3.0.0/js-detection-btn-img.png)

##  Step 5: Run the model

We‚Äôre finally ready to start interacting with Transformers.js! Let‚Äôs uncomment
the `detect(image)` function call from the snippet above. Then we‚Äôll define
the function itself:

Copied

    
    
    async function detect(img) {
      status.textContent = "Analysing...";
      const output = await detector(img.src, {
        threshold: 0.5,
        percentage: true,
      });
      status.textContent = "";
      console.log("output", output);
      // ...
    }

NOTE: The `detect` function needs to be asynchronous, since we‚Äôll `await` the
result of the the model.

Once we‚Äôve updated the `status` to ‚ÄúAnalysing‚Äù, we‚Äôre ready to perform
_inference_ , which simply means to run the model with some data. This is done
via the `detector()` function that was returned from `pipeline()`. The first
argument we‚Äôre passing is the image data (`img.src`).

The second argument is an options object:

  * We set the `threshold` property to `0.5`. This means that we want the model to be at least 50% confident before claiming it has detected an object in the image. The lower the threshold, the more objects it‚Äôll detect (but may misidentify objects); the higher the threshold, the fewer objects it‚Äôll detect (but may miss objects in the scene).
  * We also specify `percentage: true`, which means that we want the bounding box for the objects to be returned as percentages (instead of pixels).

If you now try to run the app and upload an image, you should see the
following output logged to the console:

![Demo](https://huggingface.co/datasets/Xenova/transformers.js-
docs/resolve/v3.0.0/js-detection-console.png)

In the example above, we uploaded an image of two elephants, so the `output`
variable holds an array with two objects, each containing a `label` (the
string ‚Äúelephant‚Äù), a `score` (indicating the model‚Äôs confidence in its
prediction) and a `box` object (representing the bounding box of the detected
entity).

##  Step 6: Render the boxes

The final step is to display the `box` coordinates as rectangles around each
of the elephants.

At the end of our `detect()` function, we‚Äôll run the `renderBox` function on
each object in the `output` array, using `.forEach()`.

Copied

    
    
    output.forEach(renderBox);

Here‚Äôs the code for the `renderBox()` function with comments to help you
understand what‚Äôs going on:

Copied

    
    
    // Render a bounding box and label on the image
    function renderBox({ box, label }) {
      const { xmax, xmin, ymax, ymin } = box;
    
      // Generate a random color for the box
      const color = "#" + Math.floor(Math.random() * 0xffffff).toString(16).padStart(6, 0);
    
      // Draw the box
      const boxElement = document.createElement("div");
      boxElement.className = "bounding-box";
      Object.assign(boxElement.style, {
        borderColor: color,
        left: 100 * xmin + "%",
        top: 100 * ymin + "%",
        width: 100 * (xmax - xmin) + "%",
        height: 100 * (ymax - ymin) + "%",
      });
    
      // Draw the label
      const labelElement = document.createElement("span");
      labelElement.textContent = label;
      labelElement.className = "bounding-box-label";
      labelElement.style.backgroundColor = color;
    
      boxElement.appendChild(labelElement);
      imageContainer.appendChild(boxElement);
    }

The bounding box and label span also need some styling, so add the following
to the `style.css` file:

Copied

    
    
    .bounding-box {
        position: absolute;
        box-sizing: border-box;
        border-width: 2px;
        border-style: solid;
    }
    
    .bounding-box-label {
        color: white;
        position: absolute;
        font-size: 12px;
        margin-top: -16px;
        margin-left: -2px;
        padding: 1px;
    }

**And that‚Äôs it!**

You‚Äôve now built your own fully-functional AI application that detects objects
in images, which runns completely in your browser: no external server, APIs,
or build tools. Pretty cool! ü•≥

![Demo](https://huggingface.co/datasets/Xenova/transformers.js-
docs/resolve/v3.0.0/js-detection-inference-elephant.png)

The app is live at the following URL:
<https://huggingface.co/spaces/Scrimba/vanilla-js-object-detector>

[< > Update on
GitHub](https://github.com/huggingface/transformers.js/blob/v3.0.0/docs/source/tutorials/vanilla-
js.md)

[‚ÜêCustom usage](/docs/transformers.js/v3.0.0/en/custom_usage) [Building a
React Application‚Üí](/docs/transformers.js/v3.0.0/en/tutorials/react)

Building a Vanilla JavaScript Application Step 1: HTML and CSS setup Step 2:
JavaScript setup Step 3: Create an object detection pipeline Step 4: Create
the image uploader Step 5: Run the model Step 6: Render the boxes

